{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6>\n",
    "    <b>Analyze_Text.ipynb:</b> Analyze Text with Pandas and Watson Natural Language Understanding\n",
    " </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook shows how the open source library [Text Extensions for Pandas](https://github.com/CODAIT/text-extensions-for-pandas) lets you use use [Pandas](https://pandas.pydata.org/) DataFrames and the [Watson Natural Language Understanding](https://www.ibm.com/cloud/watson-natural-language-understanding) service to analyze natural language text. \n",
    "\n",
    "We start out with an excerpt from the [plot synopsis from the Wikipedia page\n",
    "for *Monty Python and the Holy Grail*](https://en.wikipedia.org/wiki/Monty_Python_and_the_Holy_Grail#Plot). \n",
    "We pass this example document to the Watson Natural Language \n",
    "Understanding (NLU) service. Then we use Text Extensions for Pandas to convert the output of the \n",
    "Watson NLU service to Pandas DataFrames. Next, we perform an example analysis task both with \n",
    "and without Pandas to show how Pandas makes analyzing NLP information easier. Finally, we \n",
    "walk through all the different DataFrames that Text Extensions for Pandas can extract from \n",
    "the output of Watson Natural Language Understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup\n",
    "\n",
    "This notebook requires a Python 3.7 or later environment with the following packages:\n",
    "* The dependencies listed in the [\"requirements.txt\" file for Text Extensions for Pandas](https://github.com/CODAIT/text-extensions-for-pandas/blob/master/requirements.txt)\n",
    "* The \"[ibm-watson](https://pypi.org/project/ibm-watson/)\" package, available via `pip install ibm-watson`\n",
    "* `text_extensions_for_pandas`\n",
    "\n",
    "You can satisfy the dependency on `text_extensions_for_pandas` in either of two ways:\n",
    "\n",
    "* Run `pip install text_extensions_for_pandas` before running this notebook. This command adds the library to your Python environment.\n",
    "* Run this notebook out of your local copy of the Text Extensions for Pandas project's [source tree](https://github.com/CODAIT/text-extensions-for-pandas). In this case, the notebook will use the version of Text Extensions for Pandas in your local source tree **if the package is not installed in your Python environment**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Python libraries\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from typing import *\n",
    "\n",
    "# IBM Watson libraries\n",
    "import ibm_watson\n",
    "import ibm_watson.natural_language_understanding_v1 as nlu\n",
    "import ibm_cloud_sdk_core\n",
    "\n",
    "# And of course we need the text_extensions_for_pandas library itself.\n",
    "try:\n",
    "    import text_extensions_for_pandas as tp\n",
    "except ModuleNotFoundError as e:\n",
    "    # If we're running from within the project source tree and the parent Python\n",
    "    # environment doesn't have the text_extensions_for_pandas package, use the\n",
    "    # version in the local source tree.\n",
    "    if not os.getcwd().endswith(\"notebooks\"):\n",
    "        raise e\n",
    "    if \"..\" not in sys.path:\n",
    "        sys.path.insert(0, \"..\")\n",
    "    import text_extensions_for_pandas as tp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the Watson Natural Language Understanding Service\n",
    "\n",
    "In this part of the notebook, we will use the Watson Natural Language Understanding (NLU) service to extract key features from our example document.\n",
    "\n",
    "You can create an instance of Watson NLU on the IBM Cloud for free by navigating to [this page](https://www.ibm.com/cloud/watson-natural-language-understanding) and clicking on the button marked \"Get started free\". You can also install your own instance of Watson NLU on [OpenShift](https://www.openshift.com/) by using [IBM Watson Natural Language Understanding for IBM Cloud Pak for Data](\n",
    "https://catalog.redhat.com/software/operators/detail/5e9873e13f398525a0ceafe5).\n",
    "\n",
    "You'll need two pieces of information to access your instance of Watson NLU: An **API key** and a **service URL**. If you're using Watson NLU on the IBM Cloud, you can find your API key and service URL in the IBM Cloud web UI. Navigate to the [resource list](https://cloud.ibm.com/resources) and click on your instance of Natural Language Understanding to open the management UI for your service. Then click on the \"Manage\" tab to show a page with your API key and service URL.\n",
    "\n",
    "The cell that follows assumes that you are using the environment variables `IBM_API_KEY` and `IBM_SERVICE_URL` to store your credentials. If you're running this notebook in Jupyter on your laptop, you can set these environment variables while starting up `jupyter notebook` or `jupyter lab`. For example:\n",
    "``` console\n",
    "IBM_API_KEY='<my API key>' \\\n",
    "IBM_SERVICE_URL='<my service URL>' \\\n",
    "  jupyter lab\n",
    "```\n",
    "\n",
    "Alternately, you can uncomment the first two lines of code below to set the `IBM_API_KEY` and `IBM_SERVICE_URL` environment variables directly.\n",
    "**Be careful not to store your API key in any publicly-accessible location!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need to embed your credentials inline, uncomment the following two lines and\n",
    "# paste your credentials in the indicated locations.\n",
    "os.environ[\"IBM_API_KEY\"] = \"FWZd3Go3Kr9xfTF3bTns5YqCXW1-RiZ6hldC5GLb8FRw\"\n",
    "os.environ[\"IBM_SERVICE_URL\"] = \"https://api.us-south.natural-language-understanding.watson.cloud.ibm.com/instances/21b9b875-4ddb-46ad-bb22-d78747622ca7\"\n",
    "\n",
    "# Retrieve the API key for your Watson NLU service instance\n",
    "if \"IBM_API_KEY\" not in os.environ:\n",
    "    raise ValueError(\"Expected Watson NLU api key in the environment variable 'IBM_API_KEY'\")\n",
    "api_key = os.environ.get(\"IBM_API_KEY\")\n",
    "    \n",
    "# Retrieve the service URL for your Watson NLU service instance\n",
    "if \"IBM_SERVICE_URL\" not in os.environ:\n",
    "    raise ValueError(\"Expected Watson NLU service URL in the environment variable 'IBM_SERVICE_URL'\")\n",
    "service_url = os.environ.get(\"IBM_SERVICE_URL\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to the Watson Natural Language Understanding Python API\n",
    "\n",
    "This notebook uses the IBM Watson Python SDK to perform authentication on the IBM Cloud via the \n",
    "`IAMAuthenticator` class. See [the IBM Watson Python SDK documentation](https://github.com/watson-developer-cloud/python-sdk#iam) for more information. \n",
    "\n",
    "We start by using the API key and service URL from the previous cell to create an instance of the\n",
    "Python API for Watson NLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ibm_watson.natural_language_understanding_v1.NaturalLanguageUnderstandingV1 at 0x7fb728dac730>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "natural_language_understanding = ibm_watson.NaturalLanguageUnderstandingV1(\n",
    "    version=\"2019-07-12\",\n",
    "    authenticator=ibm_cloud_sdk_core.authenticators.IAMAuthenticator(api_key)\n",
    ")\n",
    "natural_language_understanding.set_service_url(service_url)\n",
    "natural_language_understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pass a Document through the Watson NLU Service\n",
    "\n",
    "Once you've opened a connection to the Watson NLU service, you can pass documents through \n",
    "the service by invoking the [`analyze()` method](https://cloud.ibm.com/apidocs/natural-language-understanding?code=python#analyze).\n",
    "\n",
    "The [example document](https://raw.githubusercontent.com/CODAIT/text-extensions-for-pandas/master/resources/holy_grail_short.txt) that we use here is an excerpt from\n",
    "the plot summary for *Monty Python and the Holy Grail*, drawn from the [Wikipedia entry](https://en.wikipedia.org/wiki/Monty_Python_and_the_Holy_Grail) for that movie.\n",
    "\n",
    "Let's show what the raw text looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Document Text:</b><blockquote>In AD 932, King Arthur and his squire, Patsy, travel throughout Britain searching for men to join the Knights of the Round Table. Along the way, he recruits Sir Bedevere the Wise, Sir Lancelot the Brave, Sir Galahad the Pure, Sir Robin the Not-Quite-So-Brave-as-Sir-Lancelot, and Sir Not-Appearing-in-this-Film, along with their squires and Robin's troubadours. Arthur leads the men to Camelot, but upon further consideration (thanks to a musical number) he decides not to go there because it is \"a silly place\". As they turn away, God (an image of W. G. Grace) speaks to them and gives Arthur the task of finding the Holy Grail.</blockquote>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "doc_file = \"../resources/holy_grail_short.txt\"\n",
    "with open(doc_file, \"r\") as f:\n",
    "    doc_text = f.read()\n",
    "    \n",
    "display(HTML(f\"<b>Document Text:</b><blockquote>{doc_text}</blockquote>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, we instruct Watson Natural Language Understanding to perform five different kinds of analysis on the example document:\n",
    "* entities (with sentiment)\n",
    "* keywords (with sentiment and emotion)\n",
    "* relations\n",
    "* semantic_roles\n",
    "* syntax (with sentences, tokens, and part of speech)\n",
    "\n",
    "See [the Watson NLU documentation](https://cloud.ibm.com/apidocs/natural-language-understanding?code=python#text-analytics-features) for a full description of the types of analysis that NLU can perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the request\n",
    "response = natural_language_understanding.analyze(\n",
    "    text=doc_text,\n",
    "    # TODO: Use this URL once we've pushed the shortened document to Github\n",
    "    #url=\"https://raw.githubusercontent.com/CODAIT/text-extensions-for-pandas/master/resources/holy_grail_short.txt\",\n",
    "    return_analyzed_text=True,\n",
    "    features=nlu.Features(\n",
    "        entities=nlu.EntitiesOptions(sentiment=True, mentions=True),\n",
    "        keywords=nlu.KeywordsOptions(sentiment=True, emotion=True),\n",
    "        relations=nlu.RelationsOptions(),\n",
    "        semantic_roles=nlu.SemanticRolesOptions(),\n",
    "        syntax=nlu.SyntaxOptions(sentences=True, \n",
    "                                 tokens=nlu.SyntaxOptionsTokens(lemma=True, part_of_speech=True))\n",
    "    )).get_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response from the `analyze()` method is a Python dictionary. The dictionary contains an entry \n",
    "for each pass of analysis requested, plus some additional entries with metadata about the API request\n",
    "itself. Here's a list of the keys in `response`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['usage', 'syntax', 'semantic_roles', 'relations', 'language', 'keywords', 'entities', 'analyzed_text'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform an Example Task\n",
    "\n",
    "Let's use the information that Watson Natural Language Understanding has extracted from our example document to perform an example task: *Find all the pronouns in each sentence, broken down by sentence.*\n",
    "\n",
    "This task could serve as first step to a number of more complex tasks, such as \n",
    "resolving anaphora (for example, associating \"King Arthur\" with \"his\" in the phrase \"King Arthur and his squire, Patsy\") or analyzing the relationship between sentiment and the gender of pronouns.\n",
    "\n",
    "We'll start by doing this task using straight Python code that operates directly over the output of Watson NLU's `analyze()` method. Then we'll redo the task using Pandas DataFrames and Text Extensions for Pandas. This exercise will show how Pandas DataFrames can represent the intermediate data structures of an NLP application in a way that is both easier to understand and easier to manipulate with less code.\n",
    "\n",
    "Let's begin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform the Task Without Using Pandas\n",
    "\n",
    "All the information that we need to perform our task is in the \"syntax\" section of the response \n",
    "we captured above from Watson NLU's `analyze()` method. Syntax analysis captures a large amount\n",
    "of information, so the \"syntax\" section of the response is very verbose. \n",
    "\n",
    "For reference, here's the text of our example document again:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Document Text:</b><blockquote>In AD 932, King Arthur and his squire, Patsy, travel throughout Britain searching for men to join the Knights of the Round Table. Along the way, he recruits Sir Bedevere the Wise, Sir Lancelot the Brave, Sir Galahad the Pure, Sir Robin the Not-Quite-So-Brave-as-Sir-Lancelot, and Sir Not-Appearing-in-this-Film, along with their squires and Robin's troubadours. Arthur leads the men to Camelot, but upon further consideration (thanks to a musical number) he decides not to go there because it is \"a silly place\". As they turn away, God (an image of W. G. Grace) speaks to them and gives Arthur the task of finding the Holy Grail.</blockquote>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML(f\"<b>Document Text:</b><blockquote>{doc_text}</blockquote>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's the output of Watson NLU's syntax analysis, converted to a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': [{'text': 'In',\n",
       "   'part_of_speech': 'ADP',\n",
       "   'location': [0, 2],\n",
       "   'lemma': 'in'},\n",
       "  {'text': 'AD', 'part_of_speech': 'NOUN', 'location': [3, 5], 'lemma': 'ad'},\n",
       "  {'text': '932', 'part_of_speech': 'NUM', 'location': [6, 9]},\n",
       "  {'text': ',', 'part_of_speech': 'PUNCT', 'location': [9, 10]},\n",
       "  {'text': 'King',\n",
       "   'part_of_speech': 'PROPN',\n",
       "   'location': [11, 15],\n",
       "   'lemma': 'king'},\n",
       "  {'text': 'Arthur', 'part_of_speech': 'PROPN', 'location': [16, 22]},\n",
       "  {'text': 'and',\n",
       "   'part_of_speech': 'CCONJ',\n",
       "   'location': [23, 26],\n",
       "   'lemma': 'and'},\n",
       "  {'text': 'his',\n",
       "   'part_of_speech': 'PRON',\n",
       "   'location': [27, 30],\n",
       "   'lemma': 'his'},\n",
       "  {'text': 'squire',\n",
       "   'part_of_speech': 'NOUN',\n",
       "   'location': [31, 37],\n",
       "   'lemma': 'squire'},\n",
       "  {'text': ',', 'part_of_speech': 'PUNCT', 'location': [37, 38]},\n",
       "  {'text': 'Patsy',\n",
       "   'part_of_speech': 'PROPN',\n",
       "   'location': [39, 44],\n",
       "   'lemma': 'patsy'},\n",
       "  {'text': ',', 'part_of_speech': 'PUNCT', 'location': [44, 45]},\n",
       "  {'text': 'travel',\n",
       "   'part_of_speech': 'NOUN',\n",
       "   'location': [46, 52],\n",
       "   'lemma': 'travel'},\n",
       "  {'text': 'throughout',\n",
       "   'part_of_speech': 'ADP',\n",
       "   'location': [53, 63],\n",
       "   'lemma': 'throughout'},\n",
       "  {'text': 'Britain', 'part_of_speech': 'PROPN', 'location': [64, 71]},\n",
       "  {'text': 'searching',\n",
       "   'part_of_speech': 'VERB',\n",
       "   'location': [72, 81],\n",
       "   'lemma': 'search'},\n",
       "  {'text': 'for',\n",
       "   'part_of_speech': 'ADP',\n",
       "   'location': [82, 85],\n",
       "   'lemma': 'for'},\n",
       "  {'text': 'men',\n",
       "   'part_of_speech': 'NOUN',\n",
       "   'location': [86, 89],\n",
       "   'lemma': 'man'},\n",
       "  {'text': 'to',\n",
       "   'part_of_speech': 'PART',\n",
       "   'location': [90, 92],\n",
       "   'lemma': 'to'},\n",
       "  {'text': 'join',\n",
       "   'part_of_speech': 'VERB',\n",
       "   'location': [93, 97],\n",
       "   'lemma': 'join'},\n",
       "  {'text': 'the',\n",
       "   'part_of_speech': 'DET',\n",
       "   'location': [98, 101],\n",
       "   'lemma': 'the'},\n",
       "  {'text': 'Knights',\n",
       "   'part_of_speech': 'PROPN',\n",
       "   'location': [102, 109],\n",
       "   'lemma': 'knight'},\n",
       "  {'text': 'of',\n",
       "   'part_of_speech': 'ADP',\n",
       "   'location': [110, 112],\n",
       "   'lemma': 'of'},\n",
       "  {'text': 'the',\n",
       "   'part_of_speech': 'DET',\n",
       "   'location': [113, 116],\n",
       "   'lemma': 'the'},\n",
       "  {'text': 'Round',\n",
       "   'part_of_speech': 'PROPN',\n",
       "   'location': [117, 122],\n",
       "   'lemma': 'round'},\n",
       "  {'text': 'Table',\n",
       "   'part_of_speech': 'PROPN',\n",
       "   'location': [123, 128],\n",
       "   'lemma': 'table'},\n",
       "  {'text': '.', 'part_of_speech': 'PUNCT', 'location': [128, 129]},\n",
       "  {'text': 'Along',\n",
       "   'part_of_speech': 'ADP',\n",
       "   'location': [130, 135],\n",
       "   'lemma': 'along'},\n",
       "  {'text': 'the',\n",
       "   'part_of_speech': 'DET',\n",
       "   'location': [136, 139],\n",
       "   'lemma': 'the'},\n",
       "  {'text': 'way',\n",
       "   'part_of_speech': 'NOUN',\n",
       "   'location': [140, 143],\n",
       "   'lemma': 'way'},\n",
       "  {'text': ',', 'part_of_speech': 'PUNCT', 'location': [143, 144]},\n",
       "  {'text': 'he',\n",
       "   'part_of_speech': 'PRON',\n",
       "   'location': [145, 147],\n",
       "   'lemma': 'he'},\n",
       "  {'text': 'recruits',\n",
       "   'part_of_speech': 'VERB',\n",
       "   'location': [148, 156],\n",
       "   'lemma': 'recruit'},\n",
       "  {'text': 'Sir',\n",
       "   'part_of_speech': 'PROPN',\n",
       "   'location': [157, 160],\n",
       "   'lemma': 'sir'},\n",
       "  {'text': 'Bedevere', 'part_of_speech': 'VERB', 'location': [161, 169]},\n",
       "  {'text': 'the',\n",
       "   'part_of_speech': 'DET',\n",
       "   'location': [170, 173],\n",
       "   'lemma': 'the'},\n",
       "  {'text': 'Wise',\n",
       "   'part_of_speech': 'PROPN',\n",
       "   'location': [174, 178],\n",
       "   'lemma': 'wise'},\n",
       "  {'text': ',', 'part_of_speech': 'PUNCT', 'location': [178, 179]},\n",
       "  {'text': 'Sir',\n",
       "   'part_of_speech': 'PROPN',\n",
       "   'location': [180, 183],\n",
       "   'lemma': 'sir'},\n",
       "  {'text': 'Lancelot', 'part_of_speech': 'PROPN', 'location': [184, 192]},\n",
       "  {'text': 'the',\n",
       "   'part_of_speech': 'DET',\n",
       "   'location': [193, 196],\n",
       "   'lemma': 'the'},\n",
       "  {'text': 'Brave',\n",
       "   'part_of_speech': 'PROPN',\n",
       "   'location': [197, 202],\n",
       "   'lemma': 'brave'},\n",
       "  {'text': ',', 'part_of_speech': 'PUNCT', 'location': [202, 203]},\n",
       "  {'text': 'Sir',\n",
       "   'part_of_speech': 'PROPN',\n",
       "   'location': [204, 207],\n",
       "   'lemma': 'sir'},\n",
       "  {'text': 'Galahad', 'part_of_speech': 'PROPN', 'location': [208, 215]},\n",
       "  {'text': 'the',\n",
       "   'part_of_speech': 'DET',\n",
       "   'location': [216, 219],\n",
       "   'lemma': 'the'},\n",
       "  {'text': 'Pure', 'part_of_speech': 'PROPN', 'location': [220, 224]},\n",
       "  {'text': ',', 'part_of_speech': 'PUNCT', 'location': [224, 225]},\n",
       "  {'text': 'Sir',\n",
       "   'part_of_speech': 'PROPN',\n",
       "   'location': [226, 229],\n",
       "   'lemma': 'sir'},\n",
       "  {'text': 'Robin',\n",
       "   'part_of_speech': 'PROPN',\n",
       "   'location': [230, 235],\n",
       "   'lemma': 'robin'},\n",
       "  {'text': 'the',\n",
       "   'part_of_speech': 'DET',\n",
       "   'location': [236, 239],\n",
       "   'lemma': 'the'},\n",
       "  {'text': 'Not',\n",
       "   'part_of_speech': 'ADV',\n",
       "   'location': [240, 243],\n",
       "   'lemma': 'not'},\n",
       "  {'text': '-', 'part_of_speech': 'PUNCT', 'location': [243, 244]},\n",
       "  {'text': 'Quite',\n",
       "   'part_of_speech': 'ADV',\n",
       "   'location': [244, 249],\n",
       "   'lemma': 'quite'},\n",
       "  {'text': '-', 'part_of_speech': 'PUNCT', 'location': [249, 250]},\n",
       "  {'text': 'So',\n",
       "   'part_of_speech': 'ADV',\n",
       "   'location': [250, 252],\n",
       "   'lemma': 'so'},\n",
       "  {'text': '-', 'part_of_speech': 'PUNCT', 'location': [252, 253]},\n",
       "  {'text': 'Brave',\n",
       "   'part_of_speech': 'ADJ',\n",
       "   'location': [253, 258],\n",
       "   'lemma': 'brave'},\n",
       "  {'text': '-', 'part_of_speech': 'PUNCT', 'location': [258, 259]},\n",
       "  {'text': 'as',\n",
       "   'part_of_speech': 'ADP',\n",
       "   'location': [259, 261],\n",
       "   'lemma': 'as'},\n",
       "  {'text': '-', 'part_of_speech': 'PUNCT', 'location': [261, 262]},\n",
       "  {'text': 'Sir',\n",
       "   'part_of_speech': 'NOUN',\n",
       "   'location': [262, 265],\n",
       "   'lemma': 'sir'},\n",
       "  {'text': '-', 'part_of_speech': 'PUNCT', 'location': [265, 266]},\n",
       "  {'text': 'Lancelot', 'part_of_speech': 'PROPN', 'location': [266, 274]},\n",
       "  {'text': ',', 'part_of_speech': 'PUNCT', 'location': [274, 275]},\n",
       "  {'text': 'and',\n",
       "   'part_of_speech': 'CCONJ',\n",
       "   'location': [276, 279],\n",
       "   'lemma': 'and'},\n",
       "  {'text': 'Sir',\n",
       "   'part_of_speech': 'PROPN',\n",
       "   'location': [280, 283],\n",
       "   'lemma': 'sir'},\n",
       "  {'text': 'Not',\n",
       "   'part_of_speech': 'ADV',\n",
       "   'location': [284, 287],\n",
       "   'lemma': 'not'},\n",
       "  {'text': '-', 'part_of_speech': 'PUNCT', 'location': [287, 288]},\n",
       "  {'text': 'Appearing', 'part_of_speech': 'PROPN', 'location': [288, 297]},\n",
       "  {'text': '-', 'part_of_speech': 'PUNCT', 'location': [297, 298]},\n",
       "  {'text': 'in',\n",
       "   'part_of_speech': 'ADP',\n",
       "   'location': [298, 300],\n",
       "   'lemma': 'in'},\n",
       "  {'text': '-', 'part_of_speech': 'PUNCT', 'location': [300, 301]},\n",
       "  {'text': 'this',\n",
       "   'part_of_speech': 'PRON',\n",
       "   'location': [301, 305],\n",
       "   'lemma': 'this'},\n",
       "  {'text': '-', 'part_of_speech': 'PUNCT', 'location': [305, 306]},\n",
       "  {'text': 'Film',\n",
       "   'part_of_speech': 'NOUN',\n",
       "   'location': [306, 310],\n",
       "   'lemma': 'film'},\n",
       "  {'text': ',', 'part_of_speech': 'PUNCT', 'location': [310, 311]},\n",
       "  {'text': 'along',\n",
       "   'part_of_speech': 'ADP',\n",
       "   'location': [312, 317],\n",
       "   'lemma': 'along'},\n",
       "  {'text': 'with',\n",
       "   'part_of_speech': 'ADP',\n",
       "   'location': [318, 322],\n",
       "   'lemma': 'with'},\n",
       "  {'text': 'their',\n",
       "   'part_of_speech': 'PRON',\n",
       "   'location': [323, 328],\n",
       "   'lemma': 'their'},\n",
       "  {'text': 'squires',\n",
       "   'part_of_speech': 'NOUN',\n",
       "   'location': [329, 336],\n",
       "   'lemma': 'squire'},\n",
       "  {'text': 'and',\n",
       "   'part_of_speech': 'CCONJ',\n",
       "   'location': [337, 340],\n",
       "   'lemma': 'and'},\n",
       "  {'text': 'Robin',\n",
       "   'part_of_speech': 'PROPN',\n",
       "   'location': [341, 346],\n",
       "   'lemma': 'robin'},\n",
       "  {'text': \"'s\",\n",
       "   'part_of_speech': 'PART',\n",
       "   'location': [346, 348],\n",
       "   'lemma': \"'s\"},\n",
       "  {'text': 'troubadours',\n",
       "   'part_of_speech': 'NOUN',\n",
       "   'location': [349, 360],\n",
       "   'lemma': 'troubadour'},\n",
       "  {'text': '.', 'part_of_speech': 'PUNCT', 'location': [360, 361]},\n",
       "  {'text': 'Arthur', 'part_of_speech': 'PROPN', 'location': [362, 368]},\n",
       "  {'text': 'leads',\n",
       "   'part_of_speech': 'VERB',\n",
       "   'location': [369, 374],\n",
       "   'lemma': 'lead'},\n",
       "  {'text': 'the',\n",
       "   'part_of_speech': 'DET',\n",
       "   'location': [375, 378],\n",
       "   'lemma': 'the'},\n",
       "  {'text': 'men',\n",
       "   'part_of_speech': 'NOUN',\n",
       "   'location': [379, 382],\n",
       "   'lemma': 'man'},\n",
       "  {'text': 'to',\n",
       "   'part_of_speech': 'ADP',\n",
       "   'location': [383, 385],\n",
       "   'lemma': 'to'},\n",
       "  {'text': 'Camelot', 'part_of_speech': 'PROPN', 'location': [386, 393]},\n",
       "  {'text': ',', 'part_of_speech': 'PUNCT', 'location': [393, 394]},\n",
       "  {'text': 'but',\n",
       "   'part_of_speech': 'CCONJ',\n",
       "   'location': [395, 398],\n",
       "   'lemma': 'but'},\n",
       "  {'text': 'upon',\n",
       "   'part_of_speech': 'ADP',\n",
       "   'location': [399, 403],\n",
       "   'lemma': 'upon'},\n",
       "  {'text': 'further',\n",
       "   'part_of_speech': 'ADJ',\n",
       "   'location': [404, 411],\n",
       "   'lemma': 'far'},\n",
       "  {'text': 'consideration',\n",
       "   'part_of_speech': 'NOUN',\n",
       "   'location': [412, 425],\n",
       "   'lemma': 'consideration'},\n",
       "  {'text': '(', 'part_of_speech': 'PUNCT', 'location': [426, 427]},\n",
       "  {'text': 'thanks',\n",
       "   'part_of_speech': 'NOUN',\n",
       "   'location': [427, 433],\n",
       "   'lemma': 'thanks'},\n",
       "  {'text': 'to',\n",
       "   'part_of_speech': 'ADP',\n",
       "   'location': [434, 436],\n",
       "   'lemma': 'to'},\n",
       "  {'text': 'a', 'part_of_speech': 'DET', 'location': [437, 438], 'lemma': 'a'},\n",
       "  {'text': 'musical',\n",
       "   'part_of_speech': 'ADJ',\n",
       "   'location': [439, 446],\n",
       "   'lemma': 'musical'},\n",
       "  {'text': 'number',\n",
       "   'part_of_speech': 'NOUN',\n",
       "   'location': [447, 453],\n",
       "   'lemma': 'number'},\n",
       "  {'text': ')', 'part_of_speech': 'PUNCT', 'location': [453, 454]},\n",
       "  {'text': 'he',\n",
       "   'part_of_speech': 'PRON',\n",
       "   'location': [455, 457],\n",
       "   'lemma': 'he'},\n",
       "  {'text': 'decides',\n",
       "   'part_of_speech': 'VERB',\n",
       "   'location': [458, 465],\n",
       "   'lemma': 'decide'},\n",
       "  {'text': 'not',\n",
       "   'part_of_speech': 'PART',\n",
       "   'location': [466, 469],\n",
       "   'lemma': 'not'},\n",
       "  {'text': 'to',\n",
       "   'part_of_speech': 'PART',\n",
       "   'location': [470, 472],\n",
       "   'lemma': 'to'},\n",
       "  {'text': 'go',\n",
       "   'part_of_speech': 'VERB',\n",
       "   'location': [473, 475],\n",
       "   'lemma': 'go'},\n",
       "  {'text': 'there',\n",
       "   'part_of_speech': 'ADV',\n",
       "   'location': [476, 481],\n",
       "   'lemma': 'there'},\n",
       "  {'text': 'because',\n",
       "   'part_of_speech': 'SCONJ',\n",
       "   'location': [482, 489],\n",
       "   'lemma': 'because'},\n",
       "  {'text': 'it',\n",
       "   'part_of_speech': 'PRON',\n",
       "   'location': [490, 492],\n",
       "   'lemma': 'it'},\n",
       "  {'text': 'is',\n",
       "   'part_of_speech': 'AUX',\n",
       "   'location': [493, 495],\n",
       "   'lemma': 'be'},\n",
       "  {'text': '\"', 'part_of_speech': 'PUNCT', 'location': [496, 497]},\n",
       "  {'text': 'a', 'part_of_speech': 'DET', 'location': [497, 498], 'lemma': 'a'},\n",
       "  {'text': 'silly',\n",
       "   'part_of_speech': 'ADJ',\n",
       "   'location': [499, 504],\n",
       "   'lemma': 'silly'},\n",
       "  {'text': 'place',\n",
       "   'part_of_speech': 'NOUN',\n",
       "   'location': [505, 510],\n",
       "   'lemma': 'place'},\n",
       "  {'text': '\"', 'part_of_speech': 'PUNCT', 'location': [510, 511]},\n",
       "  {'text': '.', 'part_of_speech': 'PUNCT', 'location': [511, 512]},\n",
       "  {'text': 'As',\n",
       "   'part_of_speech': 'SCONJ',\n",
       "   'location': [513, 515],\n",
       "   'lemma': 'as'},\n",
       "  {'text': 'they',\n",
       "   'part_of_speech': 'PRON',\n",
       "   'location': [516, 520],\n",
       "   'lemma': 'they'},\n",
       "  {'text': 'turn',\n",
       "   'part_of_speech': 'VERB',\n",
       "   'location': [521, 525],\n",
       "   'lemma': 'turn'},\n",
       "  {'text': 'away',\n",
       "   'part_of_speech': 'ADV',\n",
       "   'location': [526, 530],\n",
       "   'lemma': 'away'},\n",
       "  {'text': ',', 'part_of_speech': 'PUNCT', 'location': [530, 531]},\n",
       "  {'text': 'God',\n",
       "   'part_of_speech': 'PROPN',\n",
       "   'location': [532, 535],\n",
       "   'lemma': 'god'},\n",
       "  {'text': '(', 'part_of_speech': 'PUNCT', 'location': [536, 537]},\n",
       "  {'text': 'an',\n",
       "   'part_of_speech': 'DET',\n",
       "   'location': [537, 539],\n",
       "   'lemma': 'a'},\n",
       "  {'text': 'image',\n",
       "   'part_of_speech': 'NOUN',\n",
       "   'location': [540, 545],\n",
       "   'lemma': 'image'},\n",
       "  {'text': 'of',\n",
       "   'part_of_speech': 'ADP',\n",
       "   'location': [546, 548],\n",
       "   'lemma': 'of'},\n",
       "  {'text': 'W.', 'part_of_speech': 'PROPN', 'location': [549, 551]},\n",
       "  {'text': 'G.', 'part_of_speech': 'PROPN', 'location': [552, 554]},\n",
       "  {'text': 'Grace',\n",
       "   'part_of_speech': 'PROPN',\n",
       "   'location': [555, 560],\n",
       "   'lemma': 'grace'},\n",
       "  {'text': ')', 'part_of_speech': 'PUNCT', 'location': [560, 561]},\n",
       "  {'text': 'speaks',\n",
       "   'part_of_speech': 'VERB',\n",
       "   'location': [562, 568],\n",
       "   'lemma': 'speak'},\n",
       "  {'text': 'to',\n",
       "   'part_of_speech': 'ADP',\n",
       "   'location': [569, 571],\n",
       "   'lemma': 'to'},\n",
       "  {'text': 'them',\n",
       "   'part_of_speech': 'PRON',\n",
       "   'location': [572, 576],\n",
       "   'lemma': 'they'},\n",
       "  {'text': 'and',\n",
       "   'part_of_speech': 'CCONJ',\n",
       "   'location': [577, 580],\n",
       "   'lemma': 'and'},\n",
       "  {'text': 'gives',\n",
       "   'part_of_speech': 'VERB',\n",
       "   'location': [581, 586],\n",
       "   'lemma': 'give'},\n",
       "  {'text': 'Arthur', 'part_of_speech': 'PROPN', 'location': [587, 593]},\n",
       "  {'text': 'the',\n",
       "   'part_of_speech': 'DET',\n",
       "   'location': [594, 597],\n",
       "   'lemma': 'the'},\n",
       "  {'text': 'task',\n",
       "   'part_of_speech': 'NOUN',\n",
       "   'location': [598, 602],\n",
       "   'lemma': 'task'},\n",
       "  {'text': 'of',\n",
       "   'part_of_speech': 'SCONJ',\n",
       "   'location': [603, 605],\n",
       "   'lemma': 'of'},\n",
       "  {'text': 'finding',\n",
       "   'part_of_speech': 'VERB',\n",
       "   'location': [606, 613],\n",
       "   'lemma': 'find'},\n",
       "  {'text': 'the',\n",
       "   'part_of_speech': 'DET',\n",
       "   'location': [614, 617],\n",
       "   'lemma': 'the'},\n",
       "  {'text': 'Holy', 'part_of_speech': 'PROPN', 'location': [618, 622]},\n",
       "  {'text': 'Grail', 'part_of_speech': 'PROPN', 'location': [623, 628]},\n",
       "  {'text': '.', 'part_of_speech': 'PUNCT', 'location': [628, 629]}],\n",
       " 'sentences': [{'text': 'In AD 932, King Arthur and his squire, Patsy, travel throughout Britain searching for men to join the Knights of the Round Table.',\n",
       "   'location': [0, 129]},\n",
       "  {'text': \"Along the way, he recruits Sir Bedevere the Wise, Sir Lancelot the Brave, Sir Galahad the Pure, Sir Robin the Not-Quite-So-Brave-as-Sir-Lancelot, and Sir Not-Appearing-in-this-Film, along with their squires and Robin's troubadours.\",\n",
       "   'location': [130, 361]},\n",
       "  {'text': 'Arthur leads the men to Camelot, but upon further consideration (thanks to a musical number) he decides not to go there because it is \"a silly place\".',\n",
       "   'location': [362, 512]},\n",
       "  {'text': 'As they turn away, God (an image of W. G. Grace) speaks to them and gives Arthur the task of finding the Holy Grail.',\n",
       "   'location': [513, 629]}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"syntax\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buried in the above data structure is all the information we need to perform our example task:\n",
    "* The location of every token in the document.\n",
    "* The part of speech of every token in the document.\n",
    "* The location of every sentence in the document.\n",
    "\n",
    "The Python code in the next cell uses this information to construct a list of pronouns\n",
    "in each sentence in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sentence': {'text': 'In AD 932, King Arthur and his squire, Patsy, travel throughout Britain searching for men to join the Knights of the Round Table.',\n",
       "   'location': [0, 129]},\n",
       "  'pronouns': [{'text': 'his',\n",
       "    'part_of_speech': 'PRON',\n",
       "    'location': [27, 30],\n",
       "    'lemma': 'his'}]},\n",
       " {'sentence': {'text': \"Along the way, he recruits Sir Bedevere the Wise, Sir Lancelot the Brave, Sir Galahad the Pure, Sir Robin the Not-Quite-So-Brave-as-Sir-Lancelot, and Sir Not-Appearing-in-this-Film, along with their squires and Robin's troubadours.\",\n",
       "   'location': [130, 361]},\n",
       "  'pronouns': [{'text': 'he',\n",
       "    'part_of_speech': 'PRON',\n",
       "    'location': [145, 147],\n",
       "    'lemma': 'he'},\n",
       "   {'text': 'this',\n",
       "    'part_of_speech': 'PRON',\n",
       "    'location': [301, 305],\n",
       "    'lemma': 'this'},\n",
       "   {'text': 'their',\n",
       "    'part_of_speech': 'PRON',\n",
       "    'location': [323, 328],\n",
       "    'lemma': 'their'}]},\n",
       " {'sentence': {'text': 'Arthur leads the men to Camelot, but upon further consideration (thanks to a musical number) he decides not to go there because it is \"a silly place\".',\n",
       "   'location': [362, 512]},\n",
       "  'pronouns': [{'text': 'he',\n",
       "    'part_of_speech': 'PRON',\n",
       "    'location': [455, 457],\n",
       "    'lemma': 'he'},\n",
       "   {'text': 'it',\n",
       "    'part_of_speech': 'PRON',\n",
       "    'location': [490, 492],\n",
       "    'lemma': 'it'}]},\n",
       " {'sentence': {'text': 'As they turn away, God (an image of W. G. Grace) speaks to them and gives Arthur the task of finding the Holy Grail.',\n",
       "   'location': [513, 629]},\n",
       "  'pronouns': [{'text': 'they',\n",
       "    'part_of_speech': 'PRON',\n",
       "    'location': [516, 520],\n",
       "    'lemma': 'they'},\n",
       "   {'text': 'them',\n",
       "    'part_of_speech': 'PRON',\n",
       "    'location': [572, 576],\n",
       "    'lemma': 'they'}]}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "# Create a data structure to hold a mapping from sentence identifier\n",
    "# to a list of pronouns. This step requires defining sentence ids.\n",
    "def sentence_id(sentence_record: Dict[str, Any]):\n",
    "    return tuple(sentence_record[\"location\"])\n",
    "\n",
    "pronouns_by_sentence_id = collections.defaultdict(list)\n",
    "\n",
    "# Pass 1: Use nested for loops to identify pronouns and match them with \n",
    "#         their containing sentences.\n",
    "# Running time: O(num_tokens * num_sentences), i.e. O(document_size^2)\n",
    "for t in response[\"syntax\"][\"tokens\"]:\n",
    "    pos_str = t[\"part_of_speech\"]  # Decode numeric POS enum\n",
    "    if pos_str == \"PRON\":\n",
    "        found_sentence = False\n",
    "        for s in response[\"syntax\"][\"sentences\"]:\n",
    "            if (t[\"location\"][0] >= s[\"location\"][0] \n",
    "                    and t[\"location\"][1] <= s[\"location\"][1]):\n",
    "                found_sentence = True\n",
    "                pronouns_by_sentence_id[sentence_id(s)].append(t)\n",
    "        if not found_sentence:\n",
    "            raise ValueError(f\"Token {t} is not in any sentence\")\n",
    "            pass  # Make JupyterLab syntax highlighting happy\n",
    "\n",
    "# Pass 2: Translate sentence identifiers to full sentence metadata.\n",
    "sentence_id_to_sentence = {sentence_id(s): s \n",
    "                           for s in response[\"syntax\"][\"sentences\"]}\n",
    "result = [\n",
    "    {\n",
    "        \"sentence\": sentence_id_to_sentence[key],\n",
    "        \"pronouns\": pronouns\n",
    "    }\n",
    "    for key, pronouns in pronouns_by_sentence_id.items()\n",
    "]\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above is quite complex given the simplicity of the task. You would need to stare at the previous cell for a few minutes to convince yourself that the algorithm is correct. This implementation also has scalability issues: The worst-case running time of the nested for loops section is proportional to the square of the document length.\n",
    "\n",
    "We can do better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat the Example Task Using Pandas\n",
    "\n",
    "Let's revisit the example task we just performed in the previous cell. Again, the task is: *Find all the pronouns in each sentence, broken down by sentence.* This time around, let's perform this task using Pandas.\n",
    "\n",
    "Text Extensions for Pandas includes a function `parse_response()` that turns the output of Watson NLU's `analyze()` function into a dictionary of Pandas DataFrames. Let's run our response object through that conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['syntax', 'entities', 'entity_mentions', 'keywords', 'relations', 'semantic_roles'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = tp.io.watson.nlu.parse_response(response)\n",
    "dfs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of each analysis pass that Watson NLU performed is now a DataFrame. \n",
    "Let's look at the DataFrame for the \"syntax\" pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>span</th>\n",
       "      <th>part_of_speech</th>\n",
       "      <th>lemma</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 2): 'In'</td>\n",
       "      <td>ADP</td>\n",
       "      <td>in</td>\n",
       "      <td>[0, 129): 'In AD 932, King Arthur and his squi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[3, 5): 'AD'</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>ad</td>\n",
       "      <td>[0, 129): 'In AD 932, King Arthur and his squi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[6, 9): '932'</td>\n",
       "      <td>NUM</td>\n",
       "      <td>None</td>\n",
       "      <td>[0, 129): 'In AD 932, King Arthur and his squi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[9, 10): ','</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>None</td>\n",
       "      <td>[0, 129): 'In AD 932, King Arthur and his squi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[11, 15): 'King'</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>king</td>\n",
       "      <td>[0, 129): 'In AD 932, King Arthur and his squi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>[606, 613): 'finding'</td>\n",
       "      <td>VERB</td>\n",
       "      <td>find</td>\n",
       "      <td>[513, 629): 'As they turn away, God (an image ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>[614, 617): 'the'</td>\n",
       "      <td>DET</td>\n",
       "      <td>the</td>\n",
       "      <td>[513, 629): 'As they turn away, God (an image ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>[618, 622): 'Holy'</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>None</td>\n",
       "      <td>[513, 629): 'As they turn away, God (an image ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>[623, 628): 'Grail'</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>None</td>\n",
       "      <td>[513, 629): 'As they turn away, God (an image ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>[628, 629): '.'</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>None</td>\n",
       "      <td>[513, 629): 'As they turn away, God (an image ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      span part_of_speech lemma  \\\n",
       "0             [0, 2): 'In'            ADP    in   \n",
       "1             [3, 5): 'AD'           NOUN    ad   \n",
       "2            [6, 9): '932'            NUM  None   \n",
       "3             [9, 10): ','          PUNCT  None   \n",
       "4         [11, 15): 'King'          PROPN  king   \n",
       "..                     ...            ...   ...   \n",
       "142  [606, 613): 'finding'           VERB  find   \n",
       "143      [614, 617): 'the'            DET   the   \n",
       "144     [618, 622): 'Holy'          PROPN  None   \n",
       "145    [623, 628): 'Grail'          PROPN  None   \n",
       "146        [628, 629): '.'          PUNCT  None   \n",
       "\n",
       "                                              sentence  \n",
       "0    [0, 129): 'In AD 932, King Arthur and his squi...  \n",
       "1    [0, 129): 'In AD 932, King Arthur and his squi...  \n",
       "2    [0, 129): 'In AD 932, King Arthur and his squi...  \n",
       "3    [0, 129): 'In AD 932, King Arthur and his squi...  \n",
       "4    [0, 129): 'In AD 932, King Arthur and his squi...  \n",
       "..                                                 ...  \n",
       "142  [513, 629): 'As they turn away, God (an image ...  \n",
       "143  [513, 629): 'As they turn away, God (an image ...  \n",
       "144  [513, 629): 'As they turn away, God (an image ...  \n",
       "145  [513, 629): 'As they turn away, God (an image ...  \n",
       "146  [513, 629): 'As they turn away, God (an image ...  \n",
       "\n",
       "[147 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syntax_df = dfs[\"syntax\"]\n",
    "syntax_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrame has one row for every token in the document. Each row has information on\n",
    "the span of the token, its part of speech, its lemmatized form, and the span of the \n",
    "containing sentence.\n",
    "\n",
    "Let's use this DataFrame to perform our example task a second time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>span</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0, 129): 'In AD 932, King Arthur and his squi...</td>\n",
       "      <td>[27, 30): 'his'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[130, 361): 'Along the way, he recruits Sir Be...</td>\n",
       "      <td>[145, 147): 'he'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>[130, 361): 'Along the way, he recruits Sir Be...</td>\n",
       "      <td>[301, 305): 'this'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>[130, 361): 'Along the way, he recruits Sir Be...</td>\n",
       "      <td>[323, 328): 'their'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>[362, 512): 'Arthur leads the men to Camelot, ...</td>\n",
       "      <td>[455, 457): 'he'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>[362, 512): 'Arthur leads the men to Camelot, ...</td>\n",
       "      <td>[490, 492): 'it'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>[513, 629): 'As they turn away, God (an image ...</td>\n",
       "      <td>[516, 520): 'they'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>[513, 629): 'As they turn away, God (an image ...</td>\n",
       "      <td>[572, 576): 'them'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence                 span\n",
       "7    [0, 129): 'In AD 932, King Arthur and his squi...      [27, 30): 'his'\n",
       "31   [130, 361): 'Along the way, he recruits Sir Be...     [145, 147): 'he'\n",
       "73   [130, 361): 'Along the way, he recruits Sir Be...   [301, 305): 'this'\n",
       "79   [130, 361): 'Along the way, he recruits Sir Be...  [323, 328): 'their'\n",
       "104  [362, 512): 'Arthur leads the men to Camelot, ...     [455, 457): 'he'\n",
       "111  [362, 512): 'Arthur leads the men to Camelot, ...     [490, 492): 'it'\n",
       "120  [513, 629): 'As they turn away, God (an image ...   [516, 520): 'they'\n",
       "135  [513, 629): 'As they turn away, God (an image ...   [572, 576): 'them'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pronouns_by_sentence = syntax_df[syntax_df[\"part_of_speech\"] == \"PRON\"][[\"sentence\", \"span\"]]\n",
    "pronouns_by_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. With the DataFrame version of this data, we can perform our example task with **one line of code**.\n",
    "\n",
    "Specifically, we use a Pandas selection condition to filter out the tokens that aren't pronouns, and then then we \n",
    "project down to the columns containing sentence and token spans. The result is another DataFrame that \n",
    "we can display directly in our Jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How it Works\n",
    "\n",
    "\n",
    "Let's take a moment to drill into the internals of the DataFrames we just used.\n",
    "For reference, here are the first three rows of the syntax analysis DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>span</th>\n",
       "      <th>part_of_speech</th>\n",
       "      <th>lemma</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 2): 'In'</td>\n",
       "      <td>ADP</td>\n",
       "      <td>in</td>\n",
       "      <td>[0, 129): 'In AD 932, King Arthur and his squi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[3, 5): 'AD'</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>ad</td>\n",
       "      <td>[0, 129): 'In AD 932, King Arthur and his squi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[6, 9): '932'</td>\n",
       "      <td>NUM</td>\n",
       "      <td>None</td>\n",
       "      <td>[0, 129): 'In AD 932, King Arthur and his squi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            span part_of_speech lemma  \\\n",
       "0   [0, 2): 'In'            ADP    in   \n",
       "1   [3, 5): 'AD'           NOUN    ad   \n",
       "2  [6, 9): '932'            NUM  None   \n",
       "\n",
       "                                            sentence  \n",
       "0  [0, 129): 'In AD 932, King Arthur and his squi...  \n",
       "1  [0, 129): 'In AD 932, King Arthur and his squi...  \n",
       "2  [0, 129): 'In AD 932, King Arthur and his squi...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syntax_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is that DataFrame's data type information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "span                   SpanDtype\n",
       "part_of_speech            object\n",
       "lemma                     object\n",
       "sentence          TokenSpanDtype\n",
       "dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syntax_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two of the columns in this DataFrame &mdash; \"span\" and \"sentence\" &mdash; contain\n",
    "extension types from the Text Extensions for Pandas library. Let's look first at the \"span\"\n",
    "column. \n",
    "\n",
    "The \"span\" column is stored internally using the class `SpanArray` from \n",
    "Text Extensions for Pandas.\n",
    "`SpanArray` is a subclass of \n",
    "[`ExtensionArray`](\n",
    "    https://pandas.pydata.org/docs/reference/api/pandas.api.extensions.ExtensionArray.html), \n",
    "the base class for custom 1-D array types in Pandas.\n",
    "\n",
    "You can use the property [`pandas.Series.array`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.array.html) to access the `ExtensionArray` behind any Pandas extension type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SpanArray>\n",
      "[         [0, 2): 'In',          [3, 5): 'AD',         [6, 9): '932',\n",
      "          [9, 10): ',',      [11, 15): 'King',    [16, 22): 'Arthur',\n",
      "       [23, 26): 'and',       [27, 30): 'his',    [31, 37): 'squire',\n",
      "         [37, 38): ',',\n",
      " ...\n",
      "   [581, 586): 'gives',  [587, 593): 'Arthur',     [594, 597): 'the',\n",
      "    [598, 602): 'task',      [603, 605): 'of', [606, 613): 'finding',\n",
      "     [614, 617): 'the',    [618, 622): 'Holy',   [623, 628): 'Grail',\n",
      "       [628, 629): '.']\n",
      "Length: 147, dtype: SpanDtype\n"
     ]
    }
   ],
   "source": [
    "print(syntax_df[\"span\"].array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internally, a `SpanArray` is stored as Numpy arrays of begin and end offsets, plus a Python string \n",
    "containing the target text. You can access this internal data as properties if your application needs that\n",
    "information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  3,  6,  9, 11, 16, 23, 27, 31, 37]),\n",
       " array([ 2,  5,  9, 10, 15, 22, 26, 30, 37, 38]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syntax_df[\"span\"].array.begin[:10], syntax_df[\"span\"].array.end[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also convert an individual element of the array into a Python object of type `Span`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"[0, 2): 'In'\" is an object of type <class 'text_extensions_for_pandas.array.span.Span'>\n"
     ]
    }
   ],
   "source": [
    "span_obj = syntax_df[\"span\"].array[0]\n",
    "print(f\"\\\"{span_obj}\\\" is an object of type {type(span_obj)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can convert the entire array (or a slice of it) into Python objects, one object per span:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2): 'In', [3, 5): 'AD', [6, 9): '932', [9, 10): ',',\n",
       "       [11, 15): 'King', [16, 22): 'Arthur', [23, 26): 'and',\n",
       "       [27, 30): 'his', [31, 37): 'squire', [37, 38): ','], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syntax_df[\"span\"].iloc[:10].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `SpanArray` can also render itself using [Jupyter Notebook callbacks](https://ipython.readthedocs.io/en/stable/config/integrating.html). To\n",
    "see the HTML representation of the `SpanArray`, pass the array object\n",
    "to Jupyter's [`display()`](https://ipython.readthedocs.io/en/stable/api/generated/IPython.display.html#IPython.display.display)\n",
    "function; or make that object be the last line of the cell, as in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div id=\"spanArray\">\n",
       "            <div id=\"spans\" \n",
       "             style=\"background-color:#F0F0F0; border: 1px solid #E0E0E0; float:left; padding:10px;\">\n",
       "                <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>begin</th>\n",
       "      <th>end</th>\n",
       "      <th>covered_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>In</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>AD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>King</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16</td>\n",
       "      <td>22</td>\n",
       "      <td>Arthur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>23</td>\n",
       "      <td>26</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>27</td>\n",
       "      <td>30</td>\n",
       "      <td>his</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>31</td>\n",
       "      <td>37</td>\n",
       "      <td>squire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>37</td>\n",
       "      <td>38</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "            </div>\n",
       "            <div id=\"text\"\n",
       "             style=\"float:right; background-color:#F5F5F5; border: 1px solid #E0E0E0; width: 60%;\">\n",
       "                <div style=\"float:center; padding:10px\">\n",
       "                    <p style=\"font-family:monospace\">\n",
       "                        <span style=\"background-color:yellow\">In</span> <span style=\"background-color:yellow\">AD</span> <span style=\"background-color:yellow\">932,</span> <span style=\"background-color:yellow\">King</span> <span style=\"background-color:yellow\">Arthur</span> <span style=\"background-color:yellow\">and</span> <span style=\"background-color:yellow\">his</span> <span style=\"background-color:yellow\">squire,</span> Patsy, travel throughout Britain searching for men to join the Knights of the Round Table. Along the way, he recruits Sir Bedevere the Wise, Sir Lancelot the Brave, Sir Galahad the Pure, Sir Robin the Not-Quite-So-Brave-as-Sir-Lancelot, and Sir Not-Appearing-in-this-Film, along with their squires and Robin&#39;s troubadours. Arthur leads the men to Camelot, but upon further consideration (thanks to a musical number) he decides not to go there because it is &quot;a silly place&quot;. As they turn away, God (an image of W. G. Grace) speaks to them and gives Arthur the task of finding the Holy Grail.\n",
       "                    </p>\n",
       "                </div>\n",
       "            </div>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SpanArray>\n",
       "[      [0, 2): 'In',       [3, 5): 'AD',      [6, 9): '932',\n",
       "       [9, 10): ',',   [11, 15): 'King', [16, 22): 'Arthur',\n",
       "    [23, 26): 'and',    [27, 30): 'his', [31, 37): 'squire',\n",
       "      [37, 38): ',']\n",
       "Length: 10, dtype: SpanDtype"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the first 10 tokens in context\n",
    "syntax_df[\"span\"].iloc[:10].array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take another look at our DataFrame of syntax information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>span</th>\n",
       "      <th>part_of_speech</th>\n",
       "      <th>lemma</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 2): 'In'</td>\n",
       "      <td>ADP</td>\n",
       "      <td>in</td>\n",
       "      <td>[0, 129): 'In AD 932, King Arthur and his squi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[3, 5): 'AD'</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>ad</td>\n",
       "      <td>[0, 129): 'In AD 932, King Arthur and his squi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[6, 9): '932'</td>\n",
       "      <td>NUM</td>\n",
       "      <td>None</td>\n",
       "      <td>[0, 129): 'In AD 932, King Arthur and his squi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            span part_of_speech lemma  \\\n",
       "0   [0, 2): 'In'            ADP    in   \n",
       "1   [3, 5): 'AD'           NOUN    ad   \n",
       "2  [6, 9): '932'            NUM  None   \n",
       "\n",
       "                                            sentence  \n",
       "0  [0, 129): 'In AD 932, King Arthur and his squi...  \n",
       "1  [0, 129): 'In AD 932, King Arthur and his squi...  \n",
       "2  [0, 129): 'In AD 932, King Arthur and his squi...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syntax_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"sentence\" column is backed by an object of type `TokenSpanArray`.\n",
    "`TokenSpanArray`, another extension type from Text Extensions for Pandas,\n",
    "is a version of `SpanArray` for representing a set of spans that are \n",
    "constrained to begin and end on token boundaries. In addition to all the\n",
    "functionality of a `SpanArray`, a `TokenSpanArray` encodes additional \n",
    "information about the relationships between its spans and a tokenization\n",
    "of the document.\n",
    "\n",
    "Here are the distinct elements of the \"sentence\" column rendered as HTML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div id=\"spanArray\">\n",
       "            <div id=\"spans\" \n",
       "             style=\"background-color:#F0F0F0; border: 1px solid #E0E0E0; float:left; padding:10px;\">\n",
       "                <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>begin</th>\n",
       "      <th>end</th>\n",
       "      <th>begin_token</th>\n",
       "      <th>end_token</th>\n",
       "      <th>covered_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>129</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>In AD 932, King Arthur and his squire, Patsy, travel throughout Britain searching for men to join the Knights of the Round Table.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>130</td>\n",
       "      <td>361</td>\n",
       "      <td>27</td>\n",
       "      <td>86</td>\n",
       "      <td>Along the way, he recruits Sir Bedevere the Wise, Sir Lancelot the Brave, Sir Galahad the Pure, Sir Robin the Not-Quite-So-Brave-as-Sir-Lancelot, and Sir Not-Appearing-in-this-Film, along with their squires and Robin's troubadours.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>362</td>\n",
       "      <td>512</td>\n",
       "      <td>86</td>\n",
       "      <td>119</td>\n",
       "      <td>Arthur leads the men to Camelot, but upon further consideration (thanks to a musical number) he decides not to go there because it is \"a silly place\".</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>513</td>\n",
       "      <td>629</td>\n",
       "      <td>119</td>\n",
       "      <td>147</td>\n",
       "      <td>As they turn away, God (an image of W. G. Grace) speaks to them and gives Arthur the task of finding the Holy Grail.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "            </div>\n",
       "            <div id=\"text\"\n",
       "             style=\"float:right; background-color:#F5F5F5; border: 1px solid #E0E0E0; width: 60%;\">\n",
       "                <div style=\"float:center; padding:10px\">\n",
       "                    <p style=\"font-family:monospace\">\n",
       "                        <span style=\"background-color:yellow\">In AD 932, King Arthur and his squire, Patsy, travel throughout Britain searching for men to join the Knights of the Round Table.</span> <span style=\"background-color:yellow\">Along the way, he recruits Sir Bedevere the Wise, Sir Lancelot the Brave, Sir Galahad the Pure, Sir Robin the Not-Quite-So-Brave-as-Sir-Lancelot, and Sir Not-Appearing-in-this-Film, along with their squires and Robin&#39;s troubadours.</span> <span style=\"background-color:yellow\">Arthur leads the men to Camelot, but upon further consideration (thanks to a musical number) he decides not to go there because it is &quot;a silly place&quot;.</span> <span style=\"background-color:yellow\">As they turn away, God (an image of W. G. Grace) speaks to them and gives Arthur the task of finding the Holy Grail.\n",
       "                    </p>\n",
       "                </div>\n",
       "            </div>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<TokenSpanArray>\n",
       "[     [0, 129): 'In AD 932, King Arthur and his squire, Patsy, travel throughout Britain [...]',\n",
       "  [130, 361): 'Along the way, he recruits Sir Bedevere the Wise, Sir Lancelot the Brave, [...]',\n",
       " [362, 512): 'Arthur leads the men to Camelot, but upon further consideration (thanks to [...]',\n",
       "  [513, 629): 'As they turn away, God (an image of W. G. Grace) speaks to them and gives [...]']\n",
       "Length: 4, dtype: TokenSpanDtype"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syntax_df[\"sentence\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the table in the previous cell's output shows, each span in the `TokenSpanArray` has begin and end offsets in terms \n",
    "of both characters and tokens. Internally, the `TokenSpanArray` is stored as follows:\n",
    "* A Numpy array of begin offsets, measured in tokens\n",
    "* A Numpy array of end offsets in tokens\n",
    "* A reference to a `SpanArray` of spans representing the tokens\n",
    "\n",
    "The `TokenSpanArray` object computes the character offsets and covered text of its spans on demand.\n",
    "\n",
    "Applications can access the internals of a `TokenSpanArray` via the properties `begin_token`, `end_token`, and `tokens`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Offset information (stored in the TokenSpanArray):\n",
      "`begin_token` property: [  0  27  86 119]\n",
      "  `end_token` property: [ 27  86 119 147]\n",
      "   \n",
      "Token information (`tokens` property, shared among mulitple TokenSpanArrays):\n",
      "<SpanArray>\n",
      "[         [0, 2): 'In',          [3, 5): 'AD',         [6, 9): '932',\n",
      "          [9, 10): ',',      [11, 15): 'King',    [16, 22): 'Arthur',\n",
      "       [23, 26): 'and',       [27, 30): 'his',    [31, 37): 'squire',\n",
      "         [37, 38): ',',\n",
      " ...\n",
      "   [581, 586): 'gives',  [587, 593): 'Arthur',     [594, 597): 'the',\n",
      "    [598, 602): 'task',      [603, 605): 'of', [606, 613): 'finding',\n",
      "     [614, 617): 'the',    [618, 622): 'Holy',   [623, 628): 'Grail',\n",
      "       [628, 629): '.']\n",
      "Length: 147, dtype: SpanDtype\n",
      "\n"
     ]
    }
   ],
   "source": [
    "token_span_array = syntax_df[\"sentence\"].unique()\n",
    "print(f\"\"\"\n",
    "Offset information (stored in the TokenSpanArray):\n",
    "`begin_token` property: {token_span_array.begin_token}\n",
    "  `end_token` property: {token_span_array.end_token}\n",
    "   \n",
    "Token information (`tokens` property, shared among mulitple TokenSpanArrays):\n",
    "{token_span_array.tokens}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extension types in Text Extensions for Pandas support the full set of Pandas array operations. For example, we can build up a DataFrame of the spans of all sentences in the document by applying `pandas.DataFrame.drop_duplicates()` to the `sentence` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 129): 'In AD 932, King Arthur and his squi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[130, 361): 'Along the way, he recruits Sir Be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>[362, 512): 'Arthur leads the men to Camelot, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>[513, 629): 'As they turn away, God (an image ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence\n",
       "0    [0, 129): 'In AD 932, King Arthur and his squi...\n",
       "27   [130, 361): 'Along the way, he recruits Sir Be...\n",
       "86   [362, 512): 'Arthur leads the men to Camelot, ...\n",
       "119  [513, 629): 'As they turn away, God (an image ..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syntax_df[[\"sentence\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A More Complex Example\n",
    "\n",
    "Now that we've had an introduction to the Text Extensions for Pandas span types, let's take another\n",
    "look at the DataFrame that our \"find pronouns by sentence\" code produced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>span</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0, 129): 'In AD 932, King Arthur and his squi...</td>\n",
       "      <td>[27, 30): 'his'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[130, 361): 'Along the way, he recruits Sir Be...</td>\n",
       "      <td>[145, 147): 'he'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>[130, 361): 'Along the way, he recruits Sir Be...</td>\n",
       "      <td>[301, 305): 'this'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>[130, 361): 'Along the way, he recruits Sir Be...</td>\n",
       "      <td>[323, 328): 'their'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>[362, 512): 'Arthur leads the men to Camelot, ...</td>\n",
       "      <td>[455, 457): 'he'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>[362, 512): 'Arthur leads the men to Camelot, ...</td>\n",
       "      <td>[490, 492): 'it'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>[513, 629): 'As they turn away, God (an image ...</td>\n",
       "      <td>[516, 520): 'they'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>[513, 629): 'As they turn away, God (an image ...</td>\n",
       "      <td>[572, 576): 'them'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence                 span\n",
       "7    [0, 129): 'In AD 932, King Arthur and his squi...      [27, 30): 'his'\n",
       "31   [130, 361): 'Along the way, he recruits Sir Be...     [145, 147): 'he'\n",
       "73   [130, 361): 'Along the way, he recruits Sir Be...   [301, 305): 'this'\n",
       "79   [130, 361): 'Along the way, he recruits Sir Be...  [323, 328): 'their'\n",
       "104  [362, 512): 'Arthur leads the men to Camelot, ...     [455, 457): 'he'\n",
       "111  [362, 512): 'Arthur leads the men to Camelot, ...     [490, 492): 'it'\n",
       "120  [513, 629): 'As they turn away, God (an image ...   [516, 520): 'they'\n",
       "135  [513, 629): 'As they turn away, God (an image ...   [572, 576): 'them'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pronouns_by_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This DataFrame contains two columns backed by Text Extensions for Pandas span types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence    TokenSpanDtype\n",
       "span             SpanDtype\n",
       "dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pronouns_by_sentence.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That means that we can use the full power of Pandas' high-level operations on this DataFrame. \n",
    "Let's use the output of our earlier task to build up a more complex task: \n",
    "*Highlight all pronouns in sentences containing the word \"Arthur\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div id=\"spanArray\">\n",
       "            <div id=\"spans\" \n",
       "             style=\"background-color:#F0F0F0; border: 1px solid #E0E0E0; float:left; padding:10px;\">\n",
       "                <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>begin</th>\n",
       "      <th>end</th>\n",
       "      <th>covered_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27</td>\n",
       "      <td>30</td>\n",
       "      <td>his</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>455</td>\n",
       "      <td>457</td>\n",
       "      <td>he</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>490</td>\n",
       "      <td>492</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>516</td>\n",
       "      <td>520</td>\n",
       "      <td>they</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>572</td>\n",
       "      <td>576</td>\n",
       "      <td>them</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "            </div>\n",
       "            <div id=\"text\"\n",
       "             style=\"float:right; background-color:#F5F5F5; border: 1px solid #E0E0E0; width: 60%;\">\n",
       "                <div style=\"float:center; padding:10px\">\n",
       "                    <p style=\"font-family:monospace\">\n",
       "                        In AD 932, King Arthur and <span style=\"background-color:yellow\">his</span> squire, Patsy, travel throughout Britain searching for men to join the Knights of the Round Table. Along the way, he recruits Sir Bedevere the Wise, Sir Lancelot the Brave, Sir Galahad the Pure, Sir Robin the Not-Quite-So-Brave-as-Sir-Lancelot, and Sir Not-Appearing-in-this-Film, along with their squires and Robin&#39;s troubadours. Arthur leads the men to Camelot, but upon further consideration (thanks to a musical number) <span style=\"background-color:yellow\">he</span> decides not to go there because <span style=\"background-color:yellow\">it</span> is &quot;a silly place&quot;. As <span style=\"background-color:yellow\">they</span> turn away, God (an image of W. G. Grace) speaks to <span style=\"background-color:yellow\">them</span> and gives Arthur the task of finding the Holy Grail.\n",
       "                    </p>\n",
       "                </div>\n",
       "            </div>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SpanArray>\n",
       "[   [27, 30): 'his',   [455, 457): 'he',   [490, 492): 'it',\n",
       " [516, 520): 'they', [572, 576): 'them']\n",
       "Length: 5, dtype: SpanDtype"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = pronouns_by_sentence[\"sentence\"].map(lambda s: s.covered_text).str.contains(\"Arthur\")\n",
    "pronouns_by_sentence[\"span\"][mask].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's another variation: *Pair each instance of the word \"Arthur\" with the pronouns that occur in the same sentence.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arthur_span</th>\n",
       "      <th>pronoun_span</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[16, 22): 'Arthur'</td>\n",
       "      <td>[27, 30): 'his'</td>\n",
       "      <td>[0, 129): 'In AD 932, King Arthur and his squi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[362, 368): 'Arthur'</td>\n",
       "      <td>[455, 457): 'he'</td>\n",
       "      <td>[362, 512): 'Arthur leads the men to Camelot, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[362, 368): 'Arthur'</td>\n",
       "      <td>[490, 492): 'it'</td>\n",
       "      <td>[362, 512): 'Arthur leads the men to Camelot, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[587, 593): 'Arthur'</td>\n",
       "      <td>[516, 520): 'they'</td>\n",
       "      <td>[513, 629): 'As they turn away, God (an image ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[587, 593): 'Arthur'</td>\n",
       "      <td>[572, 576): 'them'</td>\n",
       "      <td>[513, 629): 'As they turn away, God (an image ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            arthur_span        pronoun_span  \\\n",
       "0    [16, 22): 'Arthur'     [27, 30): 'his'   \n",
       "1  [362, 368): 'Arthur'    [455, 457): 'he'   \n",
       "2  [362, 368): 'Arthur'    [490, 492): 'it'   \n",
       "3  [587, 593): 'Arthur'  [516, 520): 'they'   \n",
       "4  [587, 593): 'Arthur'  [572, 576): 'them'   \n",
       "\n",
       "                                            sentence  \n",
       "0  [0, 129): 'In AD 932, King Arthur and his squi...  \n",
       "1  [362, 512): 'Arthur leads the men to Camelot, ...  \n",
       "2  [362, 512): 'Arthur leads the men to Camelot, ...  \n",
       "3  [513, 629): 'As they turn away, God (an image ...  \n",
       "4  [513, 629): 'As they turn away, God (an image ...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    syntax_df[syntax_df[\"span\"].array.covered_text == \"Arthur\"]  # Find instances of \"Arthur\"\n",
    "    .merge(pronouns_by_sentence, on=\"sentence\")  # Match with pronouns in the same sentence\n",
    "    .rename(columns={\"span_x\": \"arthur_span\", \"span_y\": \"pronoun_span\"})\n",
    "    [[\"arthur_span\", \"pronoun_span\", \"sentence\"]]  # Reorder columns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Outputs of Watson NLU as DataFrames \n",
    "\n",
    "The examples so far have used the DataFrame representation of Watson Natural Language Understanding's syntax analysis.\n",
    "In addition to syntax analysis, Watson NLU can perform several other types of analysis. Let's take a look at the \n",
    "DataFrames that Text Extensions for Pandas can produce from the output of Watson NLU.\n",
    "\n",
    "We'll start by revisiting the results of our earlier code that ran \n",
    "```python\n",
    "dfs = tp.io.watson.nlu.parse_response(response)\n",
    "```\n",
    "over the `response` object that the Watson NLU's Python API returned. `dfs` is a dictionary of DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['syntax', 'entities', 'entity_mentions', 'keywords', 'relations', 'semantic_roles'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"syntax\" element of `dfs` contains the syntax analysis DataFrame that we showed earlier.\n",
    "Let's take a look at the other elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"entities\" element of `dfs` contains the named entities that Watson Natural Language \n",
    "Understanding found in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment.label</th>\n",
       "      <th>sentiment.score</th>\n",
       "      <th>relevance</th>\n",
       "      <th>count</th>\n",
       "      <th>confidence</th>\n",
       "      <th>disambiguation.subtype</th>\n",
       "      <th>disambiguation.name</th>\n",
       "      <th>disambiguation.dbpedia_resource</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Person</td>\n",
       "      <td>Sir Bedevere</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.835873</td>\n",
       "      <td>0.950560</td>\n",
       "      <td>1</td>\n",
       "      <td>0.982315</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Person</td>\n",
       "      <td>King Arthur</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.720381</td>\n",
       "      <td>1</td>\n",
       "      <td>0.924937</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Person</td>\n",
       "      <td>Patsy</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.679300</td>\n",
       "      <td>1</td>\n",
       "      <td>0.830596</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Person</td>\n",
       "      <td>Sir Lancelot</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.835873</td>\n",
       "      <td>0.662902</td>\n",
       "      <td>1</td>\n",
       "      <td>0.956371</td>\n",
       "      <td>[MusicalArtist, TVActor]</td>\n",
       "      <td>Sir_Lancelot_%28singer%29</td>\n",
       "      <td>http://dbpedia.org/resource/Sir_Lancelot_%28si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Person</td>\n",
       "      <td>Sir Galahad</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.835873</td>\n",
       "      <td>0.654170</td>\n",
       "      <td>1</td>\n",
       "      <td>0.948409</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     type          text sentiment.label  sentiment.score  relevance  count  \\\n",
       "0  Person  Sir Bedevere        positive         0.835873   0.950560      1   \n",
       "1  Person   King Arthur         neutral         0.000000   0.720381      1   \n",
       "2  Person         Patsy         neutral         0.000000   0.679300      1   \n",
       "3  Person  Sir Lancelot        positive         0.835873   0.662902      1   \n",
       "4  Person   Sir Galahad        positive         0.835873   0.654170      1   \n",
       "\n",
       "   confidence    disambiguation.subtype        disambiguation.name  \\\n",
       "0    0.982315                      None                       None   \n",
       "1    0.924937                      None                       None   \n",
       "2    0.830596                      None                       None   \n",
       "3    0.956371  [MusicalArtist, TVActor]  Sir_Lancelot_%28singer%29   \n",
       "4    0.948409                      None                       None   \n",
       "\n",
       "                     disambiguation.dbpedia_resource  \n",
       "0                                               None  \n",
       "1                                               None  \n",
       "2                                               None  \n",
       "3  http://dbpedia.org/resource/Sir_Lancelot_%28si...  \n",
       "4                                               None  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[\"entities\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"entity_mentions\" element of `dfs` contains the locations of individual mentions of\n",
    "entities from the \"entities\" DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "      <th>span</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Person</td>\n",
       "      <td>Sir Bedevere</td>\n",
       "      <td>[157, 169): 'Sir Bedevere'</td>\n",
       "      <td>0.982315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Person</td>\n",
       "      <td>King Arthur</td>\n",
       "      <td>[11, 22): 'King Arthur'</td>\n",
       "      <td>0.924937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Person</td>\n",
       "      <td>Patsy</td>\n",
       "      <td>[39, 44): 'Patsy'</td>\n",
       "      <td>0.830596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Person</td>\n",
       "      <td>Sir Lancelot</td>\n",
       "      <td>[180, 192): 'Sir Lancelot'</td>\n",
       "      <td>0.956371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Person</td>\n",
       "      <td>Sir Galahad</td>\n",
       "      <td>[204, 215): 'Sir Galahad'</td>\n",
       "      <td>0.948409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     type          text                        span  confidence\n",
       "0  Person  Sir Bedevere  [157, 169): 'Sir Bedevere'    0.982315\n",
       "1  Person   King Arthur     [11, 22): 'King Arthur'    0.924937\n",
       "2  Person         Patsy           [39, 44): 'Patsy'    0.830596\n",
       "3  Person  Sir Lancelot  [180, 192): 'Sir Lancelot'    0.956371\n",
       "4  Person   Sir Galahad   [204, 215): 'Sir Galahad'    0.948409"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[\"entity_mentions\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the DataFrame under \"entitiy_mentions\" may contain multiple mentions of the same\n",
    "name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "      <th>span</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Person</td>\n",
       "      <td>Arthur</td>\n",
       "      <td>[362, 368): 'Arthur'</td>\n",
       "      <td>0.996876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Person</td>\n",
       "      <td>Arthur</td>\n",
       "      <td>[587, 593): 'Arthur'</td>\n",
       "      <td>0.973795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      type    text                  span  confidence\n",
       "10  Person  Arthur  [362, 368): 'Arthur'    0.996876\n",
       "11  Person  Arthur  [587, 593): 'Arthur'    0.973795"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arthur_mentions = dfs[\"entity_mentions\"][dfs[\"entity_mentions\"][\"text\"] == \"Arthur\"]\n",
    "arthur_mentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"type\" and \"text\" columns of the \"entity_mentions\" DataFrame refer back to the \n",
    "\"entities\" DataFrame columns of the same names.\n",
    "You can combine the global and local information about entities into a single DataFrame\n",
    "using Pandas' `DataFrame.merge()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "      <th>span</th>\n",
       "      <th>confidence_mention</th>\n",
       "      <th>sentiment.label</th>\n",
       "      <th>sentiment.score</th>\n",
       "      <th>relevance</th>\n",
       "      <th>count</th>\n",
       "      <th>confidence_entity</th>\n",
       "      <th>disambiguation.subtype</th>\n",
       "      <th>disambiguation.name</th>\n",
       "      <th>disambiguation.dbpedia_resource</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Person</td>\n",
       "      <td>Arthur</td>\n",
       "      <td>[362, 368): 'Arthur'</td>\n",
       "      <td>0.996876</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.721918</td>\n",
       "      <td>0.311653</td>\n",
       "      <td>2</td>\n",
       "      <td>0.999918</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Person</td>\n",
       "      <td>Arthur</td>\n",
       "      <td>[587, 593): 'Arthur'</td>\n",
       "      <td>0.973795</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.721918</td>\n",
       "      <td>0.311653</td>\n",
       "      <td>2</td>\n",
       "      <td>0.999918</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     type    text                  span  confidence_mention sentiment.label  \\\n",
       "0  Person  Arthur  [362, 368): 'Arthur'            0.996876        positive   \n",
       "1  Person  Arthur  [587, 593): 'Arthur'            0.973795        positive   \n",
       "\n",
       "   sentiment.score  relevance  count  confidence_entity  \\\n",
       "0         0.721918   0.311653      2           0.999918   \n",
       "1         0.721918   0.311653      2           0.999918   \n",
       "\n",
       "  disambiguation.subtype disambiguation.name disambiguation.dbpedia_resource  \n",
       "0                   None                None                            None  \n",
       "1                   None                None                            None  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arthur_mentions.merge(dfs[\"entities\"], on=[\"type\", \"text\"], suffixes=[\"_mention\", \"_entity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment.label</th>\n",
       "      <th>sentiment.score</th>\n",
       "      <th>relevance</th>\n",
       "      <th>emotion.sadness</th>\n",
       "      <th>emotion.joy</th>\n",
       "      <th>emotion.fear</th>\n",
       "      <th>emotion.disgust</th>\n",
       "      <th>emotion.anger</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>King Arthur</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.864073</td>\n",
       "      <td>0.062558</td>\n",
       "      <td>0.620066</td>\n",
       "      <td>0.054894</td>\n",
       "      <td>0.088147</td>\n",
       "      <td>0.182329</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sir Lancelot</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.835873</td>\n",
       "      <td>0.820529</td>\n",
       "      <td>0.046902</td>\n",
       "      <td>0.810654</td>\n",
       "      <td>0.016340</td>\n",
       "      <td>0.095661</td>\n",
       "      <td>0.021033</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>image of W. G. Grace</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.721918</td>\n",
       "      <td>0.758922</td>\n",
       "      <td>0.047242</td>\n",
       "      <td>0.614332</td>\n",
       "      <td>0.159497</td>\n",
       "      <td>0.040378</td>\n",
       "      <td>0.155298</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sir Galahad</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.835873</td>\n",
       "      <td>0.635859</td>\n",
       "      <td>0.046902</td>\n",
       "      <td>0.810654</td>\n",
       "      <td>0.016340</td>\n",
       "      <td>0.095661</td>\n",
       "      <td>0.021033</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>musical number</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.628727</td>\n",
       "      <td>0.057009</td>\n",
       "      <td>0.365688</td>\n",
       "      <td>0.054963</td>\n",
       "      <td>0.142391</td>\n",
       "      <td>0.054547</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   text sentiment.label  sentiment.score  relevance  \\\n",
       "0           King Arthur         neutral         0.000000   0.864073   \n",
       "1          Sir Lancelot        positive         0.835873   0.820529   \n",
       "2  image of W. G. Grace        positive         0.721918   0.758922   \n",
       "3           Sir Galahad        positive         0.835873   0.635859   \n",
       "4        musical number         neutral         0.000000   0.628727   \n",
       "\n",
       "   emotion.sadness  emotion.joy  emotion.fear  emotion.disgust  emotion.anger  \\\n",
       "0         0.062558     0.620066      0.054894         0.088147       0.182329   \n",
       "1         0.046902     0.810654      0.016340         0.095661       0.021033   \n",
       "2         0.047242     0.614332      0.159497         0.040378       0.155298   \n",
       "3         0.046902     0.810654      0.016340         0.095661       0.021033   \n",
       "4         0.057009     0.365688      0.054963         0.142391       0.054547   \n",
       "\n",
       "   count  \n",
       "0      1  \n",
       "1      1  \n",
       "2      1  \n",
       "3      1  \n",
       "4      1  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[\"keywords\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>sentence_span</th>\n",
       "      <th>score</th>\n",
       "      <th>arguments.0.span</th>\n",
       "      <th>arguments.1.span</th>\n",
       "      <th>arguments.0.entities.type</th>\n",
       "      <th>arguments.1.entities.type</th>\n",
       "      <th>arguments.0.entities.text</th>\n",
       "      <th>arguments.1.entities.text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>partOfMany</td>\n",
       "      <td>[130, 361): 'Along the way, he recruits Sir Be...</td>\n",
       "      <td>0.610221</td>\n",
       "      <td>[208, 215): 'Galahad'</td>\n",
       "      <td>[323, 328): 'their'</td>\n",
       "      <td>Person</td>\n",
       "      <td>Person</td>\n",
       "      <td>Galahad</td>\n",
       "      <td>their</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>partOfMany</td>\n",
       "      <td>[130, 361): 'Along the way, he recruits Sir Be...</td>\n",
       "      <td>0.710112</td>\n",
       "      <td>[266, 274): 'Lancelot'</td>\n",
       "      <td>[323, 328): 'their'</td>\n",
       "      <td>Person</td>\n",
       "      <td>Person</td>\n",
       "      <td>Lancelot</td>\n",
       "      <td>their</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>parentOf</td>\n",
       "      <td>[130, 361): 'Along the way, he recruits Sir Be...</td>\n",
       "      <td>0.382100</td>\n",
       "      <td>[323, 328): 'their'</td>\n",
       "      <td>[329, 336): 'squires'</td>\n",
       "      <td>Person</td>\n",
       "      <td>Person</td>\n",
       "      <td>their</td>\n",
       "      <td>squires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>residesIn</td>\n",
       "      <td>[362, 512): 'Arthur leads the men to Camelot, ...</td>\n",
       "      <td>0.492869</td>\n",
       "      <td>[362, 368): 'Arthur'</td>\n",
       "      <td>[386, 393): 'Camelot'</td>\n",
       "      <td>Person</td>\n",
       "      <td>GeopoliticalEntity</td>\n",
       "      <td>King Arthur</td>\n",
       "      <td>Camelot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>locatedAt</td>\n",
       "      <td>[362, 512): 'Arthur leads the men to Camelot, ...</td>\n",
       "      <td>0.339446</td>\n",
       "      <td>[379, 382): 'men'</td>\n",
       "      <td>[386, 393): 'Camelot'</td>\n",
       "      <td>Person</td>\n",
       "      <td>GeopoliticalEntity</td>\n",
       "      <td>men</td>\n",
       "      <td>Camelot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         type                                      sentence_span     score  \\\n",
       "0  partOfMany  [130, 361): 'Along the way, he recruits Sir Be...  0.610221   \n",
       "1  partOfMany  [130, 361): 'Along the way, he recruits Sir Be...  0.710112   \n",
       "2    parentOf  [130, 361): 'Along the way, he recruits Sir Be...  0.382100   \n",
       "3   residesIn  [362, 512): 'Arthur leads the men to Camelot, ...  0.492869   \n",
       "4   locatedAt  [362, 512): 'Arthur leads the men to Camelot, ...  0.339446   \n",
       "\n",
       "         arguments.0.span       arguments.1.span arguments.0.entities.type  \\\n",
       "0   [208, 215): 'Galahad'    [323, 328): 'their'                    Person   \n",
       "1  [266, 274): 'Lancelot'    [323, 328): 'their'                    Person   \n",
       "2     [323, 328): 'their'  [329, 336): 'squires'                    Person   \n",
       "3    [362, 368): 'Arthur'  [386, 393): 'Camelot'                    Person   \n",
       "4       [379, 382): 'men'  [386, 393): 'Camelot'                    Person   \n",
       "\n",
       "  arguments.1.entities.type arguments.0.entities.text  \\\n",
       "0                    Person                   Galahad   \n",
       "1                    Person                  Lancelot   \n",
       "2                    Person                     their   \n",
       "3        GeopoliticalEntity               King Arthur   \n",
       "4        GeopoliticalEntity                       men   \n",
       "\n",
       "  arguments.1.entities.text  \n",
       "0                     their  \n",
       "1                     their  \n",
       "2                   squires  \n",
       "3                   Camelot  \n",
       "4                   Camelot  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[\"relations\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject.text</th>\n",
       "      <th>sentence</th>\n",
       "      <th>object.text</th>\n",
       "      <th>action.verb.text</th>\n",
       "      <th>action.verb.tense</th>\n",
       "      <th>action.text</th>\n",
       "      <th>action.normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>men</td>\n",
       "      <td>In AD 932, King Arthur and his squire, Patsy, ...</td>\n",
       "      <td>the Knights of the Round Table</td>\n",
       "      <td>join</td>\n",
       "      <td>future</td>\n",
       "      <td>to join</td>\n",
       "      <td>to join</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>he</td>\n",
       "      <td>Along the way, he recruits Sir Bedevere the W...</td>\n",
       "      <td>Sir Bedevere the Wise</td>\n",
       "      <td>recruit</td>\n",
       "      <td>present</td>\n",
       "      <td>recruits</td>\n",
       "      <td>recruit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arthur</td>\n",
       "      <td>Arthur leads the men to Camelot, but upon fur...</td>\n",
       "      <td>the men</td>\n",
       "      <td>lead</td>\n",
       "      <td>present</td>\n",
       "      <td>leads</td>\n",
       "      <td>lead</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>he</td>\n",
       "      <td>Arthur leads the men to Camelot, but upon fur...</td>\n",
       "      <td>not to go there</td>\n",
       "      <td>decide</td>\n",
       "      <td>present</td>\n",
       "      <td>decides</td>\n",
       "      <td>decide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>he</td>\n",
       "      <td>Arthur leads the men to Camelot, but upon fur...</td>\n",
       "      <td>a musical number)</td>\n",
       "      <td>go</td>\n",
       "      <td>future</td>\n",
       "      <td>to go</td>\n",
       "      <td>to go</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subject.text                                           sentence  \\\n",
       "0          men  In AD 932, King Arthur and his squire, Patsy, ...   \n",
       "1           he   Along the way, he recruits Sir Bedevere the W...   \n",
       "2       Arthur   Arthur leads the men to Camelot, but upon fur...   \n",
       "3           he   Arthur leads the men to Camelot, but upon fur...   \n",
       "4           he   Arthur leads the men to Camelot, but upon fur...   \n",
       "\n",
       "                      object.text action.verb.text action.verb.tense  \\\n",
       "0  the Knights of the Round Table             join            future   \n",
       "1           Sir Bedevere the Wise          recruit           present   \n",
       "2                         the men             lead           present   \n",
       "3                 not to go there           decide           present   \n",
       "4               a musical number)               go            future   \n",
       "\n",
       "  action.text action.normalized  \n",
       "0     to join           to join  \n",
       "1    recruits           recruit  \n",
       "2       leads              lead  \n",
       "3     decides            decide  \n",
       "4       to go             to go  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[\"semantic_roles\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Use Case\n",
    "Let's explore another use case by applying the Watson Natural Language Understanding API over product reviews for conducting the keyword based sentiment analysis. To do so, we have downloaded [Edmunds-Consumer Car Ratings and Reviews](https://www.kaggle.com/ankkur13/edmundsconsumer-car-ratings-and-reviews) from the Kaggle website and copied that to our resources directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Review_Date</th>\n",
       "      <th>Author_Name</th>\n",
       "      <th>Vehicle_Title</th>\n",
       "      <th>Review_Title</th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>on 10/13/05 15:30 PM (PDT)</td>\n",
       "      <td>roadking</td>\n",
       "      <td>2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...</td>\n",
       "      <td>Great delivery vehicle</td>\n",
       "      <td>It's been a great delivery vehicle for my caf...</td>\n",
       "      <td>4.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>on 07/17/05 21:59 PM (PDT)</td>\n",
       "      <td>Mark</td>\n",
       "      <td>2002 Dodge Ram Cargo Van 3500 3dr Ext Van (5.2...</td>\n",
       "      <td>Disappointmnet</td>\n",
       "      <td>Bought this car as a commuter vehicle for a v...</td>\n",
       "      <td>2.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>on 07/16/02 00:00 AM (PDT)</td>\n",
       "      <td>Tom Sheer</td>\n",
       "      <td>2002 Dodge Ram Cargo Van 3500 Maxi 3dr Ext Van...</td>\n",
       "      <td>Sweet van</td>\n",
       "      <td>This van rocks its the best, lots of \\rroom. ...</td>\n",
       "      <td>5.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>on 12/29/07 21:57 PM (PST)</td>\n",
       "      <td>Keven Smith</td>\n",
       "      <td>2001 Dodge Ram Cargo Van 2500 Maxi 3dr Ext Van...</td>\n",
       "      <td>Keven Smith</td>\n",
       "      <td>Great work vehicle. Drives nice. has lots of ...</td>\n",
       "      <td>4.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>on 02/09/05 18:52 PM (PST)</td>\n",
       "      <td>VanMan</td>\n",
       "      <td>2001 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...</td>\n",
       "      <td>Not what Dodge used to be</td>\n",
       "      <td>Good solid frame and suspension.  Well equipp...</td>\n",
       "      <td>2.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>on 05/17/04 00:00 AM (PDT)</td>\n",
       "      <td>driverage</td>\n",
       "      <td>2001 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...</td>\n",
       "      <td>2001 dodge 1500 ram van</td>\n",
       "      <td>some very early body rust on roof</td>\n",
       "      <td>2.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>on 08/01/17 16:40 PM (PDT)</td>\n",
       "      <td>Guy</td>\n",
       "      <td>2003 Dodge Ram Cargo Van 2500 3dr Ext Van (5.2...</td>\n",
       "      <td>Failur is lurking in every corner</td>\n",
       "      <td>Purchased the vehicle in 2003 second hand wit...</td>\n",
       "      <td>3.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>on 02/25/06 06:09 AM (PST)</td>\n",
       "      <td>smithtp</td>\n",
       "      <td>2003 Dodge Ram Cargo Van 1500 3dr Ext Van (3.9...</td>\n",
       "      <td>Great Van</td>\n",
       "      <td>Bought used with 34k,the conversion package b...</td>\n",
       "      <td>4.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>on 04/13/04 00:00 AM (PDT)</td>\n",
       "      <td>Regency</td>\n",
       "      <td>2003 Dodge Ram Cargo Van 1500 3dr Ext Van (3.9...</td>\n",
       "      <td>2003 Conversion Van</td>\n",
       "      <td>This is a left over new van. Great deal</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>especially with the 0 % interest. Van</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Unnamed: 0                  Review_Date  \\\n",
       "0                                       0   on 10/13/05 15:30 PM (PDT)   \n",
       "1                                       1   on 07/17/05 21:59 PM (PDT)   \n",
       "2                                       2   on 07/16/02 00:00 AM (PDT)   \n",
       "3                                       3   on 12/29/07 21:57 PM (PST)   \n",
       "4                                       4   on 02/09/05 18:52 PM (PST)   \n",
       "5                                       5   on 05/17/04 00:00 AM (PDT)   \n",
       "6                                       6   on 08/01/17 16:40 PM (PDT)   \n",
       "7                                       7   on 02/25/06 06:09 AM (PST)   \n",
       "8                                       8   on 04/13/04 00:00 AM (PDT)   \n",
       "9  especially with the 0 % interest. Van                          None   \n",
       "\n",
       "    Author_Name                                      Vehicle_Title  \\\n",
       "0     roadking   2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...   \n",
       "1         Mark   2002 Dodge Ram Cargo Van 3500 3dr Ext Van (5.2...   \n",
       "2    Tom Sheer   2002 Dodge Ram Cargo Van 3500 Maxi 3dr Ext Van...   \n",
       "3  Keven Smith   2001 Dodge Ram Cargo Van 2500 Maxi 3dr Ext Van...   \n",
       "4       VanMan   2001 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...   \n",
       "5    driverage   2001 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...   \n",
       "6          Guy   2003 Dodge Ram Cargo Van 2500 3dr Ext Van (5.2...   \n",
       "7      smithtp   2003 Dodge Ram Cargo Van 1500 3dr Ext Van (3.9...   \n",
       "8      Regency   2003 Dodge Ram Cargo Van 1500 3dr Ext Van (3.9...   \n",
       "9          None                                               None   \n",
       "\n",
       "                        Review_Title  \\\n",
       "0             Great delivery vehicle   \n",
       "1                     Disappointmnet   \n",
       "2                          Sweet van   \n",
       "3                        Keven Smith   \n",
       "4          Not what Dodge used to be   \n",
       "5            2001 dodge 1500 ram van   \n",
       "6  Failur is lurking in every corner   \n",
       "7                          Great Van   \n",
       "8                2003 Conversion Van   \n",
       "9                               None   \n",
       "\n",
       "                                              Review  Rating  \n",
       "0   It's been a great delivery vehicle for my caf...   4.625  \n",
       "1   Bought this car as a commuter vehicle for a v...   2.125  \n",
       "2   This van rocks its the best, lots of \\rroom. ...   5.000  \n",
       "3   Great work vehicle. Drives nice. has lots of ...   4.500  \n",
       "4   Good solid frame and suspension.  Well equipp...   2.875  \n",
       "5                  some very early body rust on roof   2.250  \n",
       "6   Purchased the vehicle in 2003 second hand wit...   3.000  \n",
       "7   Bought used with 34k,the conversion package b...   4.625  \n",
       "8           This is a left over new van. Great deal      NaN  \n",
       "9                                               None     NaN  "
      ]
     },
     "execution_count": 627,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dodge_rev = pd.read_csv('../resources/Scraped_Car_Review_dodge.csv',engine='python',index_col=False)\n",
    "dodge_rev.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11552, 7)"
      ]
     },
     "execution_count": 628,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dodge_rev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Review_Date</th>\n",
       "      <th>Author_Name</th>\n",
       "      <th>Vehicle_Title</th>\n",
       "      <th>Review_Title</th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review_Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>on 10/13/05 15:30 PM (PDT)</td>\n",
       "      <td>roadking</td>\n",
       "      <td>2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...</td>\n",
       "      <td>Great delivery vehicle</td>\n",
       "      <td>It's been a great delivery vehicle for my caf...</td>\n",
       "      <td>4.625</td>\n",
       "      <td>Great delivery vehicle: It's been a great deli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>on 07/17/05 21:59 PM (PDT)</td>\n",
       "      <td>Mark</td>\n",
       "      <td>2002 Dodge Ram Cargo Van 3500 3dr Ext Van (5.2...</td>\n",
       "      <td>Disappointmnet</td>\n",
       "      <td>Bought this car as a commuter vehicle for a v...</td>\n",
       "      <td>2.125</td>\n",
       "      <td>Disappointmnet: Bought this car as a commuter ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>on 07/16/02 00:00 AM (PDT)</td>\n",
       "      <td>Tom Sheer</td>\n",
       "      <td>2002 Dodge Ram Cargo Van 3500 Maxi 3dr Ext Van...</td>\n",
       "      <td>Sweet van</td>\n",
       "      <td>This van rocks its the best, lots of \\rroom. ...</td>\n",
       "      <td>5.000</td>\n",
       "      <td>Sweet van: This van rocks its the best, lots o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>on 12/29/07 21:57 PM (PST)</td>\n",
       "      <td>Keven Smith</td>\n",
       "      <td>2001 Dodge Ram Cargo Van 2500 Maxi 3dr Ext Van...</td>\n",
       "      <td>Keven Smith</td>\n",
       "      <td>Great work vehicle. Drives nice. has lots of ...</td>\n",
       "      <td>4.500</td>\n",
       "      <td>Keven Smith: Great work vehicle. Drives nice. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>on 02/09/05 18:52 PM (PST)</td>\n",
       "      <td>VanMan</td>\n",
       "      <td>2001 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...</td>\n",
       "      <td>Not what Dodge used to be</td>\n",
       "      <td>Good solid frame and suspension.  Well equipp...</td>\n",
       "      <td>2.875</td>\n",
       "      <td>Not what Dodge used to be: Good solid frame an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0                  Review_Date   Author_Name  \\\n",
       "0          0   on 10/13/05 15:30 PM (PDT)     roadking    \n",
       "1          1   on 07/17/05 21:59 PM (PDT)         Mark    \n",
       "2          2   on 07/16/02 00:00 AM (PDT)    Tom Sheer    \n",
       "3          3   on 12/29/07 21:57 PM (PST)  Keven Smith    \n",
       "4          4   on 02/09/05 18:52 PM (PST)       VanMan    \n",
       "\n",
       "                                       Vehicle_Title  \\\n",
       "0  2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...   \n",
       "1  2002 Dodge Ram Cargo Van 3500 3dr Ext Van (5.2...   \n",
       "2  2002 Dodge Ram Cargo Van 3500 Maxi 3dr Ext Van...   \n",
       "3  2001 Dodge Ram Cargo Van 2500 Maxi 3dr Ext Van...   \n",
       "4  2001 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...   \n",
       "\n",
       "                Review_Title  \\\n",
       "0     Great delivery vehicle   \n",
       "1             Disappointmnet   \n",
       "2                  Sweet van   \n",
       "3                Keven Smith   \n",
       "4  Not what Dodge used to be   \n",
       "\n",
       "                                              Review  Rating  \\\n",
       "0   It's been a great delivery vehicle for my caf...   4.625   \n",
       "1   Bought this car as a commuter vehicle for a v...   2.125   \n",
       "2   This van rocks its the best, lots of \\rroom. ...   5.000   \n",
       "3   Great work vehicle. Drives nice. has lots of ...   4.500   \n",
       "4   Good solid frame and suspension.  Well equipp...   2.875   \n",
       "\n",
       "                                      Review_Content  \n",
       "0  Great delivery vehicle: It's been a great deli...  \n",
       "1  Disappointmnet: Bought this car as a commuter ...  \n",
       "2  Sweet van: This van rocks its the best, lots o...  \n",
       "3  Keven Smith: Great work vehicle. Drives nice. ...  \n",
       "4  Not what Dodge used to be: Good solid frame an...  "
      ]
     },
     "execution_count": 629,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's combine the review titles and Review to the review_content\n",
    "dodge_rev['Review_Content']=dodge_rev['Review_Title']+ ':' + dodge_rev['Review']\n",
    "dodge_rev.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first apply watson NLU on the first review and analyze the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Watson Natural Language Understanding for analyzing the Review_Content\n",
    "# Make the request\n",
    "response_review = natural_language_understanding.analyze(\n",
    "    text=dodge_rev['Review_Content'][0],\n",
    "    return_analyzed_text=True,\n",
    "    features=nlu.Features(\n",
    "        entities=nlu.EntitiesOptions(sentiment=True, mentions=True),\n",
    "        keywords=nlu.KeywordsOptions(sentiment=True, emotion=True),\n",
    "        relations=nlu.RelationsOptions(),\n",
    "        semantic_roles=nlu.SemanticRolesOptions(),\n",
    "        syntax=nlu.SyntaxOptions(sentences=True, \n",
    "                                 tokens=nlu.SyntaxOptionsTokens(lemma=True, part_of_speech=True)),\n",
    "        sentiment=nlu.SentimentOptions(targets=['vehicle'])\n",
    "    )).get_result()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response from the analyze() method is a Python dictionary. The dictionary contains an entry for each pass of analysis requested, plus some additional entries with metadata about the API request itself. Here's a list of the keys in response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['usage', 'syntax', 'sentiment', 'semantic_roles', 'relations', 'language', 'keywords', 'entities', 'analyzed_text'])"
      ]
     },
     "execution_count": 632,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_review.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'usage': {'text_units': 1, 'text_characters': 301, 'features': 6},\n",
       " 'syntax': {'tokens': [{'text': 'Great',\n",
       "    'part_of_speech': 'ADJ',\n",
       "    'location': [0, 5],\n",
       "    'lemma': 'great'},\n",
       "   {'text': 'delivery',\n",
       "    'part_of_speech': 'NOUN',\n",
       "    'location': [6, 14],\n",
       "    'lemma': 'delivery'},\n",
       "   {'text': 'vehicle',\n",
       "    'part_of_speech': 'NOUN',\n",
       "    'location': [15, 22],\n",
       "    'lemma': 'vehicle'},\n",
       "   {'text': ':',\n",
       "    'part_of_speech': 'PUNCT',\n",
       "    'location': [22, 23],\n",
       "    'lemma': ':'},\n",
       "   {'text': 'It',\n",
       "    'part_of_speech': 'PRON',\n",
       "    'location': [24, 26],\n",
       "    'lemma': 'it'},\n",
       "   {'text': \"'s\",\n",
       "    'part_of_speech': 'AUX',\n",
       "    'location': [26, 28],\n",
       "    'lemma': 'be'},\n",
       "   {'text': 'been',\n",
       "    'part_of_speech': 'AUX',\n",
       "    'location': [29, 33],\n",
       "    'lemma': 'be'},\n",
       "   {'text': 'a', 'part_of_speech': 'DET', 'location': [34, 35], 'lemma': 'a'},\n",
       "   {'text': 'great',\n",
       "    'part_of_speech': 'ADJ',\n",
       "    'location': [36, 41],\n",
       "    'lemma': 'great'},\n",
       "   {'text': 'delivery',\n",
       "    'part_of_speech': 'NOUN',\n",
       "    'location': [42, 50],\n",
       "    'lemma': 'delivery'},\n",
       "   {'text': 'vehicle',\n",
       "    'part_of_speech': 'NOUN',\n",
       "    'location': [51, 58],\n",
       "    'lemma': 'vehicle'},\n",
       "   {'text': 'for',\n",
       "    'part_of_speech': 'ADP',\n",
       "    'location': [59, 62],\n",
       "    'lemma': 'for'},\n",
       "   {'text': 'my',\n",
       "    'part_of_speech': 'PRON',\n",
       "    'location': [63, 65],\n",
       "    'lemma': 'my'},\n",
       "   {'text': 'cafe',\n",
       "    'part_of_speech': 'NOUN',\n",
       "    'location': [66, 70],\n",
       "    'lemma': 'cafe'},\n",
       "   {'text': 'business',\n",
       "    'part_of_speech': 'NOUN',\n",
       "    'location': [71, 79],\n",
       "    'lemma': 'business'},\n",
       "   {'text': 'good',\n",
       "    'part_of_speech': 'ADJ',\n",
       "    'location': [80, 84],\n",
       "    'lemma': 'good'},\n",
       "   {'text': 'power',\n",
       "    'part_of_speech': 'NOUN',\n",
       "    'location': [85, 90],\n",
       "    'lemma': 'power'},\n",
       "   {'text': ',', 'part_of_speech': 'PUNCT', 'location': [90, 91]},\n",
       "   {'text': 'economy',\n",
       "    'part_of_speech': 'NOUN',\n",
       "    'location': [92, 99],\n",
       "    'lemma': 'economy'},\n",
       "   {'text': 'match',\n",
       "    'part_of_speech': 'NOUN',\n",
       "    'location': [100, 105],\n",
       "    'lemma': 'match'},\n",
       "   {'text': 'easily',\n",
       "    'part_of_speech': 'ADV',\n",
       "    'location': [106, 112],\n",
       "    'lemma': 'easily'},\n",
       "   {'text': 'taken',\n",
       "    'part_of_speech': 'VERB',\n",
       "    'location': [113, 118],\n",
       "    'lemma': 'take'},\n",
       "   {'text': 'care',\n",
       "    'part_of_speech': 'NOUN',\n",
       "    'location': [119, 123],\n",
       "    'lemma': 'care'},\n",
       "   {'text': 'of',\n",
       "    'part_of_speech': 'ADP',\n",
       "    'location': [124, 126],\n",
       "    'lemma': 'of'},\n",
       "   {'text': '.', 'part_of_speech': 'PUNCT', 'location': [126, 127]},\n",
       "   {'text': 'Havent', 'part_of_speech': 'PROPN', 'location': [128, 134]},\n",
       "   {'text': 'repaired',\n",
       "    'part_of_speech': 'VERB',\n",
       "    'location': [135, 143],\n",
       "    'lemma': 'repair'},\n",
       "   {'text': 'anything', 'part_of_speech': 'PRON', 'location': [144, 152]},\n",
       "   {'text': 'or',\n",
       "    'part_of_speech': 'CCONJ',\n",
       "    'location': [153, 155],\n",
       "    'lemma': 'or'},\n",
       "   {'text': 'replaced',\n",
       "    'part_of_speech': 'VERB',\n",
       "    'location': [156, 164],\n",
       "    'lemma': 'replace'},\n",
       "   {'text': 'anything', 'part_of_speech': 'PRON', 'location': [165, 173]},\n",
       "   {'text': 'but',\n",
       "    'part_of_speech': 'CCONJ',\n",
       "    'location': [174, 177],\n",
       "    'lemma': 'but'},\n",
       "   {'text': 'tires',\n",
       "    'part_of_speech': 'NOUN',\n",
       "    'location': [178, 183],\n",
       "    'lemma': 'tire'},\n",
       "   {'text': 'and',\n",
       "    'part_of_speech': 'CCONJ',\n",
       "    'location': [184, 187],\n",
       "    'lemma': 'and'},\n",
       "   {'text': 'normal',\n",
       "    'part_of_speech': 'ADJ',\n",
       "    'location': [188, 194],\n",
       "    'lemma': 'normal'},\n",
       "   {'text': 'maintenance',\n",
       "    'part_of_speech': 'NOUN',\n",
       "    'location': [195, 206],\n",
       "    'lemma': 'maintenance'},\n",
       "   {'text': 'items',\n",
       "    'part_of_speech': 'NOUN',\n",
       "    'location': [207, 212],\n",
       "    'lemma': 'item'},\n",
       "   {'text': '.', 'part_of_speech': 'PUNCT', 'location': [212, 213]},\n",
       "   {'text': 'Upgraded',\n",
       "    'part_of_speech': 'VERB',\n",
       "    'location': [214, 222],\n",
       "    'lemma': 'upgrade'},\n",
       "   {'text': 'tires',\n",
       "    'part_of_speech': 'NOUN',\n",
       "    'location': [223, 228],\n",
       "    'lemma': 'tire'},\n",
       "   {'text': 'to',\n",
       "    'part_of_speech': 'ADP',\n",
       "    'location': [229, 231],\n",
       "    'lemma': 'to'},\n",
       "   {'text': 'Michelin', 'part_of_speech': 'PROPN', 'location': [232, 240]},\n",
       "   {'text': 'LX', 'part_of_speech': 'PROPN', 'location': [241, 243]},\n",
       "   {'text': 'series',\n",
       "    'part_of_speech': 'NOUN',\n",
       "    'location': [244, 250],\n",
       "    'lemma': 'series'},\n",
       "   {'text': 'helped',\n",
       "    'part_of_speech': 'VERB',\n",
       "    'location': [251, 257],\n",
       "    'lemma': 'help'},\n",
       "   {'text': 'fuel',\n",
       "    'part_of_speech': 'NOUN',\n",
       "    'location': [258, 262],\n",
       "    'lemma': 'fuel'},\n",
       "   {'text': 'economy',\n",
       "    'part_of_speech': 'NOUN',\n",
       "    'location': [263, 270],\n",
       "    'lemma': 'economy'},\n",
       "   {'text': '.', 'part_of_speech': 'PUNCT', 'location': [270, 271]},\n",
       "   {'text': 'Would',\n",
       "    'part_of_speech': 'AUX',\n",
       "    'location': [272, 277],\n",
       "    'lemma': 'will'},\n",
       "   {'text': 'buy',\n",
       "    'part_of_speech': 'VERB',\n",
       "    'location': [278, 281],\n",
       "    'lemma': 'buy'},\n",
       "   {'text': 'another',\n",
       "    'part_of_speech': 'DET',\n",
       "    'location': [282, 289],\n",
       "    'lemma': 'another'},\n",
       "   {'text': 'in',\n",
       "    'part_of_speech': 'ADP',\n",
       "    'location': [290, 292],\n",
       "    'lemma': 'in'},\n",
       "   {'text': 'a',\n",
       "    'part_of_speech': 'DET',\n",
       "    'location': [293, 294],\n",
       "    'lemma': 'a'},\n",
       "   {'text': 'second',\n",
       "    'part_of_speech': 'ADJ',\n",
       "    'location': [295, 301],\n",
       "    'lemma': 'second'}],\n",
       "  'sentences': [{'text': \"Great delivery vehicle: It's been a great delivery vehicle for my cafe business good power, economy match easily taken care of.\",\n",
       "    'location': [0, 127]},\n",
       "   {'text': 'Havent repaired anything or replaced anything but tires and normal maintenance items.',\n",
       "    'location': [128, 213]},\n",
       "   {'text': 'Upgraded tires to Michelin LX series helped fuel economy.',\n",
       "    'location': [214, 271]},\n",
       "   {'text': 'Would buy another in a second', 'location': [272, 301]}]},\n",
       " 'sentiment': {'targets': [{'text': 'vehicle',\n",
       "    'score': 0.982545,\n",
       "    'label': 'positive'}],\n",
       "  'document': {'score': 0.578469, 'label': 'positive'}},\n",
       " 'semantic_roles': [{'subject': {'text': 'It'},\n",
       "   'sentence': \"Great delivery vehicle: It's been a great delivery vehicle for my cafe business good power, economy match easily taken care of.\",\n",
       "   'object': {'text': 'been a great delivery vehicle'},\n",
       "   'action': {'verb': {'text': 'has', 'tense': 'present'},\n",
       "    'text': 'has',\n",
       "    'normalized': 'has'}},\n",
       "  {'subject': {'text': 'economy match'},\n",
       "   'sentence': \"Great delivery vehicle: It's been a great delivery vehicle for my cafe business good power, economy match easily taken care of.\",\n",
       "   'object': {'text': 'care'},\n",
       "   'action': {'verb': {'text': 'take', 'tense': 'past'},\n",
       "    'text': 'taken',\n",
       "    'normalized': 'take'}},\n",
       "  {'subject': {'text': 'tires'},\n",
       "   'sentence': ' Upgraded tires to Michelin LX series helped fuel economy.',\n",
       "   'object': {'text': 'to Michelin LX series'},\n",
       "   'action': {'verb': {'text': 'Upgraded', 'tense': 'past'},\n",
       "    'text': 'Upgraded',\n",
       "    'normalized': 'Upgraded'}},\n",
       "  {'subject': {'text': 'Upgraded tires to Michelin LX series'},\n",
       "   'sentence': ' Upgraded tires to Michelin LX series helped fuel economy.',\n",
       "   'object': {'text': 'fuel economy'},\n",
       "   'action': {'verb': {'text': 'help', 'tense': 'past'},\n",
       "    'text': 'helped',\n",
       "    'normalized': 'help'}},\n",
       "  {'subject': {'text': 'another'},\n",
       "   'sentence': ' Would buy another in a second',\n",
       "   'action': {'verb': {'text': 'buy', 'tense': 'future'},\n",
       "    'text': 'Would buy',\n",
       "    'normalized': 'Would buy'}}],\n",
       " 'relations': [],\n",
       " 'language': 'en',\n",
       " 'keywords': [{'text': 'Great delivery vehicle',\n",
       "   'sentiment': {'score': 0.982545, 'label': 'positive'},\n",
       "   'relevance': 0.999486,\n",
       "   'emotion': {'sadness': 0.039427,\n",
       "    'joy': 0.690797,\n",
       "    'fear': 0.045917,\n",
       "    'disgust': 0.001771,\n",
       "    'anger': 0.011468},\n",
       "   'count': 2},\n",
       "  {'text': 'fuel economy',\n",
       "   'sentiment': {'score': 0.602034, 'label': 'positive'},\n",
       "   'relevance': 0.605066,\n",
       "   'emotion': {'sadness': 0.290023,\n",
       "    'joy': 0.131281,\n",
       "    'fear': 0.298556,\n",
       "    'disgust': 0.001173,\n",
       "    'anger': 0.086125},\n",
       "   'count': 1},\n",
       "  {'text': 'tires',\n",
       "   'sentiment': {'score': 0, 'mixed': '1', 'label': 'neutral'},\n",
       "   'relevance': 0.5591,\n",
       "   'emotion': {'sadness': 0.304524,\n",
       "    'joy': 0.042803,\n",
       "    'fear': 0.056519,\n",
       "    'disgust': 0.001143,\n",
       "    'anger': 0.145506},\n",
       "   'count': 2},\n",
       "  {'text': 'care',\n",
       "   'sentiment': {'score': 0.982545, 'label': 'positive'},\n",
       "   'relevance': 0.514348,\n",
       "   'emotion': {'sadness': 0.039427,\n",
       "    'joy': 0.690797,\n",
       "    'fear': 0.045917,\n",
       "    'disgust': 0.001771,\n",
       "    'anger': 0.011468},\n",
       "   'count': 1},\n",
       "  {'text': 'Michelin LX series',\n",
       "   'sentiment': {'score': 0.602034, 'label': 'positive'},\n",
       "   'relevance': 0.411711,\n",
       "   'emotion': {'sadness': 0.290023,\n",
       "    'joy': 0.131281,\n",
       "    'fear': 0.298556,\n",
       "    'disgust': 0.001173,\n",
       "    'anger': 0.086125},\n",
       "   'count': 1},\n",
       "  {'text': 'cafe business',\n",
       "   'sentiment': {'score': 0.982545, 'label': 'positive'},\n",
       "   'relevance': 0.396141,\n",
       "   'emotion': {'sadness': 0.039427,\n",
       "    'joy': 0.690797,\n",
       "    'fear': 0.045917,\n",
       "    'disgust': 0.001771,\n",
       "    'anger': 0.011468},\n",
       "   'count': 1},\n",
       "  {'text': 'good power',\n",
       "   'sentiment': {'score': 0.982545, 'label': 'positive'},\n",
       "   'relevance': 0.357562,\n",
       "   'emotion': {'sadness': 0.039427,\n",
       "    'joy': 0.690797,\n",
       "    'fear': 0.045917,\n",
       "    'disgust': 0.001771,\n",
       "    'anger': 0.011468},\n",
       "   'count': 1},\n",
       "  {'text': 'economy match',\n",
       "   'sentiment': {'score': 0.982545, 'label': 'positive'},\n",
       "   'relevance': 0.317306,\n",
       "   'emotion': {'sadness': 0.039427,\n",
       "    'joy': 0.690797,\n",
       "    'fear': 0.045917,\n",
       "    'disgust': 0.001771,\n",
       "    'anger': 0.011468},\n",
       "   'count': 1},\n",
       "  {'text': 'normal maintenance items',\n",
       "   'sentiment': {'score': -0.561817, 'label': 'negative'},\n",
       "   'relevance': 0.307532,\n",
       "   'emotion': {'sadness': 0.261543,\n",
       "    'joy': 0.025168,\n",
       "    'fear': 0.009511,\n",
       "    'disgust': 0.013281,\n",
       "    'anger': 0.192068},\n",
       "   'count': 1},\n",
       "  {'text': 'Havent',\n",
       "   'sentiment': {'score': -0.561817, 'label': 'negative'},\n",
       "   'relevance': 0.256558,\n",
       "   'emotion': {'sadness': 0.261543,\n",
       "    'joy': 0.025168,\n",
       "    'fear': 0.009511,\n",
       "    'disgust': 0.013281,\n",
       "    'anger': 0.192068},\n",
       "   'count': 1}],\n",
       " 'entities': [],\n",
       " 'analyzed_text': \"Great delivery vehicle: It's been a great delivery vehicle for my cafe business good power, economy match easily taken care of. Havent repaired anything or replaced anything but tires and normal maintenance items. Upgraded tires to Michelin LX series helped fuel economy. Would buy another in a second\"}"
      ]
     },
     "execution_count": 633,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Great delivery vehicle: It's been a great delivery vehicle for my cafe business good power, economy match easily taken care of. Havent repaired anything or replaced anything but tires and normal maintenance items. Upgraded tires to Michelin LX series helped fuel economy. Would buy another in a second\""
      ]
     },
     "execution_count": 634,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_review['analyzed_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'targets': [{'text': 'vehicle', 'score': 0.982545, 'label': 'positive'}],\n",
       " 'document': {'score': 0.578469, 'label': 'positive'}}"
      ]
     },
     "execution_count": 635,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_review['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'Great delivery vehicle',\n",
       "  'sentiment': {'score': 0.982545, 'label': 'positive'},\n",
       "  'relevance': 0.999486,\n",
       "  'emotion': {'sadness': 0.039427,\n",
       "   'joy': 0.690797,\n",
       "   'fear': 0.045917,\n",
       "   'disgust': 0.001771,\n",
       "   'anger': 0.011468},\n",
       "  'count': 2},\n",
       " {'text': 'fuel economy',\n",
       "  'sentiment': {'score': 0.602034, 'label': 'positive'},\n",
       "  'relevance': 0.605066,\n",
       "  'emotion': {'sadness': 0.290023,\n",
       "   'joy': 0.131281,\n",
       "   'fear': 0.298556,\n",
       "   'disgust': 0.001173,\n",
       "   'anger': 0.086125},\n",
       "  'count': 1},\n",
       " {'text': 'tires',\n",
       "  'sentiment': {'score': 0, 'mixed': '1', 'label': 'neutral'},\n",
       "  'relevance': 0.5591,\n",
       "  'emotion': {'sadness': 0.304524,\n",
       "   'joy': 0.042803,\n",
       "   'fear': 0.056519,\n",
       "   'disgust': 0.001143,\n",
       "   'anger': 0.145506},\n",
       "  'count': 2},\n",
       " {'text': 'care',\n",
       "  'sentiment': {'score': 0.982545, 'label': 'positive'},\n",
       "  'relevance': 0.514348,\n",
       "  'emotion': {'sadness': 0.039427,\n",
       "   'joy': 0.690797,\n",
       "   'fear': 0.045917,\n",
       "   'disgust': 0.001771,\n",
       "   'anger': 0.011468},\n",
       "  'count': 1},\n",
       " {'text': 'Michelin LX series',\n",
       "  'sentiment': {'score': 0.602034, 'label': 'positive'},\n",
       "  'relevance': 0.411711,\n",
       "  'emotion': {'sadness': 0.290023,\n",
       "   'joy': 0.131281,\n",
       "   'fear': 0.298556,\n",
       "   'disgust': 0.001173,\n",
       "   'anger': 0.086125},\n",
       "  'count': 1},\n",
       " {'text': 'cafe business',\n",
       "  'sentiment': {'score': 0.982545, 'label': 'positive'},\n",
       "  'relevance': 0.396141,\n",
       "  'emotion': {'sadness': 0.039427,\n",
       "   'joy': 0.690797,\n",
       "   'fear': 0.045917,\n",
       "   'disgust': 0.001771,\n",
       "   'anger': 0.011468},\n",
       "  'count': 1},\n",
       " {'text': 'good power',\n",
       "  'sentiment': {'score': 0.982545, 'label': 'positive'},\n",
       "  'relevance': 0.357562,\n",
       "  'emotion': {'sadness': 0.039427,\n",
       "   'joy': 0.690797,\n",
       "   'fear': 0.045917,\n",
       "   'disgust': 0.001771,\n",
       "   'anger': 0.011468},\n",
       "  'count': 1},\n",
       " {'text': 'economy match',\n",
       "  'sentiment': {'score': 0.982545, 'label': 'positive'},\n",
       "  'relevance': 0.317306,\n",
       "  'emotion': {'sadness': 0.039427,\n",
       "   'joy': 0.690797,\n",
       "   'fear': 0.045917,\n",
       "   'disgust': 0.001771,\n",
       "   'anger': 0.011468},\n",
       "  'count': 1},\n",
       " {'text': 'normal maintenance items',\n",
       "  'sentiment': {'score': -0.561817, 'label': 'negative'},\n",
       "  'relevance': 0.307532,\n",
       "  'emotion': {'sadness': 0.261543,\n",
       "   'joy': 0.025168,\n",
       "   'fear': 0.009511,\n",
       "   'disgust': 0.013281,\n",
       "   'anger': 0.192068},\n",
       "  'count': 1},\n",
       " {'text': 'Havent',\n",
       "  'sentiment': {'score': -0.561817, 'label': 'negative'},\n",
       "  'relevance': 0.256558,\n",
       "  'emotion': {'sadness': 0.261543,\n",
       "   'joy': 0.025168,\n",
       "   'fear': 0.009511,\n",
       "   'disgust': 0.013281,\n",
       "   'anger': 0.192068},\n",
       "  'count': 1}]"
      ]
     },
     "execution_count": 636,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_review['keywords']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's parse the Watson NLU response by text extensions for pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'syntax':                          span part_of_speech        lemma  \\\n",
       " 0             [0, 5): 'Great'            ADJ        great   \n",
       " 1         [6, 14): 'delivery'           NOUN     delivery   \n",
       " 2         [15, 22): 'vehicle'           NOUN      vehicle   \n",
       " 3               [22, 23): ':'          PUNCT            :   \n",
       " 4              [24, 26): 'It'           PRON           it   \n",
       " 5              [26, 28): ''s'            AUX           be   \n",
       " 6            [29, 33): 'been'            AUX           be   \n",
       " 7               [34, 35): 'a'            DET            a   \n",
       " 8           [36, 41): 'great'            ADJ        great   \n",
       " 9        [42, 50): 'delivery'           NOUN     delivery   \n",
       " 10        [51, 58): 'vehicle'           NOUN      vehicle   \n",
       " 11            [59, 62): 'for'            ADP          for   \n",
       " 12             [63, 65): 'my'           PRON           my   \n",
       " 13           [66, 70): 'cafe'           NOUN         cafe   \n",
       " 14       [71, 79): 'business'           NOUN     business   \n",
       " 15           [80, 84): 'good'            ADJ         good   \n",
       " 16          [85, 90): 'power'           NOUN        power   \n",
       " 17              [90, 91): ','          PUNCT         None   \n",
       " 18        [92, 99): 'economy'           NOUN      economy   \n",
       " 19        [100, 105): 'match'           NOUN        match   \n",
       " 20       [106, 112): 'easily'            ADV       easily   \n",
       " 21        [113, 118): 'taken'           VERB         take   \n",
       " 22         [119, 123): 'care'           NOUN         care   \n",
       " 23           [124, 126): 'of'            ADP           of   \n",
       " 24            [126, 127): '.'          PUNCT         None   \n",
       " 25       [128, 134): 'Havent'          PROPN         None   \n",
       " 26     [135, 143): 'repaired'           VERB       repair   \n",
       " 27     [144, 152): 'anything'           PRON         None   \n",
       " 28           [153, 155): 'or'          CCONJ           or   \n",
       " 29     [156, 164): 'replaced'           VERB      replace   \n",
       " 30     [165, 173): 'anything'           PRON         None   \n",
       " 31          [174, 177): 'but'          CCONJ          but   \n",
       " 32        [178, 183): 'tires'           NOUN         tire   \n",
       " 33          [184, 187): 'and'          CCONJ          and   \n",
       " 34       [188, 194): 'normal'            ADJ       normal   \n",
       " 35  [195, 206): 'maintenance'           NOUN  maintenance   \n",
       " 36        [207, 212): 'items'           NOUN         item   \n",
       " 37            [212, 213): '.'          PUNCT         None   \n",
       " 38     [214, 222): 'Upgraded'           VERB      upgrade   \n",
       " 39        [223, 228): 'tires'           NOUN         tire   \n",
       " 40           [229, 231): 'to'            ADP           to   \n",
       " 41     [232, 240): 'Michelin'          PROPN         None   \n",
       " 42           [241, 243): 'LX'          PROPN         None   \n",
       " 43       [244, 250): 'series'           NOUN       series   \n",
       " 44       [251, 257): 'helped'           VERB         help   \n",
       " 45         [258, 262): 'fuel'           NOUN         fuel   \n",
       " 46      [263, 270): 'economy'           NOUN      economy   \n",
       " 47            [270, 271): '.'          PUNCT         None   \n",
       " 48        [272, 277): 'Would'            AUX         will   \n",
       " 49          [278, 281): 'buy'           VERB          buy   \n",
       " 50      [282, 289): 'another'            DET      another   \n",
       " 51           [290, 292): 'in'            ADP           in   \n",
       " 52            [293, 294): 'a'            DET            a   \n",
       " 53       [295, 301): 'second'            ADJ       second   \n",
       " \n",
       "                                              sentence  \n",
       " 0   [0, 127): 'Great delivery vehicle: It's been a...  \n",
       " 1   [0, 127): 'Great delivery vehicle: It's been a...  \n",
       " 2   [0, 127): 'Great delivery vehicle: It's been a...  \n",
       " 3   [0, 127): 'Great delivery vehicle: It's been a...  \n",
       " 4   [0, 127): 'Great delivery vehicle: It's been a...  \n",
       " 5   [0, 127): 'Great delivery vehicle: It's been a...  \n",
       " 6   [0, 127): 'Great delivery vehicle: It's been a...  \n",
       " 7   [0, 127): 'Great delivery vehicle: It's been a...  \n",
       " 8   [0, 127): 'Great delivery vehicle: It's been a...  \n",
       " 9   [0, 127): 'Great delivery vehicle: It's been a...  \n",
       " 10  [0, 127): 'Great delivery vehicle: It's been a...  \n",
       " 11  [0, 127): 'Great delivery vehicle: It's been a...  \n",
       " 12  [0, 127): 'Great delivery vehicle: It's been a...  \n",
       " 13  [0, 127): 'Great delivery vehicle: It's been a...  \n",
       " 14  [0, 127): 'Great delivery vehicle: It's been a...  \n",
       " 15  [0, 127): 'Great delivery vehicle: It's been a...  \n",
       " 16  [0, 127): 'Great delivery vehicle: It's been a...  \n",
       " 17  [0, 127): 'Great delivery vehicle: It's been a...  \n",
       " 18  [0, 127): 'Great delivery vehicle: It's been a...  \n",
       " 19  [0, 127): 'Great delivery vehicle: It's been a...  \n",
       " 20  [0, 127): 'Great delivery vehicle: It's been a...  \n",
       " 21  [0, 127): 'Great delivery vehicle: It's been a...  \n",
       " 22  [0, 127): 'Great delivery vehicle: It's been a...  \n",
       " 23  [0, 127): 'Great delivery vehicle: It's been a...  \n",
       " 24  [0, 127): 'Great delivery vehicle: It's been a...  \n",
       " 25  [128, 213): 'Havent repaired anything or repla...  \n",
       " 26  [128, 213): 'Havent repaired anything or repla...  \n",
       " 27  [128, 213): 'Havent repaired anything or repla...  \n",
       " 28  [128, 213): 'Havent repaired anything or repla...  \n",
       " 29  [128, 213): 'Havent repaired anything or repla...  \n",
       " 30  [128, 213): 'Havent repaired anything or repla...  \n",
       " 31  [128, 213): 'Havent repaired anything or repla...  \n",
       " 32  [128, 213): 'Havent repaired anything or repla...  \n",
       " 33  [128, 213): 'Havent repaired anything or repla...  \n",
       " 34  [128, 213): 'Havent repaired anything or repla...  \n",
       " 35  [128, 213): 'Havent repaired anything or repla...  \n",
       " 36  [128, 213): 'Havent repaired anything or repla...  \n",
       " 37  [128, 213): 'Havent repaired anything or repla...  \n",
       " 38  [214, 271): 'Upgraded tires to Michelin LX ser...  \n",
       " 39  [214, 271): 'Upgraded tires to Michelin LX ser...  \n",
       " 40  [214, 271): 'Upgraded tires to Michelin LX ser...  \n",
       " 41  [214, 271): 'Upgraded tires to Michelin LX ser...  \n",
       " 42  [214, 271): 'Upgraded tires to Michelin LX ser...  \n",
       " 43  [214, 271): 'Upgraded tires to Michelin LX ser...  \n",
       " 44  [214, 271): 'Upgraded tires to Michelin LX ser...  \n",
       " 45  [214, 271): 'Upgraded tires to Michelin LX ser...  \n",
       " 46  [214, 271): 'Upgraded tires to Michelin LX ser...  \n",
       " 47  [214, 271): 'Upgraded tires to Michelin LX ser...  \n",
       " 48        [272, 301): 'Would buy another in a second'  \n",
       " 49        [272, 301): 'Would buy another in a second'  \n",
       " 50        [272, 301): 'Would buy another in a second'  \n",
       " 51        [272, 301): 'Would buy another in a second'  \n",
       " 52        [272, 301): 'Would buy another in a second'  \n",
       " 53        [272, 301): 'Would buy another in a second'  ,\n",
       " 'entities': Empty DataFrame\n",
       " Columns: []\n",
       " Index: [],\n",
       " 'entity_mentions': Empty DataFrame\n",
       " Columns: []\n",
       " Index: [],\n",
       " 'keywords':                        text sentiment.label  sentiment.score  relevance  \\\n",
       " 0    Great delivery vehicle        positive         0.982545   0.999486   \n",
       " 1              fuel economy        positive         0.602034   0.605066   \n",
       " 2                     tires         neutral         0.000000   0.559100   \n",
       " 3                      care        positive         0.982545   0.514348   \n",
       " 4        Michelin LX series        positive         0.602034   0.411711   \n",
       " 5             cafe business        positive         0.982545   0.396141   \n",
       " 6                good power        positive         0.982545   0.357562   \n",
       " 7             economy match        positive         0.982545   0.317306   \n",
       " 8  normal maintenance items        negative        -0.561817   0.307532   \n",
       " 9                    Havent        negative        -0.561817   0.256558   \n",
       " \n",
       "    emotion.sadness  emotion.joy  emotion.fear  emotion.disgust  emotion.anger  \\\n",
       " 0         0.039427     0.690797      0.045917         0.001771       0.011468   \n",
       " 1         0.290023     0.131281      0.298556         0.001173       0.086125   \n",
       " 2         0.304524     0.042803      0.056519         0.001143       0.145506   \n",
       " 3         0.039427     0.690797      0.045917         0.001771       0.011468   \n",
       " 4         0.290023     0.131281      0.298556         0.001173       0.086125   \n",
       " 5         0.039427     0.690797      0.045917         0.001771       0.011468   \n",
       " 6         0.039427     0.690797      0.045917         0.001771       0.011468   \n",
       " 7         0.039427     0.690797      0.045917         0.001771       0.011468   \n",
       " 8         0.261543     0.025168      0.009511         0.013281       0.192068   \n",
       " 9         0.261543     0.025168      0.009511         0.013281       0.192068   \n",
       " \n",
       "    count  \n",
       " 0      2  \n",
       " 1      1  \n",
       " 2      2  \n",
       " 3      1  \n",
       " 4      1  \n",
       " 5      1  \n",
       " 6      1  \n",
       " 7      1  \n",
       " 8      1  \n",
       " 9      1  ,\n",
       " 'relations': Empty DataFrame\n",
       " Columns: []\n",
       " Index: [],\n",
       " 'semantic_roles':                            subject.text  \\\n",
       " 0                                    It   \n",
       " 1                         economy match   \n",
       " 2                                 tires   \n",
       " 3  Upgraded tires to Michelin LX series   \n",
       " 4                               another   \n",
       " \n",
       "                                             sentence  \\\n",
       " 0  Great delivery vehicle: It's been a great deli...   \n",
       " 1  Great delivery vehicle: It's been a great deli...   \n",
       " 2   Upgraded tires to Michelin LX series helped f...   \n",
       " 3   Upgraded tires to Michelin LX series helped f...   \n",
       " 4                      Would buy another in a second   \n",
       " \n",
       "                      object.text action.verb.text action.verb.tense  \\\n",
       " 0  been a great delivery vehicle              has           present   \n",
       " 1                           care             take              past   \n",
       " 2          to Michelin LX series         Upgraded              past   \n",
       " 3                   fuel economy             help              past   \n",
       " 4                           None              buy            future   \n",
       " \n",
       "   action.text action.normalized  \n",
       " 0         has               has  \n",
       " 1       taken              take  \n",
       " 2    Upgraded          Upgraded  \n",
       " 3      helped              help  \n",
       " 4   Would buy         Would buy  }"
      ]
     },
     "execution_count": 637,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_analyzed_review = tp.io.watson.nlu.parse_response(response_review)\n",
    "dfs_analyzed_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['syntax', 'entities', 'entity_mentions', 'keywords', 'relations', 'semantic_roles'])"
      ]
     },
     "execution_count": 638,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_analyzed_review.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>span</th>\n",
       "      <th>part_of_speech</th>\n",
       "      <th>lemma</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 5): 'Great'</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>great</td>\n",
       "      <td>[0, 127): 'Great delivery vehicle: It's been a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[6, 14): 'delivery'</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>delivery</td>\n",
       "      <td>[0, 127): 'Great delivery vehicle: It's been a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[15, 22): 'vehicle'</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>vehicle</td>\n",
       "      <td>[0, 127): 'Great delivery vehicle: It's been a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[22, 23): ':'</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>:</td>\n",
       "      <td>[0, 127): 'Great delivery vehicle: It's been a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[24, 26): 'It'</td>\n",
       "      <td>PRON</td>\n",
       "      <td>it</td>\n",
       "      <td>[0, 127): 'Great delivery vehicle: It's been a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[26, 28): ''s'</td>\n",
       "      <td>AUX</td>\n",
       "      <td>be</td>\n",
       "      <td>[0, 127): 'Great delivery vehicle: It's been a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[29, 33): 'been'</td>\n",
       "      <td>AUX</td>\n",
       "      <td>be</td>\n",
       "      <td>[0, 127): 'Great delivery vehicle: It's been a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[34, 35): 'a'</td>\n",
       "      <td>DET</td>\n",
       "      <td>a</td>\n",
       "      <td>[0, 127): 'Great delivery vehicle: It's been a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[36, 41): 'great'</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>great</td>\n",
       "      <td>[0, 127): 'Great delivery vehicle: It's been a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[42, 50): 'delivery'</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>delivery</td>\n",
       "      <td>[0, 127): 'Great delivery vehicle: It's been a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[51, 58): 'vehicle'</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>vehicle</td>\n",
       "      <td>[0, 127): 'Great delivery vehicle: It's been a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[59, 62): 'for'</td>\n",
       "      <td>ADP</td>\n",
       "      <td>for</td>\n",
       "      <td>[0, 127): 'Great delivery vehicle: It's been a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[63, 65): 'my'</td>\n",
       "      <td>PRON</td>\n",
       "      <td>my</td>\n",
       "      <td>[0, 127): 'Great delivery vehicle: It's been a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[66, 70): 'cafe'</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>cafe</td>\n",
       "      <td>[0, 127): 'Great delivery vehicle: It's been a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[71, 79): 'business'</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>business</td>\n",
       "      <td>[0, 127): 'Great delivery vehicle: It's been a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[80, 84): 'good'</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>good</td>\n",
       "      <td>[0, 127): 'Great delivery vehicle: It's been a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[85, 90): 'power'</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>power</td>\n",
       "      <td>[0, 127): 'Great delivery vehicle: It's been a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[90, 91): ','</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>None</td>\n",
       "      <td>[0, 127): 'Great delivery vehicle: It's been a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[92, 99): 'economy'</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>economy</td>\n",
       "      <td>[0, 127): 'Great delivery vehicle: It's been a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[100, 105): 'match'</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>match</td>\n",
       "      <td>[0, 127): 'Great delivery vehicle: It's been a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[106, 112): 'easily'</td>\n",
       "      <td>ADV</td>\n",
       "      <td>easily</td>\n",
       "      <td>[0, 127): 'Great delivery vehicle: It's been a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[113, 118): 'taken'</td>\n",
       "      <td>VERB</td>\n",
       "      <td>take</td>\n",
       "      <td>[0, 127): 'Great delivery vehicle: It's been a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[119, 123): 'care'</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>care</td>\n",
       "      <td>[0, 127): 'Great delivery vehicle: It's been a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[124, 126): 'of'</td>\n",
       "      <td>ADP</td>\n",
       "      <td>of</td>\n",
       "      <td>[0, 127): 'Great delivery vehicle: It's been a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[126, 127): '.'</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>None</td>\n",
       "      <td>[0, 127): 'Great delivery vehicle: It's been a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[128, 134): 'Havent'</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>None</td>\n",
       "      <td>[128, 213): 'Havent repaired anything or repla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[135, 143): 'repaired'</td>\n",
       "      <td>VERB</td>\n",
       "      <td>repair</td>\n",
       "      <td>[128, 213): 'Havent repaired anything or repla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[144, 152): 'anything'</td>\n",
       "      <td>PRON</td>\n",
       "      <td>None</td>\n",
       "      <td>[128, 213): 'Havent repaired anything or repla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[153, 155): 'or'</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>or</td>\n",
       "      <td>[128, 213): 'Havent repaired anything or repla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[156, 164): 'replaced'</td>\n",
       "      <td>VERB</td>\n",
       "      <td>replace</td>\n",
       "      <td>[128, 213): 'Havent repaired anything or repla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[165, 173): 'anything'</td>\n",
       "      <td>PRON</td>\n",
       "      <td>None</td>\n",
       "      <td>[128, 213): 'Havent repaired anything or repla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[174, 177): 'but'</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>but</td>\n",
       "      <td>[128, 213): 'Havent repaired anything or repla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[178, 183): 'tires'</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>tire</td>\n",
       "      <td>[128, 213): 'Havent repaired anything or repla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[184, 187): 'and'</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>and</td>\n",
       "      <td>[128, 213): 'Havent repaired anything or repla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[188, 194): 'normal'</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>normal</td>\n",
       "      <td>[128, 213): 'Havent repaired anything or repla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[195, 206): 'maintenance'</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>maintenance</td>\n",
       "      <td>[128, 213): 'Havent repaired anything or repla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[207, 212): 'items'</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>item</td>\n",
       "      <td>[128, 213): 'Havent repaired anything or repla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[212, 213): '.'</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>None</td>\n",
       "      <td>[128, 213): 'Havent repaired anything or repla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[214, 222): 'Upgraded'</td>\n",
       "      <td>VERB</td>\n",
       "      <td>upgrade</td>\n",
       "      <td>[214, 271): 'Upgraded tires to Michelin LX ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[223, 228): 'tires'</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>tire</td>\n",
       "      <td>[214, 271): 'Upgraded tires to Michelin LX ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[229, 231): 'to'</td>\n",
       "      <td>ADP</td>\n",
       "      <td>to</td>\n",
       "      <td>[214, 271): 'Upgraded tires to Michelin LX ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[232, 240): 'Michelin'</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>None</td>\n",
       "      <td>[214, 271): 'Upgraded tires to Michelin LX ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[241, 243): 'LX'</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>None</td>\n",
       "      <td>[214, 271): 'Upgraded tires to Michelin LX ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[244, 250): 'series'</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>series</td>\n",
       "      <td>[214, 271): 'Upgraded tires to Michelin LX ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[251, 257): 'helped'</td>\n",
       "      <td>VERB</td>\n",
       "      <td>help</td>\n",
       "      <td>[214, 271): 'Upgraded tires to Michelin LX ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[258, 262): 'fuel'</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>fuel</td>\n",
       "      <td>[214, 271): 'Upgraded tires to Michelin LX ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[263, 270): 'economy'</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>economy</td>\n",
       "      <td>[214, 271): 'Upgraded tires to Michelin LX ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[270, 271): '.'</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>None</td>\n",
       "      <td>[214, 271): 'Upgraded tires to Michelin LX ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[272, 277): 'Would'</td>\n",
       "      <td>AUX</td>\n",
       "      <td>will</td>\n",
       "      <td>[272, 301): 'Would buy another in a second'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[278, 281): 'buy'</td>\n",
       "      <td>VERB</td>\n",
       "      <td>buy</td>\n",
       "      <td>[272, 301): 'Would buy another in a second'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[282, 289): 'another'</td>\n",
       "      <td>DET</td>\n",
       "      <td>another</td>\n",
       "      <td>[272, 301): 'Would buy another in a second'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[290, 292): 'in'</td>\n",
       "      <td>ADP</td>\n",
       "      <td>in</td>\n",
       "      <td>[272, 301): 'Would buy another in a second'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>[293, 294): 'a'</td>\n",
       "      <td>DET</td>\n",
       "      <td>a</td>\n",
       "      <td>[272, 301): 'Would buy another in a second'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>[295, 301): 'second'</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>second</td>\n",
       "      <td>[272, 301): 'Would buy another in a second'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         span part_of_speech        lemma  \\\n",
       "0             [0, 5): 'Great'            ADJ        great   \n",
       "1         [6, 14): 'delivery'           NOUN     delivery   \n",
       "2         [15, 22): 'vehicle'           NOUN      vehicle   \n",
       "3               [22, 23): ':'          PUNCT            :   \n",
       "4              [24, 26): 'It'           PRON           it   \n",
       "5              [26, 28): ''s'            AUX           be   \n",
       "6            [29, 33): 'been'            AUX           be   \n",
       "7               [34, 35): 'a'            DET            a   \n",
       "8           [36, 41): 'great'            ADJ        great   \n",
       "9        [42, 50): 'delivery'           NOUN     delivery   \n",
       "10        [51, 58): 'vehicle'           NOUN      vehicle   \n",
       "11            [59, 62): 'for'            ADP          for   \n",
       "12             [63, 65): 'my'           PRON           my   \n",
       "13           [66, 70): 'cafe'           NOUN         cafe   \n",
       "14       [71, 79): 'business'           NOUN     business   \n",
       "15           [80, 84): 'good'            ADJ         good   \n",
       "16          [85, 90): 'power'           NOUN        power   \n",
       "17              [90, 91): ','          PUNCT         None   \n",
       "18        [92, 99): 'economy'           NOUN      economy   \n",
       "19        [100, 105): 'match'           NOUN        match   \n",
       "20       [106, 112): 'easily'            ADV       easily   \n",
       "21        [113, 118): 'taken'           VERB         take   \n",
       "22         [119, 123): 'care'           NOUN         care   \n",
       "23           [124, 126): 'of'            ADP           of   \n",
       "24            [126, 127): '.'          PUNCT         None   \n",
       "25       [128, 134): 'Havent'          PROPN         None   \n",
       "26     [135, 143): 'repaired'           VERB       repair   \n",
       "27     [144, 152): 'anything'           PRON         None   \n",
       "28           [153, 155): 'or'          CCONJ           or   \n",
       "29     [156, 164): 'replaced'           VERB      replace   \n",
       "30     [165, 173): 'anything'           PRON         None   \n",
       "31          [174, 177): 'but'          CCONJ          but   \n",
       "32        [178, 183): 'tires'           NOUN         tire   \n",
       "33          [184, 187): 'and'          CCONJ          and   \n",
       "34       [188, 194): 'normal'            ADJ       normal   \n",
       "35  [195, 206): 'maintenance'           NOUN  maintenance   \n",
       "36        [207, 212): 'items'           NOUN         item   \n",
       "37            [212, 213): '.'          PUNCT         None   \n",
       "38     [214, 222): 'Upgraded'           VERB      upgrade   \n",
       "39        [223, 228): 'tires'           NOUN         tire   \n",
       "40           [229, 231): 'to'            ADP           to   \n",
       "41     [232, 240): 'Michelin'          PROPN         None   \n",
       "42           [241, 243): 'LX'          PROPN         None   \n",
       "43       [244, 250): 'series'           NOUN       series   \n",
       "44       [251, 257): 'helped'           VERB         help   \n",
       "45         [258, 262): 'fuel'           NOUN         fuel   \n",
       "46      [263, 270): 'economy'           NOUN      economy   \n",
       "47            [270, 271): '.'          PUNCT         None   \n",
       "48        [272, 277): 'Would'            AUX         will   \n",
       "49          [278, 281): 'buy'           VERB          buy   \n",
       "50      [282, 289): 'another'            DET      another   \n",
       "51           [290, 292): 'in'            ADP           in   \n",
       "52            [293, 294): 'a'            DET            a   \n",
       "53       [295, 301): 'second'            ADJ       second   \n",
       "\n",
       "                                             sentence  \n",
       "0   [0, 127): 'Great delivery vehicle: It's been a...  \n",
       "1   [0, 127): 'Great delivery vehicle: It's been a...  \n",
       "2   [0, 127): 'Great delivery vehicle: It's been a...  \n",
       "3   [0, 127): 'Great delivery vehicle: It's been a...  \n",
       "4   [0, 127): 'Great delivery vehicle: It's been a...  \n",
       "5   [0, 127): 'Great delivery vehicle: It's been a...  \n",
       "6   [0, 127): 'Great delivery vehicle: It's been a...  \n",
       "7   [0, 127): 'Great delivery vehicle: It's been a...  \n",
       "8   [0, 127): 'Great delivery vehicle: It's been a...  \n",
       "9   [0, 127): 'Great delivery vehicle: It's been a...  \n",
       "10  [0, 127): 'Great delivery vehicle: It's been a...  \n",
       "11  [0, 127): 'Great delivery vehicle: It's been a...  \n",
       "12  [0, 127): 'Great delivery vehicle: It's been a...  \n",
       "13  [0, 127): 'Great delivery vehicle: It's been a...  \n",
       "14  [0, 127): 'Great delivery vehicle: It's been a...  \n",
       "15  [0, 127): 'Great delivery vehicle: It's been a...  \n",
       "16  [0, 127): 'Great delivery vehicle: It's been a...  \n",
       "17  [0, 127): 'Great delivery vehicle: It's been a...  \n",
       "18  [0, 127): 'Great delivery vehicle: It's been a...  \n",
       "19  [0, 127): 'Great delivery vehicle: It's been a...  \n",
       "20  [0, 127): 'Great delivery vehicle: It's been a...  \n",
       "21  [0, 127): 'Great delivery vehicle: It's been a...  \n",
       "22  [0, 127): 'Great delivery vehicle: It's been a...  \n",
       "23  [0, 127): 'Great delivery vehicle: It's been a...  \n",
       "24  [0, 127): 'Great delivery vehicle: It's been a...  \n",
       "25  [128, 213): 'Havent repaired anything or repla...  \n",
       "26  [128, 213): 'Havent repaired anything or repla...  \n",
       "27  [128, 213): 'Havent repaired anything or repla...  \n",
       "28  [128, 213): 'Havent repaired anything or repla...  \n",
       "29  [128, 213): 'Havent repaired anything or repla...  \n",
       "30  [128, 213): 'Havent repaired anything or repla...  \n",
       "31  [128, 213): 'Havent repaired anything or repla...  \n",
       "32  [128, 213): 'Havent repaired anything or repla...  \n",
       "33  [128, 213): 'Havent repaired anything or repla...  \n",
       "34  [128, 213): 'Havent repaired anything or repla...  \n",
       "35  [128, 213): 'Havent repaired anything or repla...  \n",
       "36  [128, 213): 'Havent repaired anything or repla...  \n",
       "37  [128, 213): 'Havent repaired anything or repla...  \n",
       "38  [214, 271): 'Upgraded tires to Michelin LX ser...  \n",
       "39  [214, 271): 'Upgraded tires to Michelin LX ser...  \n",
       "40  [214, 271): 'Upgraded tires to Michelin LX ser...  \n",
       "41  [214, 271): 'Upgraded tires to Michelin LX ser...  \n",
       "42  [214, 271): 'Upgraded tires to Michelin LX ser...  \n",
       "43  [214, 271): 'Upgraded tires to Michelin LX ser...  \n",
       "44  [214, 271): 'Upgraded tires to Michelin LX ser...  \n",
       "45  [214, 271): 'Upgraded tires to Michelin LX ser...  \n",
       "46  [214, 271): 'Upgraded tires to Michelin LX ser...  \n",
       "47  [214, 271): 'Upgraded tires to Michelin LX ser...  \n",
       "48        [272, 301): 'Would buy another in a second'  \n",
       "49        [272, 301): 'Would buy another in a second'  \n",
       "50        [272, 301): 'Would buy another in a second'  \n",
       "51        [272, 301): 'Would buy another in a second'  \n",
       "52        [272, 301): 'Would buy another in a second'  \n",
       "53        [272, 301): 'Would buy another in a second'  "
      ]
     },
     "execution_count": 639,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_analyzed_review['syntax']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div id=\"spanArray\">\n",
       "            <div id=\"spans\" \n",
       "             style=\"background-color:#F0F0F0; border: 1px solid #E0E0E0; float:left; padding:10px;\">\n",
       "                <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>begin</th>\n",
       "      <th>end</th>\n",
       "      <th>begin_token</th>\n",
       "      <th>end_token</th>\n",
       "      <th>covered_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>127</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>Great delivery vehicle: It's been a great delivery vehicle for my cafe business good power, economy match easily taken care of.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>128</td>\n",
       "      <td>213</td>\n",
       "      <td>25</td>\n",
       "      <td>38</td>\n",
       "      <td>Havent repaired anything or replaced anything but tires and normal maintenance items.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>214</td>\n",
       "      <td>271</td>\n",
       "      <td>38</td>\n",
       "      <td>48</td>\n",
       "      <td>Upgraded tires to Michelin LX series helped fuel economy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>272</td>\n",
       "      <td>301</td>\n",
       "      <td>48</td>\n",
       "      <td>54</td>\n",
       "      <td>Would buy another in a second</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "            </div>\n",
       "            <div id=\"text\"\n",
       "             style=\"float:right; background-color:#F5F5F5; border: 1px solid #E0E0E0; width: 60%;\">\n",
       "                <div style=\"float:center; padding:10px\">\n",
       "                    <p style=\"font-family:monospace\">\n",
       "                        <span style=\"background-color:yellow\">Great delivery vehicle: It&#39;s been a great delivery vehicle for my cafe business good power, economy match easily taken care of.</span> <span style=\"background-color:yellow\">Havent repaired anything or replaced anything but tires and normal maintenance items.</span> <span style=\"background-color:yellow\">Upgraded tires to Michelin LX series helped fuel economy.</span> <span style=\"background-color:yellow\">Would buy another in a second\n",
       "                    </p>\n",
       "                </div>\n",
       "            </div>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<TokenSpanArray>\n",
       "[[0, 127): 'Great delivery vehicle: It's been a great delivery vehicle for my cafe [...]',\n",
       "   [128, 213): 'Havent repaired anything or replaced anything but tires and normal [...]',\n",
       "                  [214, 271): 'Upgraded tires to Michelin LX series helped fuel economy.',\n",
       "                                              [272, 301): 'Would buy another in a second']\n",
       "Length: 4, dtype: TokenSpanDtype"
      ]
     },
     "execution_count": 640,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_analyzed_review['syntax'][\"sentence\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject.text</th>\n",
       "      <th>sentence</th>\n",
       "      <th>object.text</th>\n",
       "      <th>action.verb.text</th>\n",
       "      <th>action.verb.tense</th>\n",
       "      <th>action.text</th>\n",
       "      <th>action.normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It</td>\n",
       "      <td>Great delivery vehicle: It's been a great deli...</td>\n",
       "      <td>been a great delivery vehicle</td>\n",
       "      <td>has</td>\n",
       "      <td>present</td>\n",
       "      <td>has</td>\n",
       "      <td>has</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>economy match</td>\n",
       "      <td>Great delivery vehicle: It's been a great deli...</td>\n",
       "      <td>care</td>\n",
       "      <td>take</td>\n",
       "      <td>past</td>\n",
       "      <td>taken</td>\n",
       "      <td>take</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tires</td>\n",
       "      <td>Upgraded tires to Michelin LX series helped f...</td>\n",
       "      <td>to Michelin LX series</td>\n",
       "      <td>Upgraded</td>\n",
       "      <td>past</td>\n",
       "      <td>Upgraded</td>\n",
       "      <td>Upgraded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Upgraded tires to Michelin LX series</td>\n",
       "      <td>Upgraded tires to Michelin LX series helped f...</td>\n",
       "      <td>fuel economy</td>\n",
       "      <td>help</td>\n",
       "      <td>past</td>\n",
       "      <td>helped</td>\n",
       "      <td>help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>another</td>\n",
       "      <td>Would buy another in a second</td>\n",
       "      <td>None</td>\n",
       "      <td>buy</td>\n",
       "      <td>future</td>\n",
       "      <td>Would buy</td>\n",
       "      <td>Would buy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           subject.text  \\\n",
       "0                                    It   \n",
       "1                         economy match   \n",
       "2                                 tires   \n",
       "3  Upgraded tires to Michelin LX series   \n",
       "4                               another   \n",
       "\n",
       "                                            sentence  \\\n",
       "0  Great delivery vehicle: It's been a great deli...   \n",
       "1  Great delivery vehicle: It's been a great deli...   \n",
       "2   Upgraded tires to Michelin LX series helped f...   \n",
       "3   Upgraded tires to Michelin LX series helped f...   \n",
       "4                      Would buy another in a second   \n",
       "\n",
       "                     object.text action.verb.text action.verb.tense  \\\n",
       "0  been a great delivery vehicle              has           present   \n",
       "1                           care             take              past   \n",
       "2          to Michelin LX series         Upgraded              past   \n",
       "3                   fuel economy             help              past   \n",
       "4                           None              buy            future   \n",
       "\n",
       "  action.text action.normalized  \n",
       "0         has               has  \n",
       "1       taken              take  \n",
       "2    Upgraded          Upgraded  \n",
       "3      helped              help  \n",
       "4   Would buy         Would buy  "
      ]
     },
     "execution_count": 641,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_analyzed_review['semantic_roles']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's concat the watson nlu keywowords based sentiment analysis dataframe(output of text enstensions for pandas) with its corresponding review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment.label</th>\n",
       "      <th>sentiment.score</th>\n",
       "      <th>relevance</th>\n",
       "      <th>emotion.sadness</th>\n",
       "      <th>emotion.joy</th>\n",
       "      <th>emotion.fear</th>\n",
       "      <th>emotion.disgust</th>\n",
       "      <th>emotion.anger</th>\n",
       "      <th>count</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Great delivery vehicle</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.982545</td>\n",
       "      <td>0.999486</td>\n",
       "      <td>0.039427</td>\n",
       "      <td>0.690797</td>\n",
       "      <td>0.045917</td>\n",
       "      <td>0.001771</td>\n",
       "      <td>0.011468</td>\n",
       "      <td>2</td>\n",
       "      <td>Great delivery vehicle: It's been a great deli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fuel economy</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.602034</td>\n",
       "      <td>0.605066</td>\n",
       "      <td>0.290023</td>\n",
       "      <td>0.131281</td>\n",
       "      <td>0.298556</td>\n",
       "      <td>0.001173</td>\n",
       "      <td>0.086125</td>\n",
       "      <td>1</td>\n",
       "      <td>Great delivery vehicle: It's been a great deli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tires</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.559100</td>\n",
       "      <td>0.304524</td>\n",
       "      <td>0.042803</td>\n",
       "      <td>0.056519</td>\n",
       "      <td>0.001143</td>\n",
       "      <td>0.145506</td>\n",
       "      <td>2</td>\n",
       "      <td>Great delivery vehicle: It's been a great deli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>care</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.982545</td>\n",
       "      <td>0.514348</td>\n",
       "      <td>0.039427</td>\n",
       "      <td>0.690797</td>\n",
       "      <td>0.045917</td>\n",
       "      <td>0.001771</td>\n",
       "      <td>0.011468</td>\n",
       "      <td>1</td>\n",
       "      <td>Great delivery vehicle: It's been a great deli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Michelin LX series</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.602034</td>\n",
       "      <td>0.411711</td>\n",
       "      <td>0.290023</td>\n",
       "      <td>0.131281</td>\n",
       "      <td>0.298556</td>\n",
       "      <td>0.001173</td>\n",
       "      <td>0.086125</td>\n",
       "      <td>1</td>\n",
       "      <td>Great delivery vehicle: It's been a great deli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cafe business</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.982545</td>\n",
       "      <td>0.396141</td>\n",
       "      <td>0.039427</td>\n",
       "      <td>0.690797</td>\n",
       "      <td>0.045917</td>\n",
       "      <td>0.001771</td>\n",
       "      <td>0.011468</td>\n",
       "      <td>1</td>\n",
       "      <td>Great delivery vehicle: It's been a great deli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>good power</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.982545</td>\n",
       "      <td>0.357562</td>\n",
       "      <td>0.039427</td>\n",
       "      <td>0.690797</td>\n",
       "      <td>0.045917</td>\n",
       "      <td>0.001771</td>\n",
       "      <td>0.011468</td>\n",
       "      <td>1</td>\n",
       "      <td>Great delivery vehicle: It's been a great deli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>economy match</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.982545</td>\n",
       "      <td>0.317306</td>\n",
       "      <td>0.039427</td>\n",
       "      <td>0.690797</td>\n",
       "      <td>0.045917</td>\n",
       "      <td>0.001771</td>\n",
       "      <td>0.011468</td>\n",
       "      <td>1</td>\n",
       "      <td>Great delivery vehicle: It's been a great deli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>normal maintenance items</td>\n",
       "      <td>negative</td>\n",
       "      <td>-0.561817</td>\n",
       "      <td>0.307532</td>\n",
       "      <td>0.261543</td>\n",
       "      <td>0.025168</td>\n",
       "      <td>0.009511</td>\n",
       "      <td>0.013281</td>\n",
       "      <td>0.192068</td>\n",
       "      <td>1</td>\n",
       "      <td>Great delivery vehicle: It's been a great deli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Havent</td>\n",
       "      <td>negative</td>\n",
       "      <td>-0.561817</td>\n",
       "      <td>0.256558</td>\n",
       "      <td>0.261543</td>\n",
       "      <td>0.025168</td>\n",
       "      <td>0.009511</td>\n",
       "      <td>0.013281</td>\n",
       "      <td>0.192068</td>\n",
       "      <td>1</td>\n",
       "      <td>Great delivery vehicle: It's been a great deli...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       text sentiment.label  sentiment.score  relevance  \\\n",
       "0    Great delivery vehicle        positive         0.982545   0.999486   \n",
       "1              fuel economy        positive         0.602034   0.605066   \n",
       "2                     tires         neutral         0.000000   0.559100   \n",
       "3                      care        positive         0.982545   0.514348   \n",
       "4        Michelin LX series        positive         0.602034   0.411711   \n",
       "5             cafe business        positive         0.982545   0.396141   \n",
       "6                good power        positive         0.982545   0.357562   \n",
       "7             economy match        positive         0.982545   0.317306   \n",
       "8  normal maintenance items        negative        -0.561817   0.307532   \n",
       "9                    Havent        negative        -0.561817   0.256558   \n",
       "\n",
       "   emotion.sadness  emotion.joy  emotion.fear  emotion.disgust  emotion.anger  \\\n",
       "0         0.039427     0.690797      0.045917         0.001771       0.011468   \n",
       "1         0.290023     0.131281      0.298556         0.001173       0.086125   \n",
       "2         0.304524     0.042803      0.056519         0.001143       0.145506   \n",
       "3         0.039427     0.690797      0.045917         0.001771       0.011468   \n",
       "4         0.290023     0.131281      0.298556         0.001173       0.086125   \n",
       "5         0.039427     0.690797      0.045917         0.001771       0.011468   \n",
       "6         0.039427     0.690797      0.045917         0.001771       0.011468   \n",
       "7         0.039427     0.690797      0.045917         0.001771       0.011468   \n",
       "8         0.261543     0.025168      0.009511         0.013281       0.192068   \n",
       "9         0.261543     0.025168      0.009511         0.013281       0.192068   \n",
       "\n",
       "   count                                                  0  \n",
       "0      2  Great delivery vehicle: It's been a great deli...  \n",
       "1      1  Great delivery vehicle: It's been a great deli...  \n",
       "2      2  Great delivery vehicle: It's been a great deli...  \n",
       "3      1  Great delivery vehicle: It's been a great deli...  \n",
       "4      1  Great delivery vehicle: It's been a great deli...  \n",
       "5      1  Great delivery vehicle: It's been a great deli...  \n",
       "6      1  Great delivery vehicle: It's been a great deli...  \n",
       "7      1  Great delivery vehicle: It's been a great deli...  \n",
       "8      1  Great delivery vehicle: It's been a great deli...  \n",
       "9      1  Great delivery vehicle: It's been a great deli...  "
      ]
     },
     "execution_count": 642,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_review = pd.concat ([dfs_analyzed_review['keywords'] , pd.Series([response_review['analyzed_text']]*len(dfs_analyzed_review['keywords']))], axis = 1)\n",
    "keywords_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's merge the above dataframe with its corresponding review's information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment.label</th>\n",
       "      <th>sentiment.score</th>\n",
       "      <th>relevance</th>\n",
       "      <th>emotion.sadness</th>\n",
       "      <th>emotion.joy</th>\n",
       "      <th>emotion.fear</th>\n",
       "      <th>emotion.disgust</th>\n",
       "      <th>emotion.anger</th>\n",
       "      <th>count</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Review_Date</th>\n",
       "      <th>Author_Name</th>\n",
       "      <th>Vehicle_Title</th>\n",
       "      <th>Review_Title</th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review_Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Great delivery vehicle</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.982545</td>\n",
       "      <td>0.999486</td>\n",
       "      <td>0.039427</td>\n",
       "      <td>0.690797</td>\n",
       "      <td>0.045917</td>\n",
       "      <td>0.001771</td>\n",
       "      <td>0.011468</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>on 10/13/05 15:30 PM (PDT)</td>\n",
       "      <td>roadking</td>\n",
       "      <td>2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...</td>\n",
       "      <td>Great delivery vehicle</td>\n",
       "      <td>It's been a great delivery vehicle for my caf...</td>\n",
       "      <td>4.625</td>\n",
       "      <td>Great delivery vehicle: It's been a great deli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fuel economy</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.602034</td>\n",
       "      <td>0.605066</td>\n",
       "      <td>0.290023</td>\n",
       "      <td>0.131281</td>\n",
       "      <td>0.298556</td>\n",
       "      <td>0.001173</td>\n",
       "      <td>0.086125</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>on 10/13/05 15:30 PM (PDT)</td>\n",
       "      <td>roadking</td>\n",
       "      <td>2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...</td>\n",
       "      <td>Great delivery vehicle</td>\n",
       "      <td>It's been a great delivery vehicle for my caf...</td>\n",
       "      <td>4.625</td>\n",
       "      <td>Great delivery vehicle: It's been a great deli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tires</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.559100</td>\n",
       "      <td>0.304524</td>\n",
       "      <td>0.042803</td>\n",
       "      <td>0.056519</td>\n",
       "      <td>0.001143</td>\n",
       "      <td>0.145506</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>on 10/13/05 15:30 PM (PDT)</td>\n",
       "      <td>roadking</td>\n",
       "      <td>2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...</td>\n",
       "      <td>Great delivery vehicle</td>\n",
       "      <td>It's been a great delivery vehicle for my caf...</td>\n",
       "      <td>4.625</td>\n",
       "      <td>Great delivery vehicle: It's been a great deli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>care</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.982545</td>\n",
       "      <td>0.514348</td>\n",
       "      <td>0.039427</td>\n",
       "      <td>0.690797</td>\n",
       "      <td>0.045917</td>\n",
       "      <td>0.001771</td>\n",
       "      <td>0.011468</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>on 10/13/05 15:30 PM (PDT)</td>\n",
       "      <td>roadking</td>\n",
       "      <td>2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...</td>\n",
       "      <td>Great delivery vehicle</td>\n",
       "      <td>It's been a great delivery vehicle for my caf...</td>\n",
       "      <td>4.625</td>\n",
       "      <td>Great delivery vehicle: It's been a great deli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Michelin LX series</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.602034</td>\n",
       "      <td>0.411711</td>\n",
       "      <td>0.290023</td>\n",
       "      <td>0.131281</td>\n",
       "      <td>0.298556</td>\n",
       "      <td>0.001173</td>\n",
       "      <td>0.086125</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>on 10/13/05 15:30 PM (PDT)</td>\n",
       "      <td>roadking</td>\n",
       "      <td>2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...</td>\n",
       "      <td>Great delivery vehicle</td>\n",
       "      <td>It's been a great delivery vehicle for my caf...</td>\n",
       "      <td>4.625</td>\n",
       "      <td>Great delivery vehicle: It's been a great deli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cafe business</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.982545</td>\n",
       "      <td>0.396141</td>\n",
       "      <td>0.039427</td>\n",
       "      <td>0.690797</td>\n",
       "      <td>0.045917</td>\n",
       "      <td>0.001771</td>\n",
       "      <td>0.011468</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>on 10/13/05 15:30 PM (PDT)</td>\n",
       "      <td>roadking</td>\n",
       "      <td>2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...</td>\n",
       "      <td>Great delivery vehicle</td>\n",
       "      <td>It's been a great delivery vehicle for my caf...</td>\n",
       "      <td>4.625</td>\n",
       "      <td>Great delivery vehicle: It's been a great deli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>good power</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.982545</td>\n",
       "      <td>0.357562</td>\n",
       "      <td>0.039427</td>\n",
       "      <td>0.690797</td>\n",
       "      <td>0.045917</td>\n",
       "      <td>0.001771</td>\n",
       "      <td>0.011468</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>on 10/13/05 15:30 PM (PDT)</td>\n",
       "      <td>roadking</td>\n",
       "      <td>2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...</td>\n",
       "      <td>Great delivery vehicle</td>\n",
       "      <td>It's been a great delivery vehicle for my caf...</td>\n",
       "      <td>4.625</td>\n",
       "      <td>Great delivery vehicle: It's been a great deli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>economy match</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.982545</td>\n",
       "      <td>0.317306</td>\n",
       "      <td>0.039427</td>\n",
       "      <td>0.690797</td>\n",
       "      <td>0.045917</td>\n",
       "      <td>0.001771</td>\n",
       "      <td>0.011468</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>on 10/13/05 15:30 PM (PDT)</td>\n",
       "      <td>roadking</td>\n",
       "      <td>2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...</td>\n",
       "      <td>Great delivery vehicle</td>\n",
       "      <td>It's been a great delivery vehicle for my caf...</td>\n",
       "      <td>4.625</td>\n",
       "      <td>Great delivery vehicle: It's been a great deli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>normal maintenance items</td>\n",
       "      <td>negative</td>\n",
       "      <td>-0.561817</td>\n",
       "      <td>0.307532</td>\n",
       "      <td>0.261543</td>\n",
       "      <td>0.025168</td>\n",
       "      <td>0.009511</td>\n",
       "      <td>0.013281</td>\n",
       "      <td>0.192068</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>on 10/13/05 15:30 PM (PDT)</td>\n",
       "      <td>roadking</td>\n",
       "      <td>2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...</td>\n",
       "      <td>Great delivery vehicle</td>\n",
       "      <td>It's been a great delivery vehicle for my caf...</td>\n",
       "      <td>4.625</td>\n",
       "      <td>Great delivery vehicle: It's been a great deli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Havent</td>\n",
       "      <td>negative</td>\n",
       "      <td>-0.561817</td>\n",
       "      <td>0.256558</td>\n",
       "      <td>0.261543</td>\n",
       "      <td>0.025168</td>\n",
       "      <td>0.009511</td>\n",
       "      <td>0.013281</td>\n",
       "      <td>0.192068</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>on 10/13/05 15:30 PM (PDT)</td>\n",
       "      <td>roadking</td>\n",
       "      <td>2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...</td>\n",
       "      <td>Great delivery vehicle</td>\n",
       "      <td>It's been a great delivery vehicle for my caf...</td>\n",
       "      <td>4.625</td>\n",
       "      <td>Great delivery vehicle: It's been a great deli...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       text sentiment.label  sentiment.score  relevance  \\\n",
       "0    Great delivery vehicle        positive         0.982545   0.999486   \n",
       "1              fuel economy        positive         0.602034   0.605066   \n",
       "2                     tires         neutral         0.000000   0.559100   \n",
       "3                      care        positive         0.982545   0.514348   \n",
       "4        Michelin LX series        positive         0.602034   0.411711   \n",
       "5             cafe business        positive         0.982545   0.396141   \n",
       "6                good power        positive         0.982545   0.357562   \n",
       "7             economy match        positive         0.982545   0.317306   \n",
       "8  normal maintenance items        negative        -0.561817   0.307532   \n",
       "9                    Havent        negative        -0.561817   0.256558   \n",
       "\n",
       "   emotion.sadness  emotion.joy  emotion.fear  emotion.disgust  emotion.anger  \\\n",
       "0         0.039427     0.690797      0.045917         0.001771       0.011468   \n",
       "1         0.290023     0.131281      0.298556         0.001173       0.086125   \n",
       "2         0.304524     0.042803      0.056519         0.001143       0.145506   \n",
       "3         0.039427     0.690797      0.045917         0.001771       0.011468   \n",
       "4         0.290023     0.131281      0.298556         0.001173       0.086125   \n",
       "5         0.039427     0.690797      0.045917         0.001771       0.011468   \n",
       "6         0.039427     0.690797      0.045917         0.001771       0.011468   \n",
       "7         0.039427     0.690797      0.045917         0.001771       0.011468   \n",
       "8         0.261543     0.025168      0.009511         0.013281       0.192068   \n",
       "9         0.261543     0.025168      0.009511         0.013281       0.192068   \n",
       "\n",
       "   count Unnamed: 0                  Review_Date Author_Name  \\\n",
       "0      2          0   on 10/13/05 15:30 PM (PDT)   roadking    \n",
       "1      1          0   on 10/13/05 15:30 PM (PDT)   roadking    \n",
       "2      2          0   on 10/13/05 15:30 PM (PDT)   roadking    \n",
       "3      1          0   on 10/13/05 15:30 PM (PDT)   roadking    \n",
       "4      1          0   on 10/13/05 15:30 PM (PDT)   roadking    \n",
       "5      1          0   on 10/13/05 15:30 PM (PDT)   roadking    \n",
       "6      1          0   on 10/13/05 15:30 PM (PDT)   roadking    \n",
       "7      1          0   on 10/13/05 15:30 PM (PDT)   roadking    \n",
       "8      1          0   on 10/13/05 15:30 PM (PDT)   roadking    \n",
       "9      1          0   on 10/13/05 15:30 PM (PDT)   roadking    \n",
       "\n",
       "                                       Vehicle_Title            Review_Title  \\\n",
       "0  2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...  Great delivery vehicle   \n",
       "1  2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...  Great delivery vehicle   \n",
       "2  2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...  Great delivery vehicle   \n",
       "3  2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...  Great delivery vehicle   \n",
       "4  2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...  Great delivery vehicle   \n",
       "5  2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...  Great delivery vehicle   \n",
       "6  2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...  Great delivery vehicle   \n",
       "7  2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...  Great delivery vehicle   \n",
       "8  2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...  Great delivery vehicle   \n",
       "9  2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...  Great delivery vehicle   \n",
       "\n",
       "                                              Review  Rating  \\\n",
       "0   It's been a great delivery vehicle for my caf...   4.625   \n",
       "1   It's been a great delivery vehicle for my caf...   4.625   \n",
       "2   It's been a great delivery vehicle for my caf...   4.625   \n",
       "3   It's been a great delivery vehicle for my caf...   4.625   \n",
       "4   It's been a great delivery vehicle for my caf...   4.625   \n",
       "5   It's been a great delivery vehicle for my caf...   4.625   \n",
       "6   It's been a great delivery vehicle for my caf...   4.625   \n",
       "7   It's been a great delivery vehicle for my caf...   4.625   \n",
       "8   It's been a great delivery vehicle for my caf...   4.625   \n",
       "9   It's been a great delivery vehicle for my caf...   4.625   \n",
       "\n",
       "                                      Review_Content  \n",
       "0  Great delivery vehicle: It's been a great deli...  \n",
       "1  Great delivery vehicle: It's been a great deli...  \n",
       "2  Great delivery vehicle: It's been a great deli...  \n",
       "3  Great delivery vehicle: It's been a great deli...  \n",
       "4  Great delivery vehicle: It's been a great deli...  \n",
       "5  Great delivery vehicle: It's been a great deli...  \n",
       "6  Great delivery vehicle: It's been a great deli...  \n",
       "7  Great delivery vehicle: It's been a great deli...  \n",
       "8  Great delivery vehicle: It's been a great deli...  \n",
       "9  Great delivery vehicle: It's been a great deli...  "
      ]
     },
     "execution_count": 643,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(keywords_review.merge(dodge_rev,left_on=0, right_on = dodge_rev.Review_Content)).drop(columns=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we can apply same operations on multiple entries from our car reviews dataset and use the outcome for correlation analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If I send too much request to Watson NLU I will get an error!\n",
    "response = dodge_rev['Review_Content'][:750].dropna().apply(lambda x: natural_language_understanding.analyze(\n",
    "        text=x,\n",
    "        return_analyzed_text=True,\n",
    "        features=nlu.Features(\n",
    "            entities=nlu.EntitiesOptions(sentiment=True, mentions=True),\n",
    "            keywords=nlu.KeywordsOptions(sentiment=True, emotion=True),\n",
    "            relations=nlu.RelationsOptions(),\n",
    "            semantic_roles=nlu.SemanticRolesOptions(),\n",
    "            syntax=nlu.SyntaxOptions(sentences=True, \n",
    "                                     tokens=nlu.SyntaxOptionsTokens(lemma=True, part_of_speech=True)),\n",
    "        )).get_result())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      {'usage': {'text_units': 1, 'text_characters':...\n",
       "1      {'usage': {'text_units': 1, 'text_characters':...\n",
       "2      {'usage': {'text_units': 1, 'text_characters':...\n",
       "3      {'usage': {'text_units': 1, 'text_characters':...\n",
       "4      {'usage': {'text_units': 1, 'text_characters':...\n",
       "                             ...                        \n",
       "745    {'usage': {'text_units': 1, 'text_characters':...\n",
       "746    {'usage': {'text_units': 1, 'text_characters':...\n",
       "747    {'usage': {'text_units': 1, 'text_characters':...\n",
       "748    {'usage': {'text_units': 1, 'text_characters':...\n",
       "749    {'usage': {'text_units': 1, 'text_characters':...\n",
       "Name: Review_Content, Length: 662, dtype: object"
      ]
     },
     "execution_count": 646,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'usage': {'text_units': 1, 'text_characters': 123, 'features': 5},\n",
       " 'syntax': {'tokens': [{'text': 'Sweet',\n",
       "    'part_of_speech': 'ADJ',\n",
       "    'location': [0, 5],\n",
       "    'lemma': 'sweet'},\n",
       "   {'text': 'van',\n",
       "    'part_of_speech': 'NOUN',\n",
       "    'location': [6, 9],\n",
       "    'lemma': 'van'},\n",
       "   {'text': ':', 'part_of_speech': 'PUNCT', 'location': [9, 10], 'lemma': ':'},\n",
       "   {'text': 'This',\n",
       "    'part_of_speech': 'DET',\n",
       "    'location': [11, 15],\n",
       "    'lemma': 'this'},\n",
       "   {'text': 'van',\n",
       "    'part_of_speech': 'NOUN',\n",
       "    'location': [16, 19],\n",
       "    'lemma': 'van'},\n",
       "   {'text': 'rocks',\n",
       "    'part_of_speech': 'VERB',\n",
       "    'location': [20, 25],\n",
       "    'lemma': 'rock'},\n",
       "   {'text': 'its',\n",
       "    'part_of_speech': 'PRON',\n",
       "    'location': [26, 29],\n",
       "    'lemma': 'its'},\n",
       "   {'text': 'the',\n",
       "    'part_of_speech': 'DET',\n",
       "    'location': [30, 33],\n",
       "    'lemma': 'the'},\n",
       "   {'text': 'best',\n",
       "    'part_of_speech': 'ADJ',\n",
       "    'location': [34, 38],\n",
       "    'lemma': 'good'},\n",
       "   {'text': ',', 'part_of_speech': 'PUNCT', 'location': [38, 39]},\n",
       "   {'text': 'lots',\n",
       "    'part_of_speech': 'NOUN',\n",
       "    'location': [40, 44],\n",
       "    'lemma': 'lot'},\n",
       "   {'text': 'of',\n",
       "    'part_of_speech': 'ADP',\n",
       "    'location': [45, 47],\n",
       "    'lemma': 'of'},\n",
       "   {'text': 'room',\n",
       "    'part_of_speech': 'NOUN',\n",
       "    'location': [49, 53],\n",
       "    'lemma': 'room'},\n",
       "   {'text': '.', 'part_of_speech': 'PUNCT', 'location': [53, 54]},\n",
       "   {'text': 'I', 'part_of_speech': 'PRON', 'location': [55, 56], 'lemma': 'I'},\n",
       "   {'text': 'carry',\n",
       "    'part_of_speech': 'VERB',\n",
       "    'location': [57, 62],\n",
       "    'lemma': 'carry'},\n",
       "   {'text': 'a', 'part_of_speech': 'DET', 'location': [63, 64], 'lemma': 'a'},\n",
       "   {'text': 'lot',\n",
       "    'part_of_speech': 'NOUN',\n",
       "    'location': [65, 68],\n",
       "    'lemma': 'lot'},\n",
       "   {'text': 'of',\n",
       "    'part_of_speech': 'ADP',\n",
       "    'location': [69, 71],\n",
       "    'lemma': 'of'},\n",
       "   {'text': 'cargo',\n",
       "    'part_of_speech': 'NOUN',\n",
       "    'location': [72, 77],\n",
       "    'lemma': 'cargo'},\n",
       "   {'text': 'in',\n",
       "    'part_of_speech': 'ADP',\n",
       "    'location': [78, 80],\n",
       "    'lemma': 'in'},\n",
       "   {'text': 'mine',\n",
       "    'part_of_speech': 'PRON',\n",
       "    'location': [81, 85],\n",
       "    'lemma': 'I'},\n",
       "   {'text': 'and',\n",
       "    'part_of_speech': 'CCONJ',\n",
       "    'location': [87, 90],\n",
       "    'lemma': 'and'},\n",
       "   {'text': 'i', 'part_of_speech': 'PRON', 'location': [91, 92]},\n",
       "   {'text': 'tow',\n",
       "    'part_of_speech': 'VERB',\n",
       "    'location': [93, 96],\n",
       "    'lemma': 'tow'},\n",
       "   {'text': 'a', 'part_of_speech': 'DET', 'location': [97, 98], 'lemma': 'a'},\n",
       "   {'text': 'lot',\n",
       "    'part_of_speech': 'NOUN',\n",
       "    'location': [99, 102],\n",
       "    'lemma': 'lot'},\n",
       "   {'text': 'too',\n",
       "    'part_of_speech': 'ADV',\n",
       "    'location': [103, 106],\n",
       "    'lemma': 'too'},\n",
       "   {'text': ',', 'part_of_speech': 'PUNCT', 'location': [106, 107]},\n",
       "   {'text': 'it',\n",
       "    'part_of_speech': 'PRON',\n",
       "    'location': [108, 110],\n",
       "    'lemma': 'it'},\n",
       "   {'text': 'works',\n",
       "    'part_of_speech': 'VERB',\n",
       "    'location': [111, 116],\n",
       "    'lemma': 'work'},\n",
       "   {'text': 'great',\n",
       "    'part_of_speech': 'ADJ',\n",
       "    'location': [117, 122],\n",
       "    'lemma': 'great'},\n",
       "   {'text': '!', 'part_of_speech': 'PUNCT', 'location': [122, 123]}],\n",
       "  'sentences': [{'text': 'Sweet van: This van rocks its the best, lots of',\n",
       "    'location': [0, 47]},\n",
       "   {'text': 'room.', 'location': [49, 54]},\n",
       "   {'text': 'I carry a lot of cargo in mine', 'location': [55, 85]},\n",
       "   {'text': 'and i tow a lot too, it works great!', 'location': [87, 123]}]},\n",
       " 'semantic_roles': [{'subject': {'text': 'This van'},\n",
       "   'sentence': 'Sweet van: This van rocks its the best, lots of',\n",
       "   'object': {'text': 'its the best, lots'},\n",
       "   'action': {'verb': {'text': 'rock', 'tense': 'present'},\n",
       "    'text': 'rocks',\n",
       "    'normalized': 'rock'}},\n",
       "  {'subject': {'text': 'room. I'},\n",
       "   'sentence': ' room. I carry a lot of cargo in mine',\n",
       "   'object': {'text': 'a lot of cargo'},\n",
       "   'action': {'verb': {'text': 'carry', 'tense': 'present'},\n",
       "    'text': 'carry',\n",
       "    'normalized': 'carry'}},\n",
       "  {'subject': {'text': 'it'},\n",
       "   'sentence': ' and i tow a lot too, it works great!',\n",
       "   'object': {'text': 'great'},\n",
       "   'action': {'verb': {'text': 'work', 'tense': 'present'},\n",
       "    'text': 'works',\n",
       "    'normalized': 'work'}}],\n",
       " 'relations': [],\n",
       " 'language': 'en',\n",
       " 'keywords': [{'text': 'Sweet van',\n",
       "   'sentiment': {'score': 0.97177, 'label': 'positive'},\n",
       "   'relevance': 0.995341,\n",
       "   'emotion': {'sadness': 0.039646,\n",
       "    'joy': 0.90253,\n",
       "    'fear': 0.005311,\n",
       "    'disgust': 0.013658,\n",
       "    'anger': 0.018734},\n",
       "   'count': 1},\n",
       "  {'text': 'van',\n",
       "   'sentiment': {'score': 0.97177, 'label': 'positive'},\n",
       "   'relevance': 0.644379,\n",
       "   'emotion': {'sadness': 0.039646,\n",
       "    'joy': 0.90253,\n",
       "    'fear': 0.005311,\n",
       "    'disgust': 0.013658,\n",
       "    'anger': 0.018734},\n",
       "   'count': 1},\n",
       "  {'text': 'lots of \\rroom',\n",
       "   'sentiment': {'score': 0.97177, 'label': 'positive'},\n",
       "   'relevance': 0.617445,\n",
       "   'emotion': {'sadness': 0, 'joy': 0, 'fear': 0, 'disgust': 0, 'anger': 0},\n",
       "   'count': 1},\n",
       "  {'text': 'lot',\n",
       "   'sentiment': {'score': 0.989437, 'label': 'positive'},\n",
       "   'relevance': 0.551342,\n",
       "   'emotion': {'sadness': 0.042466,\n",
       "    'joy': 0.875307,\n",
       "    'fear': 0.015478,\n",
       "    'disgust': 0.009036,\n",
       "    'anger': 0.029832},\n",
       "   'count': 1},\n",
       "  {'text': 'lot of cargo',\n",
       "   'sentiment': {'score': 0, 'label': 'neutral'},\n",
       "   'relevance': 0.302214,\n",
       "   'emotion': {'sadness': 0.287176,\n",
       "    'joy': 0.20337,\n",
       "    'fear': 0.128742,\n",
       "    'disgust': 0.025619,\n",
       "    'anger': 0.135182},\n",
       "   'count': 1}],\n",
       " 'entities': [],\n",
       " 'analyzed_text': 'Sweet van: This van rocks its the best, lots of \\rroom. I carry a lot of cargo in mine \\rand i tow a lot too, it works great!'}"
      ]
     },
     "execution_count": 647,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review#:8\n",
      "Error Message: The following span(s) did not align with the end offset\n",
      "of any token:\n",
      "   span_index  span_begin  span_end\n",
      "1           1          50        61\n",
      "Review#:19\n",
      "Error Message: The following span(s) did not align with the end offset\n",
      "of any token:\n",
      "   span_index  span_begin  span_end\n",
      "4           4         421       729\n",
      "Review#:20\n",
      "Error Message: The following span(s) did not align with the end offset\n",
      "of any token:\n",
      "   span_index  span_begin  span_end\n",
      "0           0           0       244\n",
      "Review#:41\n",
      "Error Message: The following span(s) did not align with the end offset\n",
      "of any token:\n",
      "   span_index  span_begin  span_end\n",
      "5           5         463       556\n",
      "Review#:86\n",
      "Error Message: The following span(s) did not align with the end offset\n",
      "of any token:\n",
      "   span_index  span_begin  span_end\n",
      "6           6         401       455\n",
      "Review#:101\n",
      "Error Message: The following span(s) did not align with the end offset\n",
      "of any token:\n",
      "   span_index  span_begin  span_end\n",
      "2           2         136       180\n",
      "Review#:104\n",
      "Error Message: The following span(s) did not align with the end offset\n",
      "of any token:\n",
      "    span_index  span_begin  span_end\n",
      "11          11         712       720\n",
      "Review#:117\n",
      "Error Message: The following span(s) did not align with the end offset\n",
      "of any token:\n",
      "   span_index  span_begin  span_end\n",
      "6           6         298       317\n",
      "Review#:184\n",
      "Error Message: The following span(s) did not align with the end offset\n",
      "of any token:\n",
      "   span_index  span_begin  span_end\n",
      "1           1          24       148\n",
      "Review#:201\n",
      "Error Message: The following span(s) did not align with the end offset\n",
      "of any token:\n",
      "    span_index  span_begin  span_end\n",
      "16          16         666       724\n",
      "Review#:291\n",
      "Error Message: The following span(s) did not align with the end offset\n",
      "of any token:\n",
      "   span_index  span_begin  span_end\n",
      "0           0           0        42\n",
      "Review#:297\n",
      "Error Message: The following span(s) did not align with the end offset\n",
      "of any token:\n",
      "   span_index  span_begin  span_end\n",
      "0           0           0        55\n",
      "Review#:306\n",
      "Error Message: The following span(s) did not align with the end offset\n",
      "of any token:\n",
      "   span_index  span_begin  span_end\n",
      "0           0           0        49\n",
      "Review#:310\n",
      "Error Message: The following span(s) did not align with the end offset\n",
      "of any token:\n",
      "   span_index  span_begin  span_end\n",
      "0           0           0        52\n",
      "Review#:337\n",
      "Error Message: The following span(s) did not align with the end offset\n",
      "of any token:\n",
      "   span_index  span_begin  span_end\n",
      "4           4         324       386\n",
      "Review#:346\n",
      "Error Message: The following span(s) did not align with the end offset\n",
      "of any token:\n",
      "   span_index  span_begin  span_end\n",
      "0           0           0        42\n",
      "Review#:349\n",
      "Error Message: The following span(s) did not align with the end offset\n",
      "of any token:\n",
      "   span_index  span_begin  span_end\n",
      "0           0           0        18\n",
      "Review#:381\n",
      "Error Message: The following span(s) did not align with the end offset\n",
      "of any token:\n",
      "   span_index  span_begin  span_end\n",
      "3           3         251       336\n",
      "Review#:390\n",
      "Error Message: The following span(s) did not align with the end offset\n",
      "of any token:\n",
      "   span_index  span_begin  span_end\n",
      "0           0           0        53\n",
      "Review#:401\n",
      "Error Message: The following span(s) did not align with the end offset\n",
      "of any token:\n",
      "   span_index  span_begin  span_end\n",
      "0           0           0        47\n",
      "Review#:423\n",
      "Error Message: The following span(s) did not align with the end offset\n",
      "of any token:\n",
      "   span_index  span_begin  span_end\n",
      "0           0           0        56\n",
      "Review#:431\n",
      "Error Message: The following span(s) did not align with the end offset\n",
      "of any token:\n",
      "   span_index  span_begin  span_end\n",
      "0           0           0        49\n",
      "Review#:434\n",
      "Error Message: The following span(s) did not align with the end offset\n",
      "of any token:\n",
      "    span_index  span_begin  span_end\n",
      "20          20         502       538\n",
      "Review#:440\n",
      "Error Message: The following span(s) did not align with the end offset\n",
      "of any token:\n",
      "   span_index  span_begin  span_end\n",
      "1           1          38        45\n",
      "Review#:445\n",
      "Error Message: The following span(s) did not align with the end offset\n",
      "of any token:\n",
      "   span_index  span_begin  span_end\n",
      "1           1          34        47\n",
      "Review#:450\n",
      "Error Message: The following span(s) did not align with the end offset\n",
      "of any token:\n",
      "   span_index  span_begin  span_end\n",
      "0           0           0        53\n",
      "Review#:463\n",
      "Error Message: The following span(s) did not align with the end offset\n",
      "of any token:\n",
      "   span_index  span_begin  span_end\n",
      "1           1          48        50\n",
      "Review#:476\n",
      "Error Message: The following span(s) did not align with the end offset\n",
      "of any token:\n",
      "   span_index  span_begin  span_end\n",
      "0           0           0        51\n",
      "Review#:498\n",
      "Error Message: The following span(s) did not align with the end offset\n",
      "of any token:\n",
      "   span_index  span_begin  span_end\n",
      "0           0           0        50\n",
      "Review#:514\n",
      "Error Message: The following span(s) did not align with the end offset\n",
      "of any token:\n",
      "   span_index  span_begin  span_end\n",
      "1           1          46        55\n",
      "Review#:517\n",
      "Error Message: The following span(s) did not align with the end offset\n",
      "of any token:\n",
      "   span_index  span_begin  span_end\n",
      "0           0           0        46\n",
      "Review#:557\n",
      "Error Message: The following span(s) did not align with the end offset\n",
      "of any token:\n",
      "    span_index  span_begin  span_end\n",
      "11          11         682       730\n",
      "Review#:563\n",
      "Error Message: The following span(s) did not align with the end offset\n",
      "of any token:\n",
      "   span_index  span_begin  span_end\n",
      "0           0           0        67\n",
      "Review#:570\n",
      "Error Message: The following span(s) did not align with the end offset\n",
      "of any token:\n",
      "   span_index  span_begin  span_end\n",
      "0           0           0       543\n",
      "Review#:671\n",
      "Error Message: The following span(s) did not align with the end offset\n",
      "of any token:\n",
      "   span_index  span_begin  span_end\n",
      "0           0           0        60\n",
      "Review#:708\n",
      "Error Message: The following span(s) did not align with the end offset\n",
      "of any token:\n",
      "    span_index  span_begin  span_end\n",
      "12          12         660       673\n",
      "Review#:713\n",
      "Error Message: The following span(s) did not align with the end offset\n",
      "of any token:\n",
      "   span_index  span_begin  span_end\n",
      "0           0           0        68\n",
      "Review#:748\n",
      "Error Message: The following span(s) did not align with the end offset\n",
      "of any token:\n",
      "   span_index  span_begin  span_end\n",
      "6           6         469       517\n"
     ]
    }
   ],
   "source": [
    "# Apply the text extension for pandas to parse the Watson NLU responses in the loop\n",
    "temp_response = []\n",
    "temp_parsed = []\n",
    "for i,t in response.iteritems():\n",
    "    try:\n",
    "        temp = tp.io.watson.nlu.parse_response(t)\n",
    "        temp_response.append(t)\n",
    "        temp_parsed.append(temp)\n",
    "    except Exception as e:\n",
    "        print(f\"Review#:{i}\")\n",
    "        print(f\"Error Message: {e}\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat each of the watson nlu keywowords based sentiment analysis dataframe(output of text enstensions for pandas) with its corresponding review.\n",
    "keywords_review = [pd.concat ([parsed_review['keywords'] , pd.Series([r['analyzed_text']]*len(parsed_review['keywords']))], axis = 1) for (parsed_review,r) in zip(temp_parsed,pd.Series(temp_response))]\n",
    "#keywords_review # list of dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment.label</th>\n",
       "      <th>sentiment.score</th>\n",
       "      <th>relevance</th>\n",
       "      <th>emotion.sadness</th>\n",
       "      <th>emotion.joy</th>\n",
       "      <th>emotion.fear</th>\n",
       "      <th>emotion.disgust</th>\n",
       "      <th>emotion.anger</th>\n",
       "      <th>count</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Great delivery vehicle</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.982545</td>\n",
       "      <td>0.999486</td>\n",
       "      <td>0.039427</td>\n",
       "      <td>0.690797</td>\n",
       "      <td>0.045917</td>\n",
       "      <td>0.001771</td>\n",
       "      <td>0.011468</td>\n",
       "      <td>2</td>\n",
       "      <td>Great delivery vehicle: It's been a great deli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fuel economy</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.602034</td>\n",
       "      <td>0.605066</td>\n",
       "      <td>0.290023</td>\n",
       "      <td>0.131281</td>\n",
       "      <td>0.298556</td>\n",
       "      <td>0.001173</td>\n",
       "      <td>0.086125</td>\n",
       "      <td>1</td>\n",
       "      <td>Great delivery vehicle: It's been a great deli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tires</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.559100</td>\n",
       "      <td>0.304524</td>\n",
       "      <td>0.042803</td>\n",
       "      <td>0.056519</td>\n",
       "      <td>0.001143</td>\n",
       "      <td>0.145506</td>\n",
       "      <td>2</td>\n",
       "      <td>Great delivery vehicle: It's been a great deli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>care</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.982545</td>\n",
       "      <td>0.514348</td>\n",
       "      <td>0.039427</td>\n",
       "      <td>0.690797</td>\n",
       "      <td>0.045917</td>\n",
       "      <td>0.001771</td>\n",
       "      <td>0.011468</td>\n",
       "      <td>1</td>\n",
       "      <td>Great delivery vehicle: It's been a great deli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Michelin LX series</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.602034</td>\n",
       "      <td>0.411711</td>\n",
       "      <td>0.290023</td>\n",
       "      <td>0.131281</td>\n",
       "      <td>0.298556</td>\n",
       "      <td>0.001173</td>\n",
       "      <td>0.086125</td>\n",
       "      <td>1</td>\n",
       "      <td>Great delivery vehicle: It's been a great deli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cafe business</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.982545</td>\n",
       "      <td>0.396141</td>\n",
       "      <td>0.039427</td>\n",
       "      <td>0.690797</td>\n",
       "      <td>0.045917</td>\n",
       "      <td>0.001771</td>\n",
       "      <td>0.011468</td>\n",
       "      <td>1</td>\n",
       "      <td>Great delivery vehicle: It's been a great deli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>good power</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.982545</td>\n",
       "      <td>0.357562</td>\n",
       "      <td>0.039427</td>\n",
       "      <td>0.690797</td>\n",
       "      <td>0.045917</td>\n",
       "      <td>0.001771</td>\n",
       "      <td>0.011468</td>\n",
       "      <td>1</td>\n",
       "      <td>Great delivery vehicle: It's been a great deli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>economy match</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.982545</td>\n",
       "      <td>0.317306</td>\n",
       "      <td>0.039427</td>\n",
       "      <td>0.690797</td>\n",
       "      <td>0.045917</td>\n",
       "      <td>0.001771</td>\n",
       "      <td>0.011468</td>\n",
       "      <td>1</td>\n",
       "      <td>Great delivery vehicle: It's been a great deli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>normal maintenance items</td>\n",
       "      <td>negative</td>\n",
       "      <td>-0.561817</td>\n",
       "      <td>0.307532</td>\n",
       "      <td>0.261543</td>\n",
       "      <td>0.025168</td>\n",
       "      <td>0.009511</td>\n",
       "      <td>0.013281</td>\n",
       "      <td>0.192068</td>\n",
       "      <td>1</td>\n",
       "      <td>Great delivery vehicle: It's been a great deli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Havent</td>\n",
       "      <td>negative</td>\n",
       "      <td>-0.561817</td>\n",
       "      <td>0.256558</td>\n",
       "      <td>0.261543</td>\n",
       "      <td>0.025168</td>\n",
       "      <td>0.009511</td>\n",
       "      <td>0.013281</td>\n",
       "      <td>0.192068</td>\n",
       "      <td>1</td>\n",
       "      <td>Great delivery vehicle: It's been a great deli...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       text sentiment.label  sentiment.score  relevance  \\\n",
       "0    Great delivery vehicle        positive         0.982545   0.999486   \n",
       "1              fuel economy        positive         0.602034   0.605066   \n",
       "2                     tires         neutral         0.000000   0.559100   \n",
       "3                      care        positive         0.982545   0.514348   \n",
       "4        Michelin LX series        positive         0.602034   0.411711   \n",
       "5             cafe business        positive         0.982545   0.396141   \n",
       "6                good power        positive         0.982545   0.357562   \n",
       "7             economy match        positive         0.982545   0.317306   \n",
       "8  normal maintenance items        negative        -0.561817   0.307532   \n",
       "9                    Havent        negative        -0.561817   0.256558   \n",
       "\n",
       "   emotion.sadness  emotion.joy  emotion.fear  emotion.disgust  emotion.anger  \\\n",
       "0         0.039427     0.690797      0.045917         0.001771       0.011468   \n",
       "1         0.290023     0.131281      0.298556         0.001173       0.086125   \n",
       "2         0.304524     0.042803      0.056519         0.001143       0.145506   \n",
       "3         0.039427     0.690797      0.045917         0.001771       0.011468   \n",
       "4         0.290023     0.131281      0.298556         0.001173       0.086125   \n",
       "5         0.039427     0.690797      0.045917         0.001771       0.011468   \n",
       "6         0.039427     0.690797      0.045917         0.001771       0.011468   \n",
       "7         0.039427     0.690797      0.045917         0.001771       0.011468   \n",
       "8         0.261543     0.025168      0.009511         0.013281       0.192068   \n",
       "9         0.261543     0.025168      0.009511         0.013281       0.192068   \n",
       "\n",
       "   count                                                  0  \n",
       "0      2  Great delivery vehicle: It's been a great deli...  \n",
       "1      1  Great delivery vehicle: It's been a great deli...  \n",
       "2      2  Great delivery vehicle: It's been a great deli...  \n",
       "3      1  Great delivery vehicle: It's been a great deli...  \n",
       "4      1  Great delivery vehicle: It's been a great deli...  \n",
       "5      1  Great delivery vehicle: It's been a great deli...  \n",
       "6      1  Great delivery vehicle: It's been a great deli...  \n",
       "7      1  Great delivery vehicle: It's been a great deli...  \n",
       "8      1  Great delivery vehicle: It's been a great deli...  \n",
       "9      1  Great delivery vehicle: It's been a great deli...  "
      ]
     },
     "execution_count": 658,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert list of dataframes to the dataframe\n",
    "keywords_review_df = pd.concat(keywords_review, axis = 0)\n",
    "keywords_review_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment.label</th>\n",
       "      <th>sentiment.score</th>\n",
       "      <th>relevance</th>\n",
       "      <th>emotion.sadness</th>\n",
       "      <th>emotion.joy</th>\n",
       "      <th>emotion.fear</th>\n",
       "      <th>emotion.disgust</th>\n",
       "      <th>emotion.anger</th>\n",
       "      <th>count</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Review_Date</th>\n",
       "      <th>Author_Name</th>\n",
       "      <th>Vehicle_Title</th>\n",
       "      <th>Review_Title</th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review_Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Great delivery vehicle</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.982545</td>\n",
       "      <td>0.999486</td>\n",
       "      <td>0.039427</td>\n",
       "      <td>0.690797</td>\n",
       "      <td>0.045917</td>\n",
       "      <td>0.001771</td>\n",
       "      <td>0.011468</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>on 10/13/05 15:30 PM (PDT)</td>\n",
       "      <td>roadking</td>\n",
       "      <td>2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...</td>\n",
       "      <td>Great delivery vehicle</td>\n",
       "      <td>It's been a great delivery vehicle for my caf...</td>\n",
       "      <td>4.625</td>\n",
       "      <td>Great delivery vehicle: It's been a great deli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fuel economy</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.602034</td>\n",
       "      <td>0.605066</td>\n",
       "      <td>0.290023</td>\n",
       "      <td>0.131281</td>\n",
       "      <td>0.298556</td>\n",
       "      <td>0.001173</td>\n",
       "      <td>0.086125</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>on 10/13/05 15:30 PM (PDT)</td>\n",
       "      <td>roadking</td>\n",
       "      <td>2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...</td>\n",
       "      <td>Great delivery vehicle</td>\n",
       "      <td>It's been a great delivery vehicle for my caf...</td>\n",
       "      <td>4.625</td>\n",
       "      <td>Great delivery vehicle: It's been a great deli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tires</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.559100</td>\n",
       "      <td>0.304524</td>\n",
       "      <td>0.042803</td>\n",
       "      <td>0.056519</td>\n",
       "      <td>0.001143</td>\n",
       "      <td>0.145506</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>on 10/13/05 15:30 PM (PDT)</td>\n",
       "      <td>roadking</td>\n",
       "      <td>2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...</td>\n",
       "      <td>Great delivery vehicle</td>\n",
       "      <td>It's been a great delivery vehicle for my caf...</td>\n",
       "      <td>4.625</td>\n",
       "      <td>Great delivery vehicle: It's been a great deli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>care</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.982545</td>\n",
       "      <td>0.514348</td>\n",
       "      <td>0.039427</td>\n",
       "      <td>0.690797</td>\n",
       "      <td>0.045917</td>\n",
       "      <td>0.001771</td>\n",
       "      <td>0.011468</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>on 10/13/05 15:30 PM (PDT)</td>\n",
       "      <td>roadking</td>\n",
       "      <td>2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...</td>\n",
       "      <td>Great delivery vehicle</td>\n",
       "      <td>It's been a great delivery vehicle for my caf...</td>\n",
       "      <td>4.625</td>\n",
       "      <td>Great delivery vehicle: It's been a great deli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Michelin LX series</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.602034</td>\n",
       "      <td>0.411711</td>\n",
       "      <td>0.290023</td>\n",
       "      <td>0.131281</td>\n",
       "      <td>0.298556</td>\n",
       "      <td>0.001173</td>\n",
       "      <td>0.086125</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>on 10/13/05 15:30 PM (PDT)</td>\n",
       "      <td>roadking</td>\n",
       "      <td>2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...</td>\n",
       "      <td>Great delivery vehicle</td>\n",
       "      <td>It's been a great delivery vehicle for my caf...</td>\n",
       "      <td>4.625</td>\n",
       "      <td>Great delivery vehicle: It's been a great deli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11049</th>\n",
       "      <td>crappy antennas</td>\n",
       "      <td>negative</td>\n",
       "      <td>-0.641939</td>\n",
       "      <td>0.582832</td>\n",
       "      <td>0.566678</td>\n",
       "      <td>0.192352</td>\n",
       "      <td>0.206886</td>\n",
       "      <td>0.026161</td>\n",
       "      <td>0.083574</td>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>on 08/06/17 12:10 PM (PDT)</td>\n",
       "      <td>Ryan</td>\n",
       "      <td>2016 Dodge Dart Sedan SE 4dr Sedan (2.0L 4cyl 6M)</td>\n",
       "      <td>Radio reception is not good at all.</td>\n",
       "      <td>A lot of new cars like this Dodge Dart are ma...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>Radio reception is not good at all.: A lot of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11050</th>\n",
       "      <td>Chicago</td>\n",
       "      <td>negative</td>\n",
       "      <td>-0.365738</td>\n",
       "      <td>0.533335</td>\n",
       "      <td>0.235903</td>\n",
       "      <td>0.034862</td>\n",
       "      <td>0.180498</td>\n",
       "      <td>0.051072</td>\n",
       "      <td>0.097922</td>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>on 08/06/17 12:10 PM (PDT)</td>\n",
       "      <td>Ryan</td>\n",
       "      <td>2016 Dodge Dart Sedan SE 4dr Sedan (2.0L 4cyl 6M)</td>\n",
       "      <td>Radio reception is not good at all.</td>\n",
       "      <td>A lot of new cars like this Dodge Dart are ma...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>Radio reception is not good at all.: A lot of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11051</th>\n",
       "      <td>city</td>\n",
       "      <td>negative</td>\n",
       "      <td>-0.365738</td>\n",
       "      <td>0.522397</td>\n",
       "      <td>0.235903</td>\n",
       "      <td>0.034862</td>\n",
       "      <td>0.180498</td>\n",
       "      <td>0.051072</td>\n",
       "      <td>0.097922</td>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>on 08/06/17 12:10 PM (PDT)</td>\n",
       "      <td>Ryan</td>\n",
       "      <td>2016 Dodge Dart Sedan SE 4dr Sedan (2.0L 4cyl 6M)</td>\n",
       "      <td>Radio reception is not good at all.</td>\n",
       "      <td>A lot of new cars like this Dodge Dart are ma...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>Radio reception is not good at all.: A lot of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11052</th>\n",
       "      <td>radio stations signals</td>\n",
       "      <td>negative</td>\n",
       "      <td>-0.641939</td>\n",
       "      <td>0.514317</td>\n",
       "      <td>0.566678</td>\n",
       "      <td>0.192352</td>\n",
       "      <td>0.206886</td>\n",
       "      <td>0.026161</td>\n",
       "      <td>0.083574</td>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>on 08/06/17 12:10 PM (PDT)</td>\n",
       "      <td>Ryan</td>\n",
       "      <td>2016 Dodge Dart Sedan SE 4dr Sedan (2.0L 4cyl 6M)</td>\n",
       "      <td>Radio reception is not good at all.</td>\n",
       "      <td>A lot of new cars like this Dodge Dart are ma...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>Radio reception is not good at all.: A lot of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11053</th>\n",
       "      <td>coverage area of many stations</td>\n",
       "      <td>negative</td>\n",
       "      <td>-0.365738</td>\n",
       "      <td>0.249834</td>\n",
       "      <td>0.235903</td>\n",
       "      <td>0.034862</td>\n",
       "      <td>0.180498</td>\n",
       "      <td>0.051072</td>\n",
       "      <td>0.097922</td>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>on 08/06/17 12:10 PM (PDT)</td>\n",
       "      <td>Ryan</td>\n",
       "      <td>2016 Dodge Dart Sedan SE 4dr Sedan (2.0L 4cyl 6M)</td>\n",
       "      <td>Radio reception is not good at all.</td>\n",
       "      <td>A lot of new cars like this Dodge Dart are ma...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>Radio reception is not good at all.: A lot of ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11054 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 text sentiment.label  sentiment.score  \\\n",
       "0              Great delivery vehicle        positive         0.982545   \n",
       "1                        fuel economy        positive         0.602034   \n",
       "2                               tires         neutral         0.000000   \n",
       "3                                care        positive         0.982545   \n",
       "4                  Michelin LX series        positive         0.602034   \n",
       "...                               ...             ...              ...   \n",
       "11049                 crappy antennas        negative        -0.641939   \n",
       "11050                         Chicago        negative        -0.365738   \n",
       "11051                            city        negative        -0.365738   \n",
       "11052          radio stations signals        negative        -0.641939   \n",
       "11053  coverage area of many stations        negative        -0.365738   \n",
       "\n",
       "       relevance  emotion.sadness  emotion.joy  emotion.fear  emotion.disgust  \\\n",
       "0       0.999486         0.039427     0.690797      0.045917         0.001771   \n",
       "1       0.605066         0.290023     0.131281      0.298556         0.001173   \n",
       "2       0.559100         0.304524     0.042803      0.056519         0.001143   \n",
       "3       0.514348         0.039427     0.690797      0.045917         0.001771   \n",
       "4       0.411711         0.290023     0.131281      0.298556         0.001173   \n",
       "...          ...              ...          ...           ...              ...   \n",
       "11049   0.582832         0.566678     0.192352      0.206886         0.026161   \n",
       "11050   0.533335         0.235903     0.034862      0.180498         0.051072   \n",
       "11051   0.522397         0.235903     0.034862      0.180498         0.051072   \n",
       "11052   0.514317         0.566678     0.192352      0.206886         0.026161   \n",
       "11053   0.249834         0.235903     0.034862      0.180498         0.051072   \n",
       "\n",
       "       emotion.anger  count Unnamed: 0                  Review_Date  \\\n",
       "0           0.011468      2          0   on 10/13/05 15:30 PM (PDT)   \n",
       "1           0.086125      1          0   on 10/13/05 15:30 PM (PDT)   \n",
       "2           0.145506      2          0   on 10/13/05 15:30 PM (PDT)   \n",
       "3           0.011468      1          0   on 10/13/05 15:30 PM (PDT)   \n",
       "4           0.086125      1          0   on 10/13/05 15:30 PM (PDT)   \n",
       "...              ...    ...        ...                          ...   \n",
       "11049       0.083574      1        661   on 08/06/17 12:10 PM (PDT)   \n",
       "11050       0.097922      1        661   on 08/06/17 12:10 PM (PDT)   \n",
       "11051       0.097922      1        661   on 08/06/17 12:10 PM (PDT)   \n",
       "11052       0.083574      1        661   on 08/06/17 12:10 PM (PDT)   \n",
       "11053       0.097922      1        661   on 08/06/17 12:10 PM (PDT)   \n",
       "\n",
       "      Author_Name                                      Vehicle_Title  \\\n",
       "0       roadking   2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...   \n",
       "1       roadking   2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...   \n",
       "2       roadking   2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...   \n",
       "3       roadking   2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...   \n",
       "4       roadking   2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...   \n",
       "...           ...                                                ...   \n",
       "11049       Ryan   2016 Dodge Dart Sedan SE 4dr Sedan (2.0L 4cyl 6M)   \n",
       "11050       Ryan   2016 Dodge Dart Sedan SE 4dr Sedan (2.0L 4cyl 6M)   \n",
       "11051       Ryan   2016 Dodge Dart Sedan SE 4dr Sedan (2.0L 4cyl 6M)   \n",
       "11052       Ryan   2016 Dodge Dart Sedan SE 4dr Sedan (2.0L 4cyl 6M)   \n",
       "11053       Ryan   2016 Dodge Dart Sedan SE 4dr Sedan (2.0L 4cyl 6M)   \n",
       "\n",
       "                              Review_Title  \\\n",
       "0                   Great delivery vehicle   \n",
       "1                   Great delivery vehicle   \n",
       "2                   Great delivery vehicle   \n",
       "3                   Great delivery vehicle   \n",
       "4                   Great delivery vehicle   \n",
       "...                                    ...   \n",
       "11049  Radio reception is not good at all.   \n",
       "11050  Radio reception is not good at all.   \n",
       "11051  Radio reception is not good at all.   \n",
       "11052  Radio reception is not good at all.   \n",
       "11053  Radio reception is not good at all.   \n",
       "\n",
       "                                                  Review  Rating  \\\n",
       "0       It's been a great delivery vehicle for my caf...   4.625   \n",
       "1       It's been a great delivery vehicle for my caf...   4.625   \n",
       "2       It's been a great delivery vehicle for my caf...   4.625   \n",
       "3       It's been a great delivery vehicle for my caf...   4.625   \n",
       "4       It's been a great delivery vehicle for my caf...   4.625   \n",
       "...                                                  ...     ...   \n",
       "11049   A lot of new cars like this Dodge Dart are ma...   1.000   \n",
       "11050   A lot of new cars like this Dodge Dart are ma...   1.000   \n",
       "11051   A lot of new cars like this Dodge Dart are ma...   1.000   \n",
       "11052   A lot of new cars like this Dodge Dart are ma...   1.000   \n",
       "11053   A lot of new cars like this Dodge Dart are ma...   1.000   \n",
       "\n",
       "                                          Review_Content  \n",
       "0      Great delivery vehicle: It's been a great deli...  \n",
       "1      Great delivery vehicle: It's been a great deli...  \n",
       "2      Great delivery vehicle: It's been a great deli...  \n",
       "3      Great delivery vehicle: It's been a great deli...  \n",
       "4      Great delivery vehicle: It's been a great deli...  \n",
       "...                                                  ...  \n",
       "11049  Radio reception is not good at all.: A lot of ...  \n",
       "11050  Radio reception is not good at all.: A lot of ...  \n",
       "11051  Radio reception is not good at all.: A lot of ...  \n",
       "11052  Radio reception is not good at all.: A lot of ...  \n",
       "11053  Radio reception is not good at all.: A lot of ...  \n",
       "\n",
       "[11054 rows x 18 columns]"
      ]
     },
     "execution_count": 651,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge with info on each review:\n",
    "merged_keywords_review_df = (keywords_review_df.merge(dodge_rev,left_on=0, right_on = dodge_rev.Review_Content)).drop(columns=[0])\n",
    "merged_keywords_review_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment.label</th>\n",
       "      <th>sentiment.score</th>\n",
       "      <th>relevance</th>\n",
       "      <th>emotion.sadness</th>\n",
       "      <th>emotion.joy</th>\n",
       "      <th>emotion.fear</th>\n",
       "      <th>emotion.disgust</th>\n",
       "      <th>emotion.anger</th>\n",
       "      <th>count</th>\n",
       "      <th>review.number</th>\n",
       "      <th>Review_Date</th>\n",
       "      <th>Author_Name</th>\n",
       "      <th>Vehicle_Title</th>\n",
       "      <th>Review_Title</th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review_Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Great delivery vehicle</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.982545</td>\n",
       "      <td>0.999486</td>\n",
       "      <td>0.039427</td>\n",
       "      <td>0.690797</td>\n",
       "      <td>0.045917</td>\n",
       "      <td>0.001771</td>\n",
       "      <td>0.011468</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>on 10/13/05 15:30 PM (PDT)</td>\n",
       "      <td>roadking</td>\n",
       "      <td>2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...</td>\n",
       "      <td>Great delivery vehicle</td>\n",
       "      <td>It's been a great delivery vehicle for my caf...</td>\n",
       "      <td>4.625</td>\n",
       "      <td>Great delivery vehicle: It's been a great deli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fuel economy</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.602034</td>\n",
       "      <td>0.605066</td>\n",
       "      <td>0.290023</td>\n",
       "      <td>0.131281</td>\n",
       "      <td>0.298556</td>\n",
       "      <td>0.001173</td>\n",
       "      <td>0.086125</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>on 10/13/05 15:30 PM (PDT)</td>\n",
       "      <td>roadking</td>\n",
       "      <td>2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...</td>\n",
       "      <td>Great delivery vehicle</td>\n",
       "      <td>It's been a great delivery vehicle for my caf...</td>\n",
       "      <td>4.625</td>\n",
       "      <td>Great delivery vehicle: It's been a great deli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tires</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.559100</td>\n",
       "      <td>0.304524</td>\n",
       "      <td>0.042803</td>\n",
       "      <td>0.056519</td>\n",
       "      <td>0.001143</td>\n",
       "      <td>0.145506</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>on 10/13/05 15:30 PM (PDT)</td>\n",
       "      <td>roadking</td>\n",
       "      <td>2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...</td>\n",
       "      <td>Great delivery vehicle</td>\n",
       "      <td>It's been a great delivery vehicle for my caf...</td>\n",
       "      <td>4.625</td>\n",
       "      <td>Great delivery vehicle: It's been a great deli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>care</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.982545</td>\n",
       "      <td>0.514348</td>\n",
       "      <td>0.039427</td>\n",
       "      <td>0.690797</td>\n",
       "      <td>0.045917</td>\n",
       "      <td>0.001771</td>\n",
       "      <td>0.011468</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>on 10/13/05 15:30 PM (PDT)</td>\n",
       "      <td>roadking</td>\n",
       "      <td>2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...</td>\n",
       "      <td>Great delivery vehicle</td>\n",
       "      <td>It's been a great delivery vehicle for my caf...</td>\n",
       "      <td>4.625</td>\n",
       "      <td>Great delivery vehicle: It's been a great deli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Michelin LX series</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.602034</td>\n",
       "      <td>0.411711</td>\n",
       "      <td>0.290023</td>\n",
       "      <td>0.131281</td>\n",
       "      <td>0.298556</td>\n",
       "      <td>0.001173</td>\n",
       "      <td>0.086125</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>on 10/13/05 15:30 PM (PDT)</td>\n",
       "      <td>roadking</td>\n",
       "      <td>2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...</td>\n",
       "      <td>Great delivery vehicle</td>\n",
       "      <td>It's been a great delivery vehicle for my caf...</td>\n",
       "      <td>4.625</td>\n",
       "      <td>Great delivery vehicle: It's been a great deli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cafe business</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.982545</td>\n",
       "      <td>0.396141</td>\n",
       "      <td>0.039427</td>\n",
       "      <td>0.690797</td>\n",
       "      <td>0.045917</td>\n",
       "      <td>0.001771</td>\n",
       "      <td>0.011468</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>on 10/13/05 15:30 PM (PDT)</td>\n",
       "      <td>roadking</td>\n",
       "      <td>2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...</td>\n",
       "      <td>Great delivery vehicle</td>\n",
       "      <td>It's been a great delivery vehicle for my caf...</td>\n",
       "      <td>4.625</td>\n",
       "      <td>Great delivery vehicle: It's been a great deli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>good power</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.982545</td>\n",
       "      <td>0.357562</td>\n",
       "      <td>0.039427</td>\n",
       "      <td>0.690797</td>\n",
       "      <td>0.045917</td>\n",
       "      <td>0.001771</td>\n",
       "      <td>0.011468</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>on 10/13/05 15:30 PM (PDT)</td>\n",
       "      <td>roadking</td>\n",
       "      <td>2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...</td>\n",
       "      <td>Great delivery vehicle</td>\n",
       "      <td>It's been a great delivery vehicle for my caf...</td>\n",
       "      <td>4.625</td>\n",
       "      <td>Great delivery vehicle: It's been a great deli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>economy match</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.982545</td>\n",
       "      <td>0.317306</td>\n",
       "      <td>0.039427</td>\n",
       "      <td>0.690797</td>\n",
       "      <td>0.045917</td>\n",
       "      <td>0.001771</td>\n",
       "      <td>0.011468</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>on 10/13/05 15:30 PM (PDT)</td>\n",
       "      <td>roadking</td>\n",
       "      <td>2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...</td>\n",
       "      <td>Great delivery vehicle</td>\n",
       "      <td>It's been a great delivery vehicle for my caf...</td>\n",
       "      <td>4.625</td>\n",
       "      <td>Great delivery vehicle: It's been a great deli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>normal maintenance items</td>\n",
       "      <td>negative</td>\n",
       "      <td>-0.561817</td>\n",
       "      <td>0.307532</td>\n",
       "      <td>0.261543</td>\n",
       "      <td>0.025168</td>\n",
       "      <td>0.009511</td>\n",
       "      <td>0.013281</td>\n",
       "      <td>0.192068</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>on 10/13/05 15:30 PM (PDT)</td>\n",
       "      <td>roadking</td>\n",
       "      <td>2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...</td>\n",
       "      <td>Great delivery vehicle</td>\n",
       "      <td>It's been a great delivery vehicle for my caf...</td>\n",
       "      <td>4.625</td>\n",
       "      <td>Great delivery vehicle: It's been a great deli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Havent</td>\n",
       "      <td>negative</td>\n",
       "      <td>-0.561817</td>\n",
       "      <td>0.256558</td>\n",
       "      <td>0.261543</td>\n",
       "      <td>0.025168</td>\n",
       "      <td>0.009511</td>\n",
       "      <td>0.013281</td>\n",
       "      <td>0.192068</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>on 10/13/05 15:30 PM (PDT)</td>\n",
       "      <td>roadking</td>\n",
       "      <td>2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...</td>\n",
       "      <td>Great delivery vehicle</td>\n",
       "      <td>It's been a great delivery vehicle for my caf...</td>\n",
       "      <td>4.625</td>\n",
       "      <td>Great delivery vehicle: It's been a great deli...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       text sentiment.label  sentiment.score  relevance  \\\n",
       "0    Great delivery vehicle        positive         0.982545   0.999486   \n",
       "1              fuel economy        positive         0.602034   0.605066   \n",
       "2                     tires         neutral         0.000000   0.559100   \n",
       "3                      care        positive         0.982545   0.514348   \n",
       "4        Michelin LX series        positive         0.602034   0.411711   \n",
       "5             cafe business        positive         0.982545   0.396141   \n",
       "6                good power        positive         0.982545   0.357562   \n",
       "7             economy match        positive         0.982545   0.317306   \n",
       "8  normal maintenance items        negative        -0.561817   0.307532   \n",
       "9                    Havent        negative        -0.561817   0.256558   \n",
       "\n",
       "   emotion.sadness  emotion.joy  emotion.fear  emotion.disgust  emotion.anger  \\\n",
       "0         0.039427     0.690797      0.045917         0.001771       0.011468   \n",
       "1         0.290023     0.131281      0.298556         0.001173       0.086125   \n",
       "2         0.304524     0.042803      0.056519         0.001143       0.145506   \n",
       "3         0.039427     0.690797      0.045917         0.001771       0.011468   \n",
       "4         0.290023     0.131281      0.298556         0.001173       0.086125   \n",
       "5         0.039427     0.690797      0.045917         0.001771       0.011468   \n",
       "6         0.039427     0.690797      0.045917         0.001771       0.011468   \n",
       "7         0.039427     0.690797      0.045917         0.001771       0.011468   \n",
       "8         0.261543     0.025168      0.009511         0.013281       0.192068   \n",
       "9         0.261543     0.025168      0.009511         0.013281       0.192068   \n",
       "\n",
       "   count review.number                  Review_Date Author_Name  \\\n",
       "0      2             0   on 10/13/05 15:30 PM (PDT)   roadking    \n",
       "1      1             0   on 10/13/05 15:30 PM (PDT)   roadking    \n",
       "2      2             0   on 10/13/05 15:30 PM (PDT)   roadking    \n",
       "3      1             0   on 10/13/05 15:30 PM (PDT)   roadking    \n",
       "4      1             0   on 10/13/05 15:30 PM (PDT)   roadking    \n",
       "5      1             0   on 10/13/05 15:30 PM (PDT)   roadking    \n",
       "6      1             0   on 10/13/05 15:30 PM (PDT)   roadking    \n",
       "7      1             0   on 10/13/05 15:30 PM (PDT)   roadking    \n",
       "8      1             0   on 10/13/05 15:30 PM (PDT)   roadking    \n",
       "9      1             0   on 10/13/05 15:30 PM (PDT)   roadking    \n",
       "\n",
       "                                       Vehicle_Title            Review_Title  \\\n",
       "0  2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...  Great delivery vehicle   \n",
       "1  2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...  Great delivery vehicle   \n",
       "2  2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...  Great delivery vehicle   \n",
       "3  2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...  Great delivery vehicle   \n",
       "4  2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...  Great delivery vehicle   \n",
       "5  2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...  Great delivery vehicle   \n",
       "6  2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...  Great delivery vehicle   \n",
       "7  2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...  Great delivery vehicle   \n",
       "8  2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...  Great delivery vehicle   \n",
       "9  2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...  Great delivery vehicle   \n",
       "\n",
       "                                              Review  Rating  \\\n",
       "0   It's been a great delivery vehicle for my caf...   4.625   \n",
       "1   It's been a great delivery vehicle for my caf...   4.625   \n",
       "2   It's been a great delivery vehicle for my caf...   4.625   \n",
       "3   It's been a great delivery vehicle for my caf...   4.625   \n",
       "4   It's been a great delivery vehicle for my caf...   4.625   \n",
       "5   It's been a great delivery vehicle for my caf...   4.625   \n",
       "6   It's been a great delivery vehicle for my caf...   4.625   \n",
       "7   It's been a great delivery vehicle for my caf...   4.625   \n",
       "8   It's been a great delivery vehicle for my caf...   4.625   \n",
       "9   It's been a great delivery vehicle for my caf...   4.625   \n",
       "\n",
       "                                      Review_Content  \n",
       "0  Great delivery vehicle: It's been a great deli...  \n",
       "1  Great delivery vehicle: It's been a great deli...  \n",
       "2  Great delivery vehicle: It's been a great deli...  \n",
       "3  Great delivery vehicle: It's been a great deli...  \n",
       "4  Great delivery vehicle: It's been a great deli...  \n",
       "5  Great delivery vehicle: It's been a great deli...  \n",
       "6  Great delivery vehicle: It's been a great deli...  \n",
       "7  Great delivery vehicle: It's been a great deli...  \n",
       "8  Great delivery vehicle: It's been a great deli...  \n",
       "9  Great delivery vehicle: It's been a great deli...  "
      ]
     },
     "execution_count": 659,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Give a proper column name to the Unnamed: 0'\n",
    "merged_keywords_review_df.rename( columns={'Unnamed: 0':'review.number'}, inplace=True )\n",
    "merged_keywords_review_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment.label</th>\n",
       "      <th>sentiment.score</th>\n",
       "      <th>relevance</th>\n",
       "      <th>emotion.sadness</th>\n",
       "      <th>emotion.joy</th>\n",
       "      <th>emotion.fear</th>\n",
       "      <th>emotion.disgust</th>\n",
       "      <th>emotion.anger</th>\n",
       "      <th>count</th>\n",
       "      <th>review.number</th>\n",
       "      <th>Review_Date</th>\n",
       "      <th>Author_Name</th>\n",
       "      <th>Vehicle_Title</th>\n",
       "      <th>Review_Title</th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review_Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6353</th>\n",
       "      <td>red color</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.945484</td>\n",
       "      <td>0.686564</td>\n",
       "      <td>0.262876</td>\n",
       "      <td>0.619660</td>\n",
       "      <td>0.072974</td>\n",
       "      <td>0.026809</td>\n",
       "      <td>0.040345</td>\n",
       "      <td>1</td>\n",
       "      <td>372</td>\n",
       "      <td>on 11/20/09 14:14 PM (PST)</td>\n",
       "      <td>Steve</td>\n",
       "      <td>1997 Dodge Avenger Coupe 2dr Coupe</td>\n",
       "      <td>still enjoyable</td>\n",
       "      <td>I bought my Avenger new in 1997 with the 5 sp...</td>\n",
       "      <td>4.25</td>\n",
       "      <td>still enjoyable: I bought my Avenger new in 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6354</th>\n",
       "      <td>best year</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.945484</td>\n",
       "      <td>0.684752</td>\n",
       "      <td>0.262876</td>\n",
       "      <td>0.619660</td>\n",
       "      <td>0.072974</td>\n",
       "      <td>0.026809</td>\n",
       "      <td>0.040345</td>\n",
       "      <td>1</td>\n",
       "      <td>372</td>\n",
       "      <td>on 11/20/09 14:14 PM (PST)</td>\n",
       "      <td>Steve</td>\n",
       "      <td>1997 Dodge Avenger Coupe 2dr Coupe</td>\n",
       "      <td>still enjoyable</td>\n",
       "      <td>I bought my Avenger new in 1997 with the 5 sp...</td>\n",
       "      <td>4.25</td>\n",
       "      <td>still enjoyable: I bought my Avenger new in 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6355</th>\n",
       "      <td>Road America</td>\n",
       "      <td>negative</td>\n",
       "      <td>-0.322828</td>\n",
       "      <td>0.679955</td>\n",
       "      <td>0.272930</td>\n",
       "      <td>0.213291</td>\n",
       "      <td>0.211228</td>\n",
       "      <td>0.140234</td>\n",
       "      <td>0.351365</td>\n",
       "      <td>1</td>\n",
       "      <td>372</td>\n",
       "      <td>on 11/20/09 14:14 PM (PST)</td>\n",
       "      <td>Steve</td>\n",
       "      <td>1997 Dodge Avenger Coupe 2dr Coupe</td>\n",
       "      <td>still enjoyable</td>\n",
       "      <td>I bought my Avenger new in 1997 with the 5 sp...</td>\n",
       "      <td>4.25</td>\n",
       "      <td>still enjoyable: I bought my Avenger new in 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6356</th>\n",
       "      <td>better brakes</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.926852</td>\n",
       "      <td>0.671791</td>\n",
       "      <td>0.015269</td>\n",
       "      <td>0.627940</td>\n",
       "      <td>0.028063</td>\n",
       "      <td>0.005312</td>\n",
       "      <td>0.013645</td>\n",
       "      <td>1</td>\n",
       "      <td>372</td>\n",
       "      <td>on 11/20/09 14:14 PM (PST)</td>\n",
       "      <td>Steve</td>\n",
       "      <td>1997 Dodge Avenger Coupe 2dr Coupe</td>\n",
       "      <td>still enjoyable</td>\n",
       "      <td>I bought my Avenger new in 1997 with the 5 sp...</td>\n",
       "      <td>4.25</td>\n",
       "      <td>still enjoyable: I bought my Avenger new in 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6357</th>\n",
       "      <td>fron end fascia</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.945484</td>\n",
       "      <td>0.640654</td>\n",
       "      <td>0.262876</td>\n",
       "      <td>0.619660</td>\n",
       "      <td>0.072974</td>\n",
       "      <td>0.026809</td>\n",
       "      <td>0.040345</td>\n",
       "      <td>1</td>\n",
       "      <td>372</td>\n",
       "      <td>on 11/20/09 14:14 PM (PST)</td>\n",
       "      <td>Steve</td>\n",
       "      <td>1997 Dodge Avenger Coupe 2dr Coupe</td>\n",
       "      <td>still enjoyable</td>\n",
       "      <td>I bought my Avenger new in 1997 with the 5 sp...</td>\n",
       "      <td>4.25</td>\n",
       "      <td>still enjoyable: I bought my Avenger new in 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6358</th>\n",
       "      <td>only real issue</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.524426</td>\n",
       "      <td>0.614013</td>\n",
       "      <td>0.309655</td>\n",
       "      <td>0.053606</td>\n",
       "      <td>0.062797</td>\n",
       "      <td>0.037489</td>\n",
       "      <td>0.379048</td>\n",
       "      <td>1</td>\n",
       "      <td>372</td>\n",
       "      <td>on 11/20/09 14:14 PM (PST)</td>\n",
       "      <td>Steve</td>\n",
       "      <td>1997 Dodge Avenger Coupe 2dr Coupe</td>\n",
       "      <td>still enjoyable</td>\n",
       "      <td>I bought my Avenger new in 1997 with the 5 sp...</td>\n",
       "      <td>4.25</td>\n",
       "      <td>still enjoyable: I bought my Avenger new in 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6359</th>\n",
       "      <td>original stainless steel exhaust</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.243223</td>\n",
       "      <td>0.597900</td>\n",
       "      <td>0.657172</td>\n",
       "      <td>0.088561</td>\n",
       "      <td>0.207029</td>\n",
       "      <td>0.087853</td>\n",
       "      <td>0.085118</td>\n",
       "      <td>1</td>\n",
       "      <td>372</td>\n",
       "      <td>on 11/20/09 14:14 PM (PST)</td>\n",
       "      <td>Steve</td>\n",
       "      <td>1997 Dodge Avenger Coupe 2dr Coupe</td>\n",
       "      <td>still enjoyable</td>\n",
       "      <td>I bought my Avenger new in 1997 with the 5 sp...</td>\n",
       "      <td>4.25</td>\n",
       "      <td>still enjoyable: I bought my Avenger new in 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6360</th>\n",
       "      <td>speed</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.939953</td>\n",
       "      <td>0.546267</td>\n",
       "      <td>0.035011</td>\n",
       "      <td>0.663947</td>\n",
       "      <td>0.027223</td>\n",
       "      <td>0.007594</td>\n",
       "      <td>0.016850</td>\n",
       "      <td>2</td>\n",
       "      <td>372</td>\n",
       "      <td>on 11/20/09 14:14 PM (PST)</td>\n",
       "      <td>Steve</td>\n",
       "      <td>1997 Dodge Avenger Coupe 2dr Coupe</td>\n",
       "      <td>still enjoyable</td>\n",
       "      <td>I bought my Avenger new in 1997 with the 5 sp...</td>\n",
       "      <td>4.25</td>\n",
       "      <td>still enjoyable: I bought my Avenger new in 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6361</th>\n",
       "      <td>kids</td>\n",
       "      <td>negative</td>\n",
       "      <td>-0.322828</td>\n",
       "      <td>0.545298</td>\n",
       "      <td>0.272930</td>\n",
       "      <td>0.213291</td>\n",
       "      <td>0.211228</td>\n",
       "      <td>0.140234</td>\n",
       "      <td>0.351365</td>\n",
       "      <td>1</td>\n",
       "      <td>372</td>\n",
       "      <td>on 11/20/09 14:14 PM (PST)</td>\n",
       "      <td>Steve</td>\n",
       "      <td>1997 Dodge Avenger Coupe 2dr Coupe</td>\n",
       "      <td>still enjoyable</td>\n",
       "      <td>I bought my Avenger new in 1997 with the 5 sp...</td>\n",
       "      <td>4.25</td>\n",
       "      <td>still enjoyable: I bought my Avenger new in 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6362</th>\n",
       "      <td>track</td>\n",
       "      <td>negative</td>\n",
       "      <td>-0.322828</td>\n",
       "      <td>0.533147</td>\n",
       "      <td>0.272930</td>\n",
       "      <td>0.213291</td>\n",
       "      <td>0.211228</td>\n",
       "      <td>0.140234</td>\n",
       "      <td>0.351365</td>\n",
       "      <td>1</td>\n",
       "      <td>372</td>\n",
       "      <td>on 11/20/09 14:14 PM (PST)</td>\n",
       "      <td>Steve</td>\n",
       "      <td>1997 Dodge Avenger Coupe 2dr Coupe</td>\n",
       "      <td>still enjoyable</td>\n",
       "      <td>I bought my Avenger new in 1997 with the 5 sp...</td>\n",
       "      <td>4.25</td>\n",
       "      <td>still enjoyable: I bought my Avenger new in 19...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  text sentiment.label  sentiment.score  \\\n",
       "6353                         red color        positive         0.945484   \n",
       "6354                         best year        positive         0.945484   \n",
       "6355                      Road America        negative        -0.322828   \n",
       "6356                     better brakes        positive         0.926852   \n",
       "6357                   fron end fascia        positive         0.945484   \n",
       "6358                   only real issue        positive         0.524426   \n",
       "6359  original stainless steel exhaust        positive         0.243223   \n",
       "6360                             speed        positive         0.939953   \n",
       "6361                              kids        negative        -0.322828   \n",
       "6362                             track        negative        -0.322828   \n",
       "\n",
       "      relevance  emotion.sadness  emotion.joy  emotion.fear  emotion.disgust  \\\n",
       "6353   0.686564         0.262876     0.619660      0.072974         0.026809   \n",
       "6354   0.684752         0.262876     0.619660      0.072974         0.026809   \n",
       "6355   0.679955         0.272930     0.213291      0.211228         0.140234   \n",
       "6356   0.671791         0.015269     0.627940      0.028063         0.005312   \n",
       "6357   0.640654         0.262876     0.619660      0.072974         0.026809   \n",
       "6358   0.614013         0.309655     0.053606      0.062797         0.037489   \n",
       "6359   0.597900         0.657172     0.088561      0.207029         0.087853   \n",
       "6360   0.546267         0.035011     0.663947      0.027223         0.007594   \n",
       "6361   0.545298         0.272930     0.213291      0.211228         0.140234   \n",
       "6362   0.533147         0.272930     0.213291      0.211228         0.140234   \n",
       "\n",
       "      emotion.anger  count review.number                  Review_Date  \\\n",
       "6353       0.040345      1           372   on 11/20/09 14:14 PM (PST)   \n",
       "6354       0.040345      1           372   on 11/20/09 14:14 PM (PST)   \n",
       "6355       0.351365      1           372   on 11/20/09 14:14 PM (PST)   \n",
       "6356       0.013645      1           372   on 11/20/09 14:14 PM (PST)   \n",
       "6357       0.040345      1           372   on 11/20/09 14:14 PM (PST)   \n",
       "6358       0.379048      1           372   on 11/20/09 14:14 PM (PST)   \n",
       "6359       0.085118      1           372   on 11/20/09 14:14 PM (PST)   \n",
       "6360       0.016850      2           372   on 11/20/09 14:14 PM (PST)   \n",
       "6361       0.351365      1           372   on 11/20/09 14:14 PM (PST)   \n",
       "6362       0.351365      1           372   on 11/20/09 14:14 PM (PST)   \n",
       "\n",
       "     Author_Name                       Vehicle_Title     Review_Title  \\\n",
       "6353      Steve   1997 Dodge Avenger Coupe 2dr Coupe  still enjoyable   \n",
       "6354      Steve   1997 Dodge Avenger Coupe 2dr Coupe  still enjoyable   \n",
       "6355      Steve   1997 Dodge Avenger Coupe 2dr Coupe  still enjoyable   \n",
       "6356      Steve   1997 Dodge Avenger Coupe 2dr Coupe  still enjoyable   \n",
       "6357      Steve   1997 Dodge Avenger Coupe 2dr Coupe  still enjoyable   \n",
       "6358      Steve   1997 Dodge Avenger Coupe 2dr Coupe  still enjoyable   \n",
       "6359      Steve   1997 Dodge Avenger Coupe 2dr Coupe  still enjoyable   \n",
       "6360      Steve   1997 Dodge Avenger Coupe 2dr Coupe  still enjoyable   \n",
       "6361      Steve   1997 Dodge Avenger Coupe 2dr Coupe  still enjoyable   \n",
       "6362      Steve   1997 Dodge Avenger Coupe 2dr Coupe  still enjoyable   \n",
       "\n",
       "                                                 Review  Rating  \\\n",
       "6353   I bought my Avenger new in 1997 with the 5 sp...    4.25   \n",
       "6354   I bought my Avenger new in 1997 with the 5 sp...    4.25   \n",
       "6355   I bought my Avenger new in 1997 with the 5 sp...    4.25   \n",
       "6356   I bought my Avenger new in 1997 with the 5 sp...    4.25   \n",
       "6357   I bought my Avenger new in 1997 with the 5 sp...    4.25   \n",
       "6358   I bought my Avenger new in 1997 with the 5 sp...    4.25   \n",
       "6359   I bought my Avenger new in 1997 with the 5 sp...    4.25   \n",
       "6360   I bought my Avenger new in 1997 with the 5 sp...    4.25   \n",
       "6361   I bought my Avenger new in 1997 with the 5 sp...    4.25   \n",
       "6362   I bought my Avenger new in 1997 with the 5 sp...    4.25   \n",
       "\n",
       "                                         Review_Content  \n",
       "6353  still enjoyable: I bought my Avenger new in 19...  \n",
       "6354  still enjoyable: I bought my Avenger new in 19...  \n",
       "6355  still enjoyable: I bought my Avenger new in 19...  \n",
       "6356  still enjoyable: I bought my Avenger new in 19...  \n",
       "6357  still enjoyable: I bought my Avenger new in 19...  \n",
       "6358  still enjoyable: I bought my Avenger new in 19...  \n",
       "6359  still enjoyable: I bought my Avenger new in 19...  \n",
       "6360  still enjoyable: I bought my Avenger new in 19...  \n",
       "6361  still enjoyable: I bought my Avenger new in 19...  \n",
       "6362  still enjoyable: I bought my Avenger new in 19...  "
      ]
     },
     "execution_count": 660,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_merged_keywords_review_df = merged_keywords_review_df.groupby('Review_Title')\n",
    "grouped_merged_keywords_review_df.get_group('still enjoyable').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment.score</th>\n",
       "      <th>relevance</th>\n",
       "      <th>emotion.sadness</th>\n",
       "      <th>emotion.joy</th>\n",
       "      <th>emotion.fear</th>\n",
       "      <th>emotion.disgust</th>\n",
       "      <th>emotion.anger</th>\n",
       "      <th>count</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Review_Title</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>I like my \" monster\"</th>\n",
       "      <td>-0.306308</td>\n",
       "      <td>0.553495</td>\n",
       "      <td>0.220241</td>\n",
       "      <td>0.279775</td>\n",
       "      <td>0.229891</td>\n",
       "      <td>0.058756</td>\n",
       "      <td>0.110712</td>\n",
       "      <td>1.217391</td>\n",
       "      <td>4.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'06 Magnum</th>\n",
       "      <td>0.487433</td>\n",
       "      <td>0.567790</td>\n",
       "      <td>0.125580</td>\n",
       "      <td>0.438638</td>\n",
       "      <td>0.076288</td>\n",
       "      <td>0.062533</td>\n",
       "      <td>0.081828</td>\n",
       "      <td>1.060606</td>\n",
       "      <td>4.594697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'09 Avenger!</th>\n",
       "      <td>-0.688769</td>\n",
       "      <td>0.662289</td>\n",
       "      <td>0.028387</td>\n",
       "      <td>0.246789</td>\n",
       "      <td>0.084654</td>\n",
       "      <td>0.038706</td>\n",
       "      <td>0.043655</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>06 R/T AWD</th>\n",
       "      <td>0.237867</td>\n",
       "      <td>0.560739</td>\n",
       "      <td>0.206878</td>\n",
       "      <td>0.421021</td>\n",
       "      <td>0.070283</td>\n",
       "      <td>0.081966</td>\n",
       "      <td>0.079011</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1 1/2 years later I still love it</th>\n",
       "      <td>-0.178809</td>\n",
       "      <td>0.553075</td>\n",
       "      <td>0.430174</td>\n",
       "      <td>0.311697</td>\n",
       "      <td>0.115977</td>\n",
       "      <td>0.050470</td>\n",
       "      <td>0.110762</td>\n",
       "      <td>1.047619</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11 year new car drought</th>\n",
       "      <td>0.063141</td>\n",
       "      <td>0.524719</td>\n",
       "      <td>0.363201</td>\n",
       "      <td>0.336566</td>\n",
       "      <td>0.102854</td>\n",
       "      <td>0.057663</td>\n",
       "      <td>0.212307</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997 Dodge Avenger 2 Dr Coupe 6 cyl 2.5</th>\n",
       "      <td>0.005320</td>\n",
       "      <td>0.556675</td>\n",
       "      <td>0.200715</td>\n",
       "      <td>0.360239</td>\n",
       "      <td>0.128520</td>\n",
       "      <td>0.119739</td>\n",
       "      <td>0.155009</td>\n",
       "      <td>1.058824</td>\n",
       "      <td>3.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997 avenger</th>\n",
       "      <td>0.031169</td>\n",
       "      <td>0.576201</td>\n",
       "      <td>0.183959</td>\n",
       "      <td>0.183045</td>\n",
       "      <td>0.153545</td>\n",
       "      <td>0.066643</td>\n",
       "      <td>0.092339</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999 Avenger ES</th>\n",
       "      <td>0.036593</td>\n",
       "      <td>0.550951</td>\n",
       "      <td>0.233643</td>\n",
       "      <td>0.210605</td>\n",
       "      <td>0.105920</td>\n",
       "      <td>0.064464</td>\n",
       "      <td>0.103785</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999 Dodge Avenger ES Coupe</th>\n",
       "      <td>0.833189</td>\n",
       "      <td>0.618432</td>\n",
       "      <td>0.109233</td>\n",
       "      <td>0.481067</td>\n",
       "      <td>0.070001</td>\n",
       "      <td>0.027062</td>\n",
       "      <td>0.029643</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         sentiment.score  relevance  \\\n",
       "Review_Title                                                          \n",
       " I like my \" monster\"                          -0.306308   0.553495   \n",
       "'06 Magnum                                      0.487433   0.567790   \n",
       "'09 Avenger!                                   -0.688769   0.662289   \n",
       "06 R/T AWD                                      0.237867   0.560739   \n",
       "1 1/2 years later I still love it              -0.178809   0.553075   \n",
       "11 year new car drought                         0.063141   0.524719   \n",
       "1997 Dodge Avenger 2 Dr Coupe 6 cyl 2.5         0.005320   0.556675   \n",
       "1997 avenger                                    0.031169   0.576201   \n",
       "1999 Avenger ES                                 0.036593   0.550951   \n",
       "1999 Dodge Avenger ES Coupe                     0.833189   0.618432   \n",
       "\n",
       "                                         emotion.sadness  emotion.joy  \\\n",
       "Review_Title                                                            \n",
       " I like my \" monster\"                           0.220241     0.279775   \n",
       "'06 Magnum                                      0.125580     0.438638   \n",
       "'09 Avenger!                                    0.028387     0.246789   \n",
       "06 R/T AWD                                      0.206878     0.421021   \n",
       "1 1/2 years later I still love it               0.430174     0.311697   \n",
       "11 year new car drought                         0.363201     0.336566   \n",
       "1997 Dodge Avenger 2 Dr Coupe 6 cyl 2.5         0.200715     0.360239   \n",
       "1997 avenger                                    0.183959     0.183045   \n",
       "1999 Avenger ES                                 0.233643     0.210605   \n",
       "1999 Dodge Avenger ES Coupe                     0.109233     0.481067   \n",
       "\n",
       "                                         emotion.fear  emotion.disgust  \\\n",
       "Review_Title                                                             \n",
       " I like my \" monster\"                        0.229891         0.058756   \n",
       "'06 Magnum                                   0.076288         0.062533   \n",
       "'09 Avenger!                                 0.084654         0.038706   \n",
       "06 R/T AWD                                   0.070283         0.081966   \n",
       "1 1/2 years later I still love it            0.115977         0.050470   \n",
       "11 year new car drought                      0.102854         0.057663   \n",
       "1997 Dodge Avenger 2 Dr Coupe 6 cyl 2.5      0.128520         0.119739   \n",
       "1997 avenger                                 0.153545         0.066643   \n",
       "1999 Avenger ES                              0.105920         0.064464   \n",
       "1999 Dodge Avenger ES Coupe                  0.070001         0.027062   \n",
       "\n",
       "                                         emotion.anger     count    Rating  \n",
       "Review_Title                                                                \n",
       " I like my \" monster\"                         0.110712  1.217391  4.250000  \n",
       "'06 Magnum                                    0.081828  1.060606  4.594697  \n",
       "'09 Avenger!                                  0.043655  1.000000  4.000000  \n",
       "06 R/T AWD                                    0.079011  1.000000  4.875000  \n",
       "1 1/2 years later I still love it             0.110762  1.047619  5.000000  \n",
       "11 year new car drought                       0.212307  1.000000  4.375000  \n",
       "1997 Dodge Avenger 2 Dr Coupe 6 cyl 2.5       0.155009  1.058824  3.625000  \n",
       "1997 avenger                                  0.092339  1.000000  4.125000  \n",
       "1999 Avenger ES                               0.103785  1.000000  4.375000  \n",
       "1999 Dodge Avenger ES Coupe                   0.029643  1.000000  5.000000  "
      ]
     },
     "execution_count": 661,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_merged_keywords_review_df = grouped_merged_keywords_review_df.mean()\n",
    "agg_merged_keywords_review_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_e5535e1e_70a1_11eb_af07_acde48001122row0_col0,#T_e5535e1e_70a1_11eb_af07_acde48001122row1_col1,#T_e5535e1e_70a1_11eb_af07_acde48001122row2_col2,#T_e5535e1e_70a1_11eb_af07_acde48001122row3_col3,#T_e5535e1e_70a1_11eb_af07_acde48001122row4_col4,#T_e5535e1e_70a1_11eb_af07_acde48001122row5_col5,#T_e5535e1e_70a1_11eb_af07_acde48001122row6_col6,#T_e5535e1e_70a1_11eb_af07_acde48001122row7_col7,#T_e5535e1e_70a1_11eb_af07_acde48001122row8_col8{\n",
       "            background-color:  #b40426;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_e5535e1e_70a1_11eb_af07_acde48001122row0_col1{\n",
       "            background-color:  #9bbcff;\n",
       "            color:  #000000;\n",
       "        }#T_e5535e1e_70a1_11eb_af07_acde48001122row0_col2,#T_e5535e1e_70a1_11eb_af07_acde48001122row0_col4,#T_e5535e1e_70a1_11eb_af07_acde48001122row0_col6,#T_e5535e1e_70a1_11eb_af07_acde48001122row0_col7,#T_e5535e1e_70a1_11eb_af07_acde48001122row2_col0,#T_e5535e1e_70a1_11eb_af07_acde48001122row2_col1,#T_e5535e1e_70a1_11eb_af07_acde48001122row2_col3,#T_e5535e1e_70a1_11eb_af07_acde48001122row2_col8,#T_e5535e1e_70a1_11eb_af07_acde48001122row3_col5{\n",
       "            background-color:  #3b4cc0;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_e5535e1e_70a1_11eb_af07_acde48001122row0_col3{\n",
       "            background-color:  #e36c55;\n",
       "            color:  #000000;\n",
       "        }#T_e5535e1e_70a1_11eb_af07_acde48001122row0_col5,#T_e5535e1e_70a1_11eb_af07_acde48001122row5_col1{\n",
       "            background-color:  #4257c9;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_e5535e1e_70a1_11eb_af07_acde48001122row0_col8{\n",
       "            background-color:  #f7aa8c;\n",
       "            color:  #000000;\n",
       "        }#T_e5535e1e_70a1_11eb_af07_acde48001122row1_col0{\n",
       "            background-color:  #e2dad5;\n",
       "            color:  #000000;\n",
       "        }#T_e5535e1e_70a1_11eb_af07_acde48001122row1_col2{\n",
       "            background-color:  #a7c5fe;\n",
       "            color:  #000000;\n",
       "        }#T_e5535e1e_70a1_11eb_af07_acde48001122row1_col3{\n",
       "            background-color:  #e5d8d1;\n",
       "            color:  #000000;\n",
       "        }#T_e5535e1e_70a1_11eb_af07_acde48001122row1_col4{\n",
       "            background-color:  #a9c6fd;\n",
       "            color:  #000000;\n",
       "        }#T_e5535e1e_70a1_11eb_af07_acde48001122row1_col5{\n",
       "            background-color:  #85a8fc;\n",
       "            color:  #000000;\n",
       "        }#T_e5535e1e_70a1_11eb_af07_acde48001122row1_col6{\n",
       "            background-color:  #97b8ff;\n",
       "            color:  #000000;\n",
       "        }#T_e5535e1e_70a1_11eb_af07_acde48001122row1_col7{\n",
       "            background-color:  #516ddb;\n",
       "            color:  #000000;\n",
       "        }#T_e5535e1e_70a1_11eb_af07_acde48001122row1_col8{\n",
       "            background-color:  #b3cdfb;\n",
       "            color:  #000000;\n",
       "        }#T_e5535e1e_70a1_11eb_af07_acde48001122row2_col4{\n",
       "            background-color:  #f5c0a7;\n",
       "            color:  #000000;\n",
       "        }#T_e5535e1e_70a1_11eb_af07_acde48001122row2_col5{\n",
       "            background-color:  #efcfbf;\n",
       "            color:  #000000;\n",
       "        }#T_e5535e1e_70a1_11eb_af07_acde48001122row2_col6{\n",
       "            background-color:  #f7ba9f;\n",
       "            color:  #000000;\n",
       "        }#T_e5535e1e_70a1_11eb_af07_acde48001122row2_col7,#T_e5535e1e_70a1_11eb_af07_acde48001122row6_col7{\n",
       "            background-color:  #a1c0ff;\n",
       "            color:  #000000;\n",
       "        }#T_e5535e1e_70a1_11eb_af07_acde48001122row3_col0{\n",
       "            background-color:  #e36b54;\n",
       "            color:  #000000;\n",
       "        }#T_e5535e1e_70a1_11eb_af07_acde48001122row3_col1{\n",
       "            background-color:  #aac7fd;\n",
       "            color:  #000000;\n",
       "        }#T_e5535e1e_70a1_11eb_af07_acde48001122row3_col2{\n",
       "            background-color:  #455cce;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_e5535e1e_70a1_11eb_af07_acde48001122row3_col4{\n",
       "            background-color:  #445acc;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_e5535e1e_70a1_11eb_af07_acde48001122row3_col6{\n",
       "            background-color:  #465ecf;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_e5535e1e_70a1_11eb_af07_acde48001122row3_col7,#T_e5535e1e_70a1_11eb_af07_acde48001122row7_col1{\n",
       "            background-color:  #4e68d8;\n",
       "            color:  #000000;\n",
       "        }#T_e5535e1e_70a1_11eb_af07_acde48001122row3_col8{\n",
       "            background-color:  #f7b89c;\n",
       "            color:  #000000;\n",
       "        }#T_e5535e1e_70a1_11eb_af07_acde48001122row4_col0,#T_e5535e1e_70a1_11eb_af07_acde48001122row4_col8{\n",
       "            background-color:  #5b7ae5;\n",
       "            color:  #000000;\n",
       "        }#T_e5535e1e_70a1_11eb_af07_acde48001122row4_col1,#T_e5535e1e_70a1_11eb_af07_acde48001122row6_col8{\n",
       "            background-color:  #5d7ce6;\n",
       "            color:  #000000;\n",
       "        }#T_e5535e1e_70a1_11eb_af07_acde48001122row4_col2{\n",
       "            background-color:  #f7b497;\n",
       "            color:  #000000;\n",
       "        }#T_e5535e1e_70a1_11eb_af07_acde48001122row4_col3,#T_e5535e1e_70a1_11eb_af07_acde48001122row5_col8{\n",
       "            background-color:  #5a78e4;\n",
       "            color:  #000000;\n",
       "        }#T_e5535e1e_70a1_11eb_af07_acde48001122row4_col5{\n",
       "            background-color:  #dedcdb;\n",
       "            color:  #000000;\n",
       "        }#T_e5535e1e_70a1_11eb_af07_acde48001122row4_col6{\n",
       "            background-color:  #f5c4ac;\n",
       "            color:  #000000;\n",
       "        }#T_e5535e1e_70a1_11eb_af07_acde48001122row4_col7{\n",
       "            background-color:  #98b9ff;\n",
       "            color:  #000000;\n",
       "        }#T_e5535e1e_70a1_11eb_af07_acde48001122row5_col0{\n",
       "            background-color:  #7295f4;\n",
       "            color:  #000000;\n",
       "        }#T_e5535e1e_70a1_11eb_af07_acde48001122row5_col2{\n",
       "            background-color:  #f6bfa6;\n",
       "            color:  #000000;\n",
       "        }#T_e5535e1e_70a1_11eb_af07_acde48001122row5_col3{\n",
       "            background-color:  #5e7de7;\n",
       "            color:  #000000;\n",
       "        }#T_e5535e1e_70a1_11eb_af07_acde48001122row5_col4{\n",
       "            background-color:  #e4d9d2;\n",
       "            color:  #000000;\n",
       "        }#T_e5535e1e_70a1_11eb_af07_acde48001122row5_col6{\n",
       "            background-color:  #f6bea4;\n",
       "            color:  #000000;\n",
       "        }#T_e5535e1e_70a1_11eb_af07_acde48001122row5_col7{\n",
       "            background-color:  #84a7fc;\n",
       "            color:  #000000;\n",
       "        }#T_e5535e1e_70a1_11eb_af07_acde48001122row6_col0,#T_e5535e1e_70a1_11eb_af07_acde48001122row6_col3,#T_e5535e1e_70a1_11eb_af07_acde48001122row8_col5{\n",
       "            background-color:  #5875e1;\n",
       "            color:  #000000;\n",
       "        }#T_e5535e1e_70a1_11eb_af07_acde48001122row6_col1{\n",
       "            background-color:  #4358cb;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_e5535e1e_70a1_11eb_af07_acde48001122row6_col2{\n",
       "            background-color:  #f7b093;\n",
       "            color:  #000000;\n",
       "        }#T_e5535e1e_70a1_11eb_af07_acde48001122row6_col4,#T_e5535e1e_70a1_11eb_af07_acde48001122row6_col5{\n",
       "            background-color:  #f4c5ad;\n",
       "            color:  #000000;\n",
       "        }#T_e5535e1e_70a1_11eb_af07_acde48001122row7_col0{\n",
       "            background-color:  #a5c3fe;\n",
       "            color:  #000000;\n",
       "        }#T_e5535e1e_70a1_11eb_af07_acde48001122row7_col2{\n",
       "            background-color:  #e3d9d3;\n",
       "            color:  #000000;\n",
       "        }#T_e5535e1e_70a1_11eb_af07_acde48001122row7_col3{\n",
       "            background-color:  #abc8fd;\n",
       "            color:  #000000;\n",
       "        }#T_e5535e1e_70a1_11eb_af07_acde48001122row7_col4{\n",
       "            background-color:  #d1dae9;\n",
       "            color:  #000000;\n",
       "        }#T_e5535e1e_70a1_11eb_af07_acde48001122row7_col5{\n",
       "            background-color:  #b9d0f9;\n",
       "            color:  #000000;\n",
       "        }#T_e5535e1e_70a1_11eb_af07_acde48001122row7_col6{\n",
       "            background-color:  #d7dce3;\n",
       "            color:  #000000;\n",
       "        }#T_e5535e1e_70a1_11eb_af07_acde48001122row7_col8{\n",
       "            background-color:  #90b2fe;\n",
       "            color:  #000000;\n",
       "        }#T_e5535e1e_70a1_11eb_af07_acde48001122row8_col0{\n",
       "            background-color:  #f49a7b;\n",
       "            color:  #000000;\n",
       "        }#T_e5535e1e_70a1_11eb_af07_acde48001122row8_col1{\n",
       "            background-color:  #779af7;\n",
       "            color:  #000000;\n",
       "        }#T_e5535e1e_70a1_11eb_af07_acde48001122row8_col2,#T_e5535e1e_70a1_11eb_af07_acde48001122row8_col4{\n",
       "            background-color:  #688aef;\n",
       "            color:  #000000;\n",
       "        }#T_e5535e1e_70a1_11eb_af07_acde48001122row8_col3{\n",
       "            background-color:  #f7ac8e;\n",
       "            color:  #000000;\n",
       "        }#T_e5535e1e_70a1_11eb_af07_acde48001122row8_col6{\n",
       "            background-color:  #6e90f2;\n",
       "            color:  #000000;\n",
       "        }#T_e5535e1e_70a1_11eb_af07_acde48001122row8_col7{\n",
       "            background-color:  #506bda;\n",
       "            color:  #000000;\n",
       "        }</style><table id=\"T_e5535e1e_70a1_11eb_af07_acde48001122\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >sentiment.score</th>        <th class=\"col_heading level0 col1\" >relevance</th>        <th class=\"col_heading level0 col2\" >emotion.sadness</th>        <th class=\"col_heading level0 col3\" >emotion.joy</th>        <th class=\"col_heading level0 col4\" >emotion.fear</th>        <th class=\"col_heading level0 col5\" >emotion.disgust</th>        <th class=\"col_heading level0 col6\" >emotion.anger</th>        <th class=\"col_heading level0 col7\" >count</th>        <th class=\"col_heading level0 col8\" >Rating</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_e5535e1e_70a1_11eb_af07_acde48001122level0_row0\" class=\"row_heading level0 row0\" >sentiment.score</th>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row0_col0\" class=\"data row0 col0\" >1.00</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row0_col1\" class=\"data row0 col1\" >0.17</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row0_col2\" class=\"data row0 col2\" >-0.73</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row0_col3\" class=\"data row0 col3\" >0.75</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row0_col4\" class=\"data row0 col4\" >-0.55</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row0_col5\" class=\"data row0 col5\" >-0.43</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row0_col6\" class=\"data row0 col6\" >-0.57</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row0_col7\" class=\"data row0 col7\" >-0.18</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row0_col8\" class=\"data row0 col8\" >0.56</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e5535e1e_70a1_11eb_af07_acde48001122level0_row1\" class=\"row_heading level0 row1\" >relevance</th>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row1_col0\" class=\"data row1 col0\" >0.17</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row1_col1\" class=\"data row1 col1\" >1.00</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row1_col2\" class=\"data row1 col2\" >-0.17</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row1_col3\" class=\"data row1 col3\" >0.22</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row1_col4\" class=\"data row1 col4\" >-0.04</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row1_col5\" class=\"data row1 col5\" >-0.14</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row1_col6\" class=\"data row1 col6\" >-0.13</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row1_col7\" class=\"data row1 col7\" >-0.09</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row1_col8\" class=\"data row1 col8\" >0.06</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e5535e1e_70a1_11eb_af07_acde48001122level0_row2\" class=\"row_heading level0 row2\" >emotion.sadness</th>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row2_col0\" class=\"data row2 col0\" >-0.73</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row2_col1\" class=\"data row2 col1\" >-0.17</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row2_col2\" class=\"data row2 col2\" >1.00</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row2_col3\" class=\"data row2 col3\" >-0.67</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row2_col4\" class=\"data row2 col4\" >0.44</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row2_col5\" class=\"data row2 col5\" >0.38</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row2_col6\" class=\"data row2 col6\" >0.46</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row2_col7\" class=\"data row2 col7\" >0.18</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row2_col8\" class=\"data row2 col8\" >-0.48</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e5535e1e_70a1_11eb_af07_acde48001122level0_row3\" class=\"row_heading level0 row3\" >emotion.joy</th>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row3_col0\" class=\"data row3 col0\" >0.75</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row3_col1\" class=\"data row3 col1\" >0.22</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row3_col2\" class=\"data row3 col2\" >-0.67</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row3_col3\" class=\"data row3 col3\" >1.00</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row3_col4\" class=\"data row3 col4\" >-0.50</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row3_col5\" class=\"data row3 col5\" >-0.47</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row3_col6\" class=\"data row3 col6\" >-0.50</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row3_col7\" class=\"data row3 col7\" >-0.10</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row3_col8\" class=\"data row3 col8\" >0.50</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e5535e1e_70a1_11eb_af07_acde48001122level0_row4\" class=\"row_heading level0 row4\" >emotion.fear</th>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row4_col0\" class=\"data row4 col0\" >-0.55</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row4_col1\" class=\"data row4 col1\" >-0.04</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row4_col2\" class=\"data row4 col2\" >0.44</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row4_col3\" class=\"data row4 col3\" >-0.50</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row4_col4\" class=\"data row4 col4\" >1.00</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row4_col5\" class=\"data row4 col5\" >0.27</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row4_col6\" class=\"data row4 col6\" >0.41</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row4_col7\" class=\"data row4 col7\" >0.15</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row4_col8\" class=\"data row4 col8\" >-0.32</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e5535e1e_70a1_11eb_af07_acde48001122level0_row5\" class=\"row_heading level0 row5\" >emotion.disgust</th>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row5_col0\" class=\"data row5 col0\" >-0.43</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row5_col1\" class=\"data row5 col1\" >-0.14</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row5_col2\" class=\"data row5 col2\" >0.38</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row5_col3\" class=\"data row5 col3\" >-0.47</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row5_col4\" class=\"data row5 col4\" >0.27</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row5_col5\" class=\"data row5 col5\" >1.00</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row5_col6\" class=\"data row5 col6\" >0.44</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row5_col7\" class=\"data row5 col7\" >0.09</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row5_col8\" class=\"data row5 col8\" >-0.33</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e5535e1e_70a1_11eb_af07_acde48001122level0_row6\" class=\"row_heading level0 row6\" >emotion.anger</th>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row6_col0\" class=\"data row6 col0\" >-0.57</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row6_col1\" class=\"data row6 col1\" >-0.13</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row6_col2\" class=\"data row6 col2\" >0.46</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row6_col3\" class=\"data row6 col3\" >-0.50</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row6_col4\" class=\"data row6 col4\" >0.41</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row6_col5\" class=\"data row6 col5\" >0.44</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row6_col6\" class=\"data row6 col6\" >1.00</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row6_col7\" class=\"data row6 col7\" >0.18</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row6_col8\" class=\"data row6 col8\" >-0.31</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e5535e1e_70a1_11eb_af07_acde48001122level0_row7\" class=\"row_heading level0 row7\" >count</th>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row7_col0\" class=\"data row7 col0\" >-0.18</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row7_col1\" class=\"data row7 col1\" >-0.09</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row7_col2\" class=\"data row7 col2\" >0.18</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row7_col3\" class=\"data row7 col3\" >-0.10</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row7_col4\" class=\"data row7 col4\" >0.15</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row7_col5\" class=\"data row7 col5\" >0.09</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row7_col6\" class=\"data row7 col6\" >0.18</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row7_col7\" class=\"data row7 col7\" >1.00</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row7_col8\" class=\"data row7 col8\" >-0.09</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e5535e1e_70a1_11eb_af07_acde48001122level0_row8\" class=\"row_heading level0 row8\" >Rating</th>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row8_col0\" class=\"data row8 col0\" >0.56</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row8_col1\" class=\"data row8 col1\" >0.06</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row8_col2\" class=\"data row8 col2\" >-0.48</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row8_col3\" class=\"data row8 col3\" >0.50</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row8_col4\" class=\"data row8 col4\" >-0.32</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row8_col5\" class=\"data row8 col5\" >-0.33</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row8_col6\" class=\"data row8 col6\" >-0.31</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row8_col7\" class=\"data row8 col7\" >-0.09</td>\n",
       "                        <td id=\"T_e5535e1e_70a1_11eb_af07_acde48001122row8_col8\" class=\"data row8 col8\" >1.00</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fb72b6ed520>"
      ]
     },
     "execution_count": 655,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To find the correlation among the columns using pearson method \n",
    "corr = agg_merged_keywords_review_df.corr(method ='pearson')\n",
    "corr.style.background_gradient(cmap='coolwarm').set_precision(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the table above shows, there is an association between the review's Ratings and the Watson NLU sentiment score and joy emotion but repulsion between review's Ratings and sadness emotion. The results also demonstrate the strong positive correlation between Watson NLU sentiment score and Watson NLU joy emotion. In contrary, there is a strong negative correlation between sadness emotion and the sentiment score as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.matshow(agg_merged_keywords_review_df.corr())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text-estensions-for-pandas",
   "language": "python",
   "name": "text-estensions-for-pandas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
